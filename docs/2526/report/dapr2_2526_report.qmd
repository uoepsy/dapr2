---
title: "DAPR2 Assessed Group Report 2025/26"
format: 
  html:
    toc: true
    number_sections: false
    toc-location: left
editor_options: 
  chunk_output_type: console
---

```{css echo=FALSE}
div.blue, div.red, div.green, div.yellow, div.frame{ 
    border-radius: 5px; 
    padding: 20px 20px 10px 20px; 
    margin-top: 20px; 
    margin-bottom: 20px; 
}
.blue { background-color:#d9edf7 !important; }
.green { background-color:#dff0d8 !important; }
.yellow { background-color:#fcf8e3 !important; }
.red { background-color:#F3E3E5 !important; 
.frame {border: 1px solid #333333 !important; }
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval=FALSE, warning=FALSE, message=FALSE)
```

---

# General Info

Your task is to describe and analyse the data provided in order to answer a set of research question(s). Analyses will draw on the methodologies we have discussed in lectures and labs. The specific study contexts and research questions can be found below in the [data & research questions section](#task). 

### Key Dates

**Coursework set**: Thursday 12th February 2026 (12 noon)    
**Coursework due**: Thursday 5th March 2026 (12 noon)    
  
# Group Work Policy

+ Each group should submit a __single report__ (two documents: see [Files to Submit](#Files-to-Submit)).
+ As this is a group-based report, __no extensions are permitted__.
+ All group members are expected and encouraged to contribute to the report, but __you should not work with other groups__. Similarity checks will be performed, and further investigations will be carried out if assignments from different groups are similar.


# Use of AI

For group work, the other members of your group are a better resource than any LLM!  

- Using AI in non-assessed work = ✅
    - e.g., LLMs are sometimes helpful for understanding R documentation.
    - But be aware of the environmental cost (<https://www.bbc.co.uk/sounds/play/w3ct6vz4>)
- using AI in assessed work = ❌
    - Do not use LLMs at all in your assessments, including:
        - planning, conducting, or interpreting analyses
        - structuring or formatting a report 
        - generating text or code 

Just like plagiarising from human-written texts, presenting AI work as your own is academic misconduct. Engaging with every step of the process yourself is what will help you learn. And learning is why we are all here.


# Structure & Rubric {#rubric}

Your report should **include the following three sections**:

1. Analysis strategy
2. Results
3. Discussion

You may also include an appendix for assumption and diagnostic checks.
  
Your report will be **marked based on four criteria**: the quality of each of the three numbered sections (analysis strategy, results, discussion), as well as writing and formatting. The associated grade percentages are shown in parentheses below. 

### 1. Analysis Strategy (40%)

In this section you should describe how you are going to address the research aim(s) of the study described.
Your goal should be to describe everything clearly enough that another researcher could fully reproduce your analysis without ever seeing your code.



*Important:*   
  
- You do not need to include an introduction to the study unless you feel it is helpful in writing your analysis strategy.
- Your analysis strategy should not contain any results.
- You may be able to answer multiple research questions using a single analysis, by drawing on multiple parts of the same results. Think carefully about whether or not this is the case before running several analyses/fitting several linear models.

:::green 

**The marking of the analysis strategy section will be based on how thoroughly you describe**:

- data cleaning and variable recoding  
- any descriptive statistics or visualizations you will use prior to running models  
- the analyses you undertook (description of the model(s))  
- how you will check your model(s) (assumptions and diagnostics including the criteria/thresholds you have used to evaluate each)  
- what specific information from each model provides the answer to the questions, including details of your $\alpha$ levels and any required corrections  
- rationales for all choices

:::





### 2. Results (40%)

The results section should follow logically from your analysis strategy and present the outcomes of your analyses.
Typically, results sections begin by presenting descriptive statistics and then move on to inferential tests.

:::green 

**The marking of the results section will be based on the following**:

- All key model results should be presented in the main body of the report (tables are very useful here).
- You should provide a full interpretation of the results that addresses the research question(s).
- Model assumption and diagnostic checks should be noted and interpreted (where you can refer the reader to both decision rules from your analysis strategy, and to figures included in the appendix).  
- Concision (i.e., no unnecessary repetition of information, including of descriptive statistics). 

:::

### 3. Discussion (10%)

The discussion section should be very brief (one or two sentences). 
Give the reader succinct take-home messages about what you conclude for the research question(s). 

This section should not introduce any new statistical results, nor repeat detailed statistical results (but it can/should reference the information presented in the other two sections of the report).

:::green 

**The marking of the discussion section will be based on the following**:

- Concise (<4 sentences), coherent and accurate summary statements that link the results of your analysis to the research question(s). 

:::

### Writing & Formatting (10%)

Your report should be a **maximum of four sides of A4** when the default formatting and font settings within RStudio are used when knitting your file.
Do not include a title page—it will count toward the page count total.

Except for the appendix, no content beyond the fourth page will be read or marked.

:::green 

**The marking for writing and formatting will be based on the following (applies both to main report and appendix)**:

- There should be no visible R code or direct output in your report. This means that code chunks should be hidden in the pdf produced by your rmd file. To tell RMarkdown to not show your code when knitting to HTML, add `echo=FALSE` next to the `r` that [appears after the backticks](https://uoepsy.github.io/scs/rmd-bootcamp/05-echoeval.html).   
- All figures and tables should be numbered and captioned, and they should all be referred to somewhere in the text.
- Important statistical outcomes should be summarised in the text.
- Reporting should be clear and consistent. If in doubt, follow [APA 7th Edition guidelines](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf). Avoid errors in reporting of statistical results, rounding errors, or e.g., e-04 notation.   
- Ensure writing is clear and well-structured. Would someone else who's not in your group understand it?
- Avoid unnecessary jargon or overly complex explanations.
- Avoid unnecessary repetition of information/duplication of tables/plots..   
- Report is a maximum of 4 pages long.  

:::


### Assumption and Diagnostics Appendix (Optional)

This optional section has no page limit. You may use this to present assumption and diagnostic plots.

Please note:

- The appendix is _only_ for figures and results from assumption and diagnostic checks. Any results from your main models that appear in the appendix will not be marked.
- You must still describe your assumption tests in your analysis strategy section, including how you will evaluate them.
- You must still summarise the results in the results section.
- You must refer accurately to the figures and tables labels presented in the appendix.

For example:   
  
- good use of the appendix:  
_"The model met the assumptions of linearity with residuals showing a constant mean of approximately zero across the fitted values (see Appendix X Fig X), and ..."_
  
- not so good use of the appendix:  
_"The model met assumptions (see Appendix X Fig X)."_



# File formats for writing

### For writing the text

You can write the text of the report either in RMarkdown (`.Rmd`, the type of file you've been working with in DAPR1 and DAPR2) or in a separate word processor.

Writing in a word processor might be useful because you have precise control over the formatting, and you can also work collaboratively in e.g. Google Docs.

Either way, the report must be exported to `.pdf` format for submission (more details below).

We don't mind which of these approaches you take.
**The important thing to remember is that the data analysis (cleaning, models, and plots) that are presented in the report should match those produced by your code.** 


:::yellow 

For a guide on writing and formatting in RMarkdown, you may find these resources helpful:

- [UoEPsy Rmd-Bootcamp](https://uoepsy.github.io/scs/rmd-bootcamp){target="_blank"}  
- [RMarkdown CheatSheet](https://rmarkdown.rstudio.com/lesson-15.HTML){target="_blank"}  
- [Writing Math in Rmd](https://rpruim.github.io/s341/S19/from-class/MathinRmd.html#:~:text=Math%20inside%20RMarkdown,10n%3D1n2.){target="_blank"}
:::


:::yellow
__Knitting .Rmd to PDF__  

Please note that to knit .Rmd directly to pdf, you should:

1. Make sure the tinytex package is installed.  
2. Makes sure the 'yaml' (bit at the very top of your document) looks something like this. Use the correct date; in `title`, replace _(GROUP NAME)_ with your group name (e.g., Asymptotic Antelopes); and in `author`, specify the exam number of each individual within the group (for example: B000001, B000002, B000003, ...):  
```{}
---
title: "DAPR2 Group Report (GROUP NAME)"
author: "B000001, B000002, B000003, ..."
date: "DD/MM/YYYY"
output: bookdown::pdf_document2
---
```

__If you cannot knit directly to pdf, then try the following steps:__   
  1) Knit to .html file  <br>
  2) Open your html in a web-browser (e.g. Chrome, Firefox)  <br>
  3) Print to pdf (Ctrl+P, then choose to save to pdf)  <br>
  4) Submit the pdf you just saved.  <br>

:::



### For writing the code

You must write your code in R and save it in either an `.Rmd` or `.R` file.
There are unfortunately no straightforward ways to live edit  `.Rmd` or `.R` documents collaboratively.
We suggest that each group nominates one person to hold the "master copy" of the script that will be submitted.



# Submission {#Files-to-Submit}

## File Types

You are required to submit __two files__:

1. a text-only report in **.pdf** format
2. the associated **.Rmd**/**.R** file that reproduces (the code for) your report

## File Names  

For both files which you submit, the filename should be your group name with the appropriate extension, and nothing else. 
  
For example, the group **Asymptotic Antelopes** would submit two files:  
  
  - one of **AsymptoticAntelopes.Rmd** / **AsymptoticAntelopes.R**, AND
  - **AsymptoticAntelopes.pdf** 

::: {.callout-note collapse=true}

#### Working Individually?

For anyone who has obtained permission to complete the task individually, format the 'yaml' (bit at the very top of your document) of your .Rmd file as follows, **with the correct date and with your exam number in the "author" field**:

```
---
title: "DAPR2 Report"
author: "B123456"
date: "MM/DD/YYYY"
output: bookdown::pdf_document2
---
```

Please name each file with your exam number (the letter "B" followed by a six digit number - which can be found on your student card: [See here for more information](https://buddy.vet.ed.ac.uk/help/how-to-find-your-exam-number/)). For example, if your exam number was B123456 you would submit:

- one of **B123456.Rmd** / **B123456.R**, AND
- **B123456.pdf**

You should also write your exam number in the "Submission Title" box prior to submission.

:::

## Who Submits & Where   

:::red

**ONLY ONE PERSON FROM EACH GROUP NEEDS TO SUBMIT**

We suggest that you do this together in-person/on a call, so that all group members are able to confirm that they are happy to submit. 

Please submit both files online via the Turnitin links on the LEARN page for DAPR2 (Go to the Report (Group Based) folder within the Assessments section on the Learn page). There will be two links, clearly labelled, as the files need to be submitted individually. 

For **each file** you should complete the "Submit File" popup by entering the exam numbers of _all of your group members_ in the "Submission Title" box (see below).

![](imgs/submissionbox.png){width="50%" fig-align="center"}

:::

# Marking

## Report Marking

As explained in detail above (see [Structure & Rubric](#rubric)), compiled reports will be assessed according to the following components, with the following weightings:  

- Analysis Strategy = 40%
- Results = 40%
- Discussion = 10%
- Writing and Formatting = 10%

The overall mark will be rounded to the nearest value on the [Psychology 20-point marking scale](https://uoe.sharepoint.com/sites/hss/ppls/PPLS-Undergraduate-Student-Hub-home/SitePages/Assessment-%26-Marking--Psychology.aspx).  

**We are primarily marking each group's *report*, and *not* your code.**
Grades and feedback are provided for the finished reports, with marks awarded for providing evidence of the ability to:  

  - understand and execute appropriate statistical methods to answer each of the questions,
  - provide clear explanation of the methods undertaken, and
  - provide clear and accurate presentation and interpretation of results and conclusions.  

**We still require your code so that we can assess the reproducibility of your work.** The code provided must successfully conduct the analysis described in your report, and return the exact results, figures and tables that are detailed in your report. If it does not, then the [code penalties outlined below](#Code-Pen) will apply. 

## Peer-Adjusted Marking 

Once the group project has been submitted, every member of the group will complete the peer-assessment, in which you will be asked to award "mark adjustments" to yourself and to each of the other members of your group. This will be done through Learn; details will be made available over the next couple of weeks. Each submitted mark should be understood as follows: Relative to the group as a whole, how much did each member contribute? If someone made an average contribution, you should award that person the middle score. If they contributed substantially more, you may choose to give a higher mark; if they genuinely contributed less, you might choose a lower mark. Marks for each group member are scaled then averaged together, and then used as “weights” to adjust the overall project mark. You can see an example of how this logic works by visiting <https://uoe-psy.shinyapps.io/peer_adj/> where there is a "live demo" of a peer-adjustment system.

If you don’t contribute any peer-adjustment marks, the relative weight of the group members who do complete the peer-assessment will be higher. 

**The assessed group reports are marked on the [Psychology 20-point marking scale](https://uoe.sharepoint.com/sites/hss/ppls/PPLS-Undergraduate-Student-Hub-home/SitePages/Assessment-%26-Marking--Psychology.aspx), and peer-assessment ratings adjust grades by up to 20% of the report mark**. 

## Penalties

### Code {#Code-Pen}

We will apply penalties for code errors or non-reproducibility.  

Prior to submitting, check the following:  

+ Does the code run line-by-line without throwing any errors? 
+ Does the code lead to the __exact same results__ that you have reported?
+ If .Rmd: Does the file knit successfully? 

If any of the lines in your report generate an error, or the code doesn't match the reported results, your grade will be lowered by ten points.

In other words, a 62 becomes 52, and a 42 becomes 32.

If your submission files are not either .Rmd/.R & PDF, you might lose all points and get a grade of zero (i.e., if we are unable to open the files).

If you have any technical issues with the Turnitin submission, please contact the Teaching Office (ppls.ugteaching@ed.ac.uk) and cc the PPLS Psych Stats email address (ppls.psych.stats@ed.ac.uk).


### Late

Submissions are considered late until __both__ files (in .Rmd/.R & PDF format) are submitted on Turnitin.   


# Questions

If you have questions because you're unsure about a section of the material, then look back over it, come and discuss with us the examples from class, and then apply what you learned to the report.

If you have other questions about the report, then [please ask them on Piazza](https://piazza.com/class/mecp4hqm47n5np/) so that everyone else can benefit from the answer.
(Check first to make sure that same question hasn't already been answered!)


<div style="height: 50px"></div>


---

<div style="height: 50px"></div>

# The study brief: Data & research questions {#task}

> Conduct and report on an analysis that addresses the research aims.  
The data is available at: <https://uoepsy.github.io/data/neuro_explanations.csv>   

## Study background and aims

The data used for the current report are simulated, designed to expand on the methods and results reported in the following paper:  

> Weisberg, D. S., Keil, F. C., Goodstein, J., Rawson, E., & Gray, J. R. (2008). The seductive allure of neuroscience explanations. *Journal of Cognitive Neuroscience, 20*(3), 470-477. https://dx.doi.org/10.1162%2Fjocn.2008.20040 

**You're not expected to provide a background/introduction section in your report, so you do not have to read this article.** 

In the study above, Weisberg and colleagues asked people to evaluate explanations of psychological phenomena.
They investigated whether mentioning irrelevant neuroscientific information in the explanation would affect people's judgements.
They found that this additional information made explanations more *satisfying*, but only for people who weren't experts in neursocience.
People who _were_ experts in neuroscience didn't show this effect.

What's not clear from that paper is how *accurate* people judge these explanations to be.
It's also unclear whether the mode of presentation (e.g., written, visual, auditory, etc.) of the neuroscience information might lead to different judgements.  

That's where the current study comes in.
This study is concerned with whether accuracy judgements for bad explanations of psychological phenomena are affected if the explanations are accompanied by different modes of information, and also if the explanations are judged by people with different levels of education.


## Procedure

In total, 270 participants were recruited: 90 with no degree, 90 with a UG degree, and 90 with a PG degree. These are the three education groups. The UG and PG degrees were all in cognitive neuroscience, cognitive psychology, or strongly related fields.

On arrival, all participants completed a questionnaire assessing their personality, measured by a self-report Big Five measure from the International Personality Item Pool (IPIP).

Then, participants from each education group were equally assigned into one of three neuroscience information conditions: 90 participants to the "no neuroscience" condition, 90 to "text neuroscience", and 90 to "visual neuroscience". In the "no neuroscience" condition, no neuroscience information (i.e., no brain-related information in any format) was provided. In the "text neuroscience" condition, a description of the brain areas involved in the phenomenon was added to each explanation. In the "visual neuroscience" condition, an image showing the brain area(s) involved in the phenomenon was added to each explanation.  

After being allocated to a neuroscience condition, participants were shown to the room in which they were to complete the test measures. In this testing block, participants judged the accuracy of 10 bad (i.e., inaccurate) explanations of psychological phenomena on a scale of 0 (inaccurate) to 10 (accurate). Each participants’ accuracy judgements across the 10 explanations were then summed up, which gives us an accuracy score from 0 to 100 for each person.


## Research questions

We want to know:

a) After accounting for differences in personality traits, do any of the differences in accuracy ratings between neuroscience conditions depend on level of education?

Based on the above model, we also wanted to know:

b) Do differences in accuracy ratings between the visual and text-based neuroscience conditions differ between people who have a degree (either UG or PG) and people without a degree?

## Data dictionary

```{r echo=FALSE, eval=TRUE}
library(tidyverse)

tibble(
  variable = names(read_csv("https://uoepsy.github.io/data/neuro_explanations.csv")),
  description = c(
    "Participant identifier",
    "Education level (ND = no degree; UG = undergraduate degree; PG = postgraduate degree)",
    "Neuroscience condition (None = no neuroscience; Desc = text description; Pic = visual presentation)",
    "Score on 10 personality items assessing Openness from the IPIP Big Five Measure (z-scored)",
    "Score on 10 personality items assessing Conscientiousness from the IPIP Big Five Measure (z-scored)",
    "Score on 10 personality items assessing Extraversion from the IPIP Big Five Measure (z-scored)",
    "Score on 10 personality items assessing Agreeableness from the IPIP Big Five Measure (z-scored)",
    "Score on 10 personality items assessing Neuroticism from the IPIP Big Five Measure (z-scored)",
    "Accuracy judgement summed across 10 explanations (0 to 100)")
) %>% knitr::kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped")
```

