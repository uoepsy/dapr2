---
title: "Bootstrapping"
editor_options: 
  chunk_output_type: console
format:
  revealjs:
    smaller: true
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')
```


# Course Overview

<br>

:::: {.columns}

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week = 9)
```

:::

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block3_name,block4_name,block3_lecs,block4_lecs,week = 0)
```

:::

::::

## This week's learning objectives

<br>

::: {.fragment}
::: {.dapr2callout}
x
:::
:::

::: {.fragment}
::: {.dapr2callout}
x
:::
:::

::: {.fragment}
::: {.dapr2callout}
x
:::
:::

## What's a bootstrap?

![](figs/TODO-bootstrap.jpeg)

## What do bootstraps have to do with stats?

<br>

**Unclear.**

We'll revisit this question later once we've seen the method in action.


# Background


## Motivation: Why bootstrap?

<!-- intuitions borrowed from nice SO post: https://stats.stackexchange.com/a/26093 -->

- we want to know something about a population, but that's impossible, we only have a sample
- so we ask a question of the sample and get a single answer
- do we think this answer is representative of the population overall?

- one solution: take many samples from the population, ask question of them, and get a sense of the variability
- but we only have this one sample
- so our chosen solution: we'll treat our sample as if it is the population, and take many samples from *it* 

- this is an OK thing to do for a few reasons
  - the sample contains the best possible information we can get about a population (basically anything we guessed would be worse)
  - if samples are randomly chosen, they'll probably look quite a bit like the population
  - so consequently, it's likely that our current sample looks like the population too

- sampling w replacement is a way to treat our sample like it's a population, mathematically speaking


## What is bootstrapping good for?

Lots of things, but our main use case will be dealing with situations when assumptions about errors not met.


How do linear models typically estimate the standard error and confidence intervals of model coefficients?

 - With maths. Formulae are a little bit familiar by now.
 - This maths is relatively simple because it makes simplifying assumptions about the data: namely, that the errors are normally distributed and have equal variance.

But what if they don't?
Then those formulae will all be bad estimates of the actual variability in the population.

Enter: bootstrapping.



## Concretely: How bootstrap?

- process of repeatedly sampling w replacement
- computing summary statistic for ea sample




# Bootstrapping demo


## What just happened?

![](figs/TODO-boot-schematic.jpeg)



## We used bootstrapping to find the 95% CI of the mean

- SE of mean = SD of bootstrap means
- 95% CI = 2.5th percentile up to 97.5th percentile

But there's nothing special about the mean.
We can use bootstrapping for any summary statistic.

Including linear model parameters.


# Bootstrapping linear model parameters

- show data/model where assumptions are violated (can ask for votes on which assumptions)



## Manual bootstrapping 


- repeatedly sample + fit model + save coef estims
- show distribution


```{r bootstrap 100 samples, include=F, message=F}
# Create original sample
shapes <- .3
set.seed(1)
beta_errors <- rbeta(101, shape1 = shapes, shape2 = shapes) |> datawizard::standardize()
error_inc_terms <- seq(.5, 3, length.out = 101)
orig_data <- tibble(
  x = seq(-2, 2, length.out = 101),
  y = 2 + (-1.1 * x) + beta_errors * error_inc_terms,
) |>
  rowid_to_column()

boot_sample_size <- nrow(orig_data)
n_resamples <- 100

draw_boot_samples <- function(dataset, resample_size, n_resamples){
  # dataset: dataframe or tibble
  # resample_size: integer, size of the sample to draw
  # n_resamples: integer, number of bootstrap samples to draw
  # returns list with each element a subset of dataset
  
  boot_accum <- list()
  for(i in 1:n_resamples){
    boot_accum[[i]] <- dataset |>
      slice_sample(n = resample_size, replace = TRUE)
  }
  return(boot_accum)
}

plot_boot_sample <- function(boot_sample){
  # boot_sample: dataset, outcome of draw_boot_sample
  boot_sample |>
    ggplot(aes(x = x, y = y)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = 'lm', se = FALSE) +
    xlim(-2, 2) +
    NULL
  
}

set.seed(1)
boot_samples <- draw_boot_samples(orig_data, boot_sample_size, n_resamples)
boot_plot_list <- lapply(boot_samples, plot_boot_sample)
```


```{r plot boot samples, echo=F}
wrap_plots(boot_plot_list[1:6])
```


---


```{r boot lm coefs, echo=F}
fit_lm <- function(boot_sample){
  # boot_sample: df with cols x, y
  # returns coefs of LM fit to boot_sample
  
  m <- lm(y ~ x, data = boot_sample)
  return(m$coefficients)
}

boot_coefs <- lapply(boot_samples, fit_lm)
boot_coefs_df <- boot_coefs |>
  bind_rows(.id = 'boot_idx') |>
  rename(Intercept = `(Intercept)`)
```


```{r make gifs, echo=F, eval=F, message=F, warning=F}
plot_limits <- list(
  'Intercept' = c(1.51, 2.71),
  'x' = c(-1.29, -0.33)
)

gen_boot_plots <- function(all_boot_data, plot_limits, param_to_plot){
  # all_boot_data: dataset of the full sampling distribution
  #                must contain col boot_idx
  # plot_limits: list with upper and lower bounds of xlim for each param
  # param_to_plot: string, the parameter we want to get the samples of
  
  n_boot_samples <- nrow(all_boot_data)
  
  for(i in 1:n_boot_samples){
    
    curr_data <- all_boot_data |>
      slice_head(n = i) |>
      select(boot_idx, all_of(param_to_plot))
    
    curr_mean <- curr_data |> pull(param_to_plot) |> mean()
    curr_sd <- curr_data |> pull(param_to_plot) |> sd()
    
    # Make current plot
    p <- curr_data |>
      ggplot(aes_string(x = param_to_plot)) +
      geom_histogram(fill = 'darkgrey', bins = 30) +
      geom_vline(xintercept = curr_mean, linewidth = 2, colour = 'black') +
      labs(
        title = paste0(param_to_plot, '\nNumber of bootstrapped samples = ', i),
        x = 'Mean of bootstrapped samples',
        subtitle = paste0(
          'Mean of sampling dist. = ', round(curr_mean, 3), 
          '\nSD of sampling dist. = standard error = ', round(curr_sd, 3))
      ) +
      geom_vline(
        xintercept = curr_mean + curr_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
      ) +
      geom_vline(
        xintercept = curr_mean - curr_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
      ) +
      xlim(
        plot_limits[[param_to_plot]][1],
        plot_limits[[param_to_plot]][2]
      ) +
      scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
      NULL
    
    print(p)
  }
}

# Intercept GIF
gifski::save_gif(
  gen_boot_plots(boot_coefs_df, plot_limits, 'Intercept'), 
  'figs/boot-int.gif', 
  width = 600, 
  height = 720, 
  res = 144, 
  delay = 0.2
)

# Slope GIF
gifski::save_gif(
  gen_boot_plots(boot_coefs_df, plot_limits, 'x'), 
  'figs/boot-slp.gif', 
  width = 600, 
  height = 720, 
  res = 144, 
  delay = 0.2
)
```

<!-- TODO include gifs -->


## Automatic bootstrapping

```{r}
m_viol <- lm(y ~ x, data = orig_data)
m_viol_boot <- car::Boot(m_viol, R = 1000)
summary(m_viol_boot)
```




## Interpreting the results of a bootstrapped linear model











## What do bootstraps have to do with stats?

<br>

Bradley Efron is the inventor of bootstrapping, and he introduced it in his 1979 paper by saying it's a method

![](figs/efron-obv.png)

(Reader, the reasons did not become obvious.)

<br>

The internet's best guess: it has to do with the saying **"pull yourself up by your bootstraps"**.

- Previously, this saying meant "a pointless/useless act".
- These days, people use it to mean that 
  - you're a self-starter, 
  - you're independent, 
  - you're standing on your own two feet.
- We think that "bootstrapping" is used in that sense: **you can get estimates of uncertainty from just a single sample, so in a way, your estimate is "pulling itself up by its bootstraps".**















## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-09-0.svg){.fragment height="550" }

![](figs/block2-flowchart-09-1.svg){.fragment height="550" }
:::





## Revisiting this week's learning objectives

<br>

::: {}
::: {.dapr2callout}
**What is bootstrapping?**

::: {style="font-size: 80%;"}
- An alternative way of estimating the variability of a coefficient estimate (i.e., the standard error and confidence interval).
- Unlike regular linear models, bootstrapping doesn't require that errors be normally distributed or equal across the range of the predictor.
:::

:::
:::


::: {}
::: {.dapr2callout}
**How does bootstrapping work?**

::: {style="font-size: 80%;"}
- It is based on repeatedly resampling from a given sample and then computing summary statistics on those samples.
- By combining those summary values, we get a sampling distribution: our best guess about how that statistic is distributed in the population.
:::

:::
:::


## Revisiting this week's learning objectives

<br>

::: {}
::: {.dapr2callout}
**When would we bootstrap a linear model's estimates?**

::: {style="font-size: 80%;"}
- When a linear model's residuals don't fulfil the model's assumptions about normality of errors and equal variance of errors.
- Those assumptions are what makes the formulae for estimating standard error and confidence intervals work.
- When those assumptions aren't met, then we need a different way of estimating standard error and CIs: bootstrapping.
:::

:::
:::



## This week 

<br>

:::: {.columns}
::: {.column width="50%"}

### Tasks

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

:::
::: {.column width="50%"}

### Support

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

:::
::::

<br>

:::: {.columns}
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

:::
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

:::
::::







