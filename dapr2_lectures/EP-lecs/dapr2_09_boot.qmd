---
title: "Bootstrapping and <br> confidence intervals"
editor_options: 
  chunk_output_type: console
format:
  revealjs:
    smaller: true
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')

dapr2red <- "#BF1932" 
```


# Course Overview

<br>

:::: {.columns}

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week = 9)
```

:::

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block3_name,block4_name,block3_lecs,block4_lecs,week = 0)
```

:::

::::

## This week's learning objectives

<br>

::: {style="font-size: 125%;"}

::: {.fragment}
::: {.dapr2callout}
What is bootstrapping?
:::
:::

::: {.fragment}
::: {.dapr2callout}
How does bootstrapping work?
:::
:::

::: {.fragment}
::: {.dapr2callout}
When would we bootstrap a linear model's estimates?
:::
:::

::: {.fragment}
::: {.dapr2callout}
What does "confidence" in "95% confidence interval" actually mean?
:::
:::

:::

## What's a bootstrap?

:::{.fragment}

![](figs/bootstrap.svg){fig-align="center" height="500"}

:::

<br>

:::{.fragment}

What do bootstraps have to do with stats?

**Unclear.**

<br>

We'll revisit this question later once we've seen the method in action.

:::


# Background


## What problem does bootstrapping solve?
<!-- intuitions borrowed from nice SO post: https://stats.stackexchange.com/a/26093 -->

<br>

The core challenge in statistics: 

**We want to know something about a population, but we can't possibly study the whole population.**

**We can only draw a sample from it.**


<br>


We ask a question of the sample (e.g., what's the mean?).
And we get a single answer (e.g., 10).

**How do we know if this answer is representative of the population overall?**

<br>

Ideally, we would take many samples from the population, get their means, and compare them all.

**But we are constrained to this one sample.**

<br>

:::: {.columns}
::: {.column width="50%"}
:::{.dapr2callout}

One strategy: **Use mathematical tools** like the formula you learned for the standard error.

But those tools only give valid information if the assumptions behind them are met (e.g., that errors are normally distributed).

Otherwise, they give bad estimates of the actual variability in the population.

:::

:::
::: {.column width="50%"}
:::{.dapr2callout}

Another strategy: **Treat the sample we've drawn as if it is a population,** and draw many new samples from *it*.

This strategy doesn't rely on mathematical assumptions.

This method is called **bootstrapping.**

:::

:::
::::

## Other problems that bootstrapping can solve

<br>

1. When **samples are very small,** uncertainty around parameters like the mean will be high.
Bootstrapping can help us get more precise estimates of the parameters, though it assumes that our original sample will accurately represent the population (a shaky assumption when the samples are small!).

<br>

2. Context: Linear models can test whether a coefficient's value is different from zero because we have a measure of difference (the t value) and we know how this statistic is distributed when the H0 is true.
So: **If we don't know the sampling distribution of the measure we're using when the H0 is true**, then we can use bootstrapping to estimate the null distribution of whatever measure we're using.




## Bootstrapping: Resampling from a sample

![](figs/boot-schematic.svg){fig-align="center" height="600"}

## Why is bootstrapping an OK thing to do?

<br>

Our main sample contains the best possible information we can get about a population.
(Basically anything that we guessed would be worse.)

If our main sample was drawn at random, it will probably look quite a bit like the population.

So, drawing bootstrap samples from our main sample and summarising them will probably create a sampling distribution that looks a lot like the one we *would* get *if* we could sample repeatedly from the whole population.


## Generate bootstrapped samples by sampling with replacement

Imagine our main sample contains five shapes:

:::{.hcenter}
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
:::

:::: {.columns}
::: {.column width="50%"}

If we could only sample each data point once:

:::{.hcenter}

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">


Each sample is just a shuffled version of the original.
:::

:::
::: {.column width="50%"}

If we could sample each data point repeatedly:


:::{.hcenter}

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--square-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--change-history-outline.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">

<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--circle-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--favorite-outline.svg">
<img style="vertical-align: text-bottom; width: 2em;" src="figs/material-symbols--kid-star-outline-sharp.svg"> 


Each sample is distinct!

:::


:::
::::

**To create diverse samples, we need to be able to sample each data point repeatedly.**

This is called **"sampling with replacement"**.



# Bootstrapping demo

## Activity: Bootstrapping our own samples

<br>

**Population:** All numbers from 1 to 100.

**Main sample (size 10):**

:::{.hcenter style="font-size: 125%;"}

6 &nbsp;&nbsp;&nbsp; 8 &nbsp;&nbsp;&nbsp; 17 &nbsp;&nbsp;&nbsp; 32  &nbsp;&nbsp;&nbsp; 70 &nbsp;&nbsp;&nbsp; 76  &nbsp;&nbsp;&nbsp; 79  &nbsp;&nbsp;&nbsp; 81  &nbsp;&nbsp;&nbsp; 85  &nbsp;&nbsp;&nbsp; 93

:::

<br>

**Bootstrapped samples (size 10):** Over to you!

:::{.hcenter}

![](figs/boot-qualtrics-QR.svg){width=400px}

[https://edin.ac/4nTcnMP](https://edin.ac/4nTcnMP){target=_blank}

:::


## What just happened?

![](figs/boot-schematic.svg){fig-align="center" height="600"}



## Estimating variability using the sampling distribution

<br>

- For each bootstrapped sample, we compute a summary statistic.
- We represent all those summary statistics together using a **sampling distribution**.
- A sampling distribution is useful because with it, **we can estimate a parameter's variability** (i.e., standard error, confidence intervals).

<br>

:::{.dapr2callout}
**The standard error of the parameter = ** the standard deviation of the sampling distribution.
:::


:::{.dapr2callout}
**The 95% CI of the parameter =** the range from the 2.5th percentile to the 97.5th percentile of the sampling distribution.

- 2.5th percentile = 2.5% of values in the distribution are below this value
- 97.5th percentile = 97.5% of values in the distribution are below this value
:::


## Any summary statistic can be bootstrapped

<br>

There's nothing special about the mean.

We can use bootstrapping for any measure that summarises data points.

<br>

Most usefully for us: **We can bootstrap the parameters of a linear model.**


# Bootstrapping linear model parameters

## Which model assumptions are not met?

<br>

:::: {.columns}
::: {.column width="60%"}

```{r create orig sample, include=F, message=F}
shapes <- .3
set.seed(1)
beta_errors <- rbeta(101, shape1 = shapes, shape2 = shapes) |> datawizard::standardize()
error_inc_terms <- seq(.5, 3, length.out = 101)
orig_data <- tibble(
  x = seq(-2, 2, length.out = 101),
  y = 2 + (-1.1 * x) + beta_errors * error_inc_terms,
) |>
  rowid_to_column()
```

```{r echo=F, fig.width=10}
orig_data |>
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 5)
```
:::
::: {.column width="40%"}

<br>
<br>

 <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg"> &nbsp;  **L**inearity?

<br>

<img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg"> &nbsp; **I**ndependent errors?

<br>

<img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg"> &nbsp; **N**ormally-distributed errors?

<br>

<img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg"> &nbsp; **E**qual variance of errors?
:::
::::


## Check: Linearity

<br>

Use `geom_smooth()` to plot a straight line (`method = 'lm'`) and a curvy line (`method = 'loess'`).
The curvy line should match the straight line.

```{r fig.align = 'center', echo=F, fig.asp = .6, fig.width = 10}
orig_data |>
  ggplot(aes(x=x, y=y)) +
  geom_point(size = 5) +
  geom_smooth(method = 'lm', colour = 'blue', se = FALSE, linewidth = 3) +
  geom_smooth(method = 'loess', colour = 'red', se = FALSE, linewidth = 3)
```

Not perfect, but relatively good.



## Check: Normally-distributed errors

<br>

With a Q-Q plot, compare the actual standardised residuals from the model (y axis) to the ones we'd expect, if errors were perfectly normally distributed (x axis).

```{r include=F}
m1 <- lm(y ~ x, data = orig_data)
```

```{r echo=F, fig.asp = .7, fig.width = 8, fig.align = 'center'}
plot(m1, which = 2, cex.axis = 1.5, cex.lab=1.5)
```

Big divergences toward the extremes: there are more large residuals in our data than a normal distribution would have.


## Check: Equal variance of errors

<br>

With a plot of residuals vs. fitted values, check whether the residuals (y axis) are similar across all of the fitted/predicted values (x axis).

```{r echo=F, fig.asp=.7, fig.width=7, fig.align = 'center'}
plot(m1, which = 1, cex.axis = 1.5, cex.lab=1.5)
```

There's a definite cone-shape here (ideally we'd want a random-looking cloud), so residuals do not have equal variance.


## The verdict: uh oh

<br>

Bootstrapping can't make a maybe-linear effect more linear.

<br>

But it *can* help us sidestep the problems that arise from non-normal residuals and unequal variance of residuals.

**When residuals are not normal and when they have unequal variance, then the standard error that a linear model gives us is a bad estimate of the actual variability**.

<br>

So instead: we estimate the variability using bootstrapping.


# Bootstrapping, step by step

---

## A note

<br><br><br>

:::{.dapr2callout}

**You will never need to do bootstrapping step by step, the way I'm illustrating it here.**

I am showing you the steps to help you understand what's happening behind the scenes when you run the simple code you'll see later.

:::

```{r bootstrap 100 samples, include=F, message=F}
boot_sample_size <- nrow(orig_data)
n_resamples <- 100

draw_boot_samples <- function(dataset, resample_size, n_resamples){
  # dataset: dataframe or tibble
  # resample_size: integer, size of the sample to draw
  # n_resamples: integer, number of bootstrap samples to draw
  # returns list with each element a subset of dataset
  
  boot_accum <- list()
  for(i in 1:n_resamples){
    boot_accum[[i]] <- dataset |>
      slice_sample(n = resample_size, replace = TRUE)
  }
  return(boot_accum)
}

plot_boot_sample <- function(boot_sample){
  # boot_sample: dataset, outcome of draw_boot_sample
  boot_sample |>
    ggplot(aes(x = x, y = y)) +
    geom_point(alpha = 0.3, size = 5) +
    # geom_smooth(method = 'lm', se = FALSE) +
    xlim(-2, 2) +
    NULL
  
}

set.seed(1)
boot_samples <- draw_boot_samples(orig_data, boot_sample_size, n_resamples)
boot_plot_list <- lapply(boot_samples, plot_boot_sample)
```


```{r boot lm coefs, echo=F}
fit_lm <- function(boot_sample){
  # boot_sample: df with cols x, y
  # returns coefs of LM fit to boot_sample
  
  m <- lm(y ~ x, data = boot_sample)
  return(m$coefficients)
}

boot_coefs <- lapply(boot_samples, fit_lm)
boot_coefs_df <- boot_coefs |>
  bind_rows(.id = 'boot_idx') |>
  rename(Intercept = `(Intercept)`)
```


## Bootstrapping, step by step

1. From the main sample, take a new sample with replacement. You always sample the same number of data points as there were in the main sample. One sample bootstrapped.

2. Fit the linear model to the bootstrapped sample. Save the intercept and slope estimates.

3. Do 1 and 2 again and again. We'll get a sampling distribution of intercept estimates and a sampling distribution of slope estimates.


:::: {.columns}
::: {.column width="5%"}
**(1)**
:::
::: {.column width="35%"}
```{r echo=F, warning = F, message = F, fig.width = 5}
boot_plot_list[[1]]  +
  ylim(-3.5, 5.5)
```
:::
::: {.column width="5%"}
**(2)**
:::
::: {.column width="55%"}
```{r echo=F}
lm(y ~ x, data = boot_samples[[1]])
```
:::
::::

:::: {.columns}
::: {.column width="5%"}
**(3)**
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(1) |>
  ggplot(aes(x = Intercept)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Intercept',
    x = 'Bootstrapped sample intercepts',
  ) +
  xlim( 1.51, 2.71 ) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(1) |>
  ggplot(aes(x = x)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Slope',
    x = 'Bootstrapped sample slopes',
  ) +
  xlim(-1.29, -0.33) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::::



## Bootstrapping, step by step (Sample 2)

1. From the main sample, take a new sample with replacement. You always sample the same number of data points as there were in the main sample. One sample bootstrapped.

2. Fit the linear model to the bootstrapped sample. Save the intercept and slope estimates.

3. Do 1 and 2 again and again. We'll get a sampling distribution of intercept estimates and a sampling distribution of slope estimates.


:::: {.columns}
::: {.column width="5%"}
**(1)**
:::
::: {.column width="35%"}
```{r echo=F, warning = F, message = F, fig.width = 5}
boot_plot_list[[2]]  +
  ylim(-3.5, 5.5)
```
:::
::: {.column width="5%"}
**(2)**
:::
::: {.column width="55%"}
```{r echo=F}
lm(y ~ x, data = boot_samples[[2]])
```
:::
::::

:::: {.columns}
::: {.column width="5%"}
**(3)**
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(2) |>
  ggplot(aes(x = Intercept)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Intercept',
    x = 'Bootstrapped sample intercepts',
  ) +
  xlim( 1.51, 2.71 ) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(2) |>
  ggplot(aes(x = x)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Slope',
    x = 'Bootstrapped sample slopes',
  ) +
  xlim(-1.29, -0.33) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::::


## Bootstrapping, step by step (Sample 3)

1. From the main sample, take a new sample with replacement. You always sample the same number of data points as there were in the main sample. One sample bootstrapped.

2. Fit the linear model to the bootstrapped sample. Save the intercept and slope estimates.

3. Do 1 and 2 again and again. We'll get a sampling distribution of intercept estimates and a sampling distribution of slope estimates.


:::: {.columns}
::: {.column width="5%"}
**(1)**
:::
::: {.column width="35%"}
```{r echo=F, warning = F, message = F, fig.width = 5}
boot_plot_list[[3]] +
  ylim(-3.5, 5.5)
```
:::
::: {.column width="5%"}
**(2)**
:::
::: {.column width="55%"}
```{r echo=F}
lm(y ~ x, data = boot_samples[[3]])
```
:::
::::

:::: {.columns}
::: {.column width="5%"}
**(3)**
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(3) |>
  ggplot(aes(x = Intercept)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Intercept',
    x = 'Bootstrapped sample intercepts',
  ) +
  xlim( 1.51, 2.71 ) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::: {.column width="47%"}
```{r echo=F, fig.width = 7, fig.asp = .6, fig.align='center'}
boot_coefs_df |>
  head(3) |>
  ggplot(aes(x = x)) +
  geom_histogram(bins = 30) +
  labs(
    subtitle = 'Slope',
    x = 'Bootstrapped sample slopes',
  ) +
  xlim(-1.29, -0.33) +
      theme(panel.grid.minor = element_blank()) +
  scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
  NULL
```
:::
::::


## Sampling distribution of the intercept

```{r make gifs, echo=F, eval=F, message=F, warning=F}
plot_limits <- list(
  'Intercept' = c(1.51, 2.71),
  'x' = c(-1.29, -0.33)
)

gen_boot_plots <- function(all_boot_data, plot_limits, param_to_plot){
  # all_boot_data: dataset of the full sampling distribution
  #                must contain col boot_idx
  # plot_limits: list with upper and lower bounds of xlim for each param
  # param_to_plot: string, the parameter we want to get the samples of
  
  n_boot_samples <- nrow(all_boot_data)
  
  for(i in 1:n_boot_samples){
    
    curr_data <- all_boot_data |>
      slice_head(n = i) |>
      select(boot_idx, all_of(param_to_plot))
    
    curr_mean <- curr_data |> pull(param_to_plot) |> mean()
    curr_sd <- curr_data |> pull(param_to_plot) |> sd()
    
    # Make current plot
    p <- curr_data |>
      ggplot(aes_string(x = param_to_plot)) +
      geom_histogram(fill = 'darkgrey', bins = 30) +
      geom_vline(xintercept = curr_mean, linewidth = 4, colour = 'black') +
      labs(
        title = paste0(param_to_plot, '\nBootstrapped samples = ', i),
        x = 'Value from bootstrapped sample',
        subtitle = paste0(
          'Mean of sampling dist. = ', round(curr_mean, 3), 
          '\nSD of sampling dist. = standard error = ', round(curr_sd, 3))
      ) +
      theme(panel.grid.minor = element_blank()) +
      geom_vline(
        xintercept = curr_mean + curr_sd, linewidth = 2, colour = 'black', linetype = 'dotted'
      ) +
      geom_vline(
        xintercept = curr_mean - curr_sd, linewidth = 2, colour = 'black', linetype = 'dotted'
      ) +
      xlim(
        plot_limits[[param_to_plot]][1],
        plot_limits[[param_to_plot]][2]
      ) +
      scale_y_continuous(limits = c(0, 12), breaks = seq(0, 12, by = 2)) +
      NULL
    
    print(p)
  }
}

# Intercept GIF
gifski::save_gif(
  gen_boot_plots(boot_coefs_df, plot_limits, 'Intercept'), 
  'EP-lecs/figs/boot-int.gif', 
  width = 1400, 
  height = 1600, 
  res = 144, 
  delay = 0.25
)

# Slope GIF
gifski::save_gif(
  gen_boot_plots(boot_coefs_df, plot_limits, 'x'), 
  'EP-lecs/figs/boot-slp.gif', 
  width = 1400, 
  height = 1600,
  res = 144, 
  delay = 0.25
)
```



:::: {.columns}
::: {.column width="50%"}
![](figs/boot-int.gif)
:::
::: {.column width="50%"}

<br>
<br>
<br>

$\leftarrow$ This sampling distribution consists of every Intercept value estimated by a linear model that was fit to a bootstrapped sample of data.

<br>

As we draw more bootstrapped samples, the mean Intercept (solid line) will come to match the original sample's Intercept.

<br>

The standard deviation of the sampling distribution (dotted lines) = the standard error of the Intercept.

:::
::::

## Sampling distribution of the slope

:::: {.columns}
::: {.column width="50%"}
![](figs/boot-slp.gif)
:::
::: {.column width="50%"}


<br>
<br>
<br>

$\leftarrow$ This sampling distribution consists of every slope value estimated by a linear model that was fit to a bootstrapped sample of data.

<br>

As we draw more bootstrapped samples, the mean slope (solid line) will come to match the original sample's slope.

<br>

The standard deviation of the sampling distribution (dotted lines) = the standard error of the slope.

<br>

**Bootstrapping is a different way of estimating a parameter's standard error.**


:::
::::

# Bootstrapping a linear model in R


## Bootstrapping a linear model in R

<br>

Fit a standard linear model to the main sample.

```{r}
mod <- lm(y ~ x, data = orig_data)
```

<br>

Use `Boot()` from the package `car` to draw `R = 1000` bootstrapped samples.

By default, the bootstrapped samples will contain the same number of data points as the original sample.

```{r}
mod_boot <- car::Boot(mod, R = 1000)
```

<br>

Now `summary()` will give us a standard error for each parameter (but we know this is just the standard deviation of each parameter's sampling distribution).


```{r}
summary(mod_boot)
```



## Manual bootstrapping closely matches `Boot()`

```{r bootstrap 1000 samples, include=F, message=F}
n_resamples <- 1000

set.seed(1)
boot_samples_1k <- draw_boot_samples(orig_data, boot_sample_size, n_resamples)
boot_coefs_1k <- lapply(boot_samples_1k, fit_lm)
boot_coefs_df_1k <- boot_coefs_1k |>
  bind_rows(.id = 'boot_idx') |>
  rename(Intercept = `(Intercept)`)

```

```{r echo=F}
summary(mod_boot)
```

<br>

:::: {.columns}
::: {.column width="50%"}

```{r echo=F, fig.width = 7, fig.asp = 1.2, fig.align='center'}
boot_int_mean <- mean(boot_coefs_df_1k$Intercept)
boot_int_sd <- sd(boot_coefs_df_1k$Intercept)

boot_coefs_df_1k |>
  ggplot(aes(x = Intercept)) +
  geom_histogram(fill = 'darkgrey', bins = 30) +
  geom_vline(xintercept = boot_int_mean, linewidth = 2, colour = 'black') +
  labs(
    title = 'Intercept',
    subtitle = paste0('1000 manually bootstrapped samples\nMean = ', round(boot_int_mean, 3), '\nSD = ', round(boot_int_sd, 3)),
    x = 'Bootstrapped sample intercepts'
  ) +
  theme(
    panel.grid.minor = element_blank(),
    plot.subtitle = element_text(size = 24, hjust = 0)
    ) +
  geom_vline(
    xintercept = boot_int_mean + boot_int_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
  ) +
  geom_vline(
    xintercept = boot_int_mean - boot_int_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
  ) +
  xlim( 1.41, 2.81 ) +
  ylim(0, 130) +
  NULL
```


:::
::: {.column width="50%"}
```{r echo=F, fig.width = 7, fig.asp = 1.2, fig.align='center'}
boot_slp_mean <- mean(boot_coefs_df_1k$x)
boot_slp_sd <- sd(boot_coefs_df_1k$x)

boot_coefs_df_1k |>
  ggplot(aes(x = x)) +
  geom_histogram(fill = 'darkgrey', bins = 30) +
  geom_vline(xintercept = boot_slp_mean, linewidth = 2, colour = 'black') +
  labs(
    title = 'Slope',
    subtitle = paste0('1000 manually bootstrapped samples\nMean = ', round(boot_slp_mean, 3), '\nSD = ', round(boot_slp_sd, 3)),
    x = 'Bootstrapped sample slopes'
  ) +
  theme(
    panel.grid.minor = element_blank(),
    plot.subtitle = element_text(size = 24, hjust = 0)
    ) +
  geom_vline(
    xintercept = boot_slp_mean + boot_slp_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
  ) +
  geom_vline(
    xintercept = boot_slp_mean - boot_slp_sd, linewidth = 1, colour = 'black', linetype = 'dotted'
  ) +
  xlim(-1.49, -0.13) +
  ylim(0, 130) +
  NULL
```

:::
::::


## Getting our 95% confidence intervals

<br>

```{r}
confint(mod_boot, type = 'perc')
```

<br>

**Hypothesis tests:**

Because these 95% confidence intervals don't include 0,

- we can reject the H0 that the Intercept = 0 at $\alpha$ = 0.05.
- we can reject the H0 that the slope = 0 at $\alpha$ = 0.05.



## Reporting results of a bootstrapped linear model

<br>

Report parameter estimates the same way as usual, but make sure to include something like

<br>

> Because the assumptions of normality of errors and equal variance of errors were not met, all the standard errors and 95% CIs we report here were estimated from 1000 bootstrapped samples, using the `Boot()` function from the `car` package (Fox & Weisberg, 2019).

<br>

The citation:

- Fox, J., & Weisberg, S. (2019). *An R companion to applied regression* (3rd edn). Sage. <https://www.john-fox.ca/Companion/>

^ These are the people who developed the `car` (**C**ompanion to **A**pplied **R**egression) package.

# So, what do bootstraps have to do with stats?


## What do bootstraps have to do with stats?

<br>

Bradley Efron is the inventor of bootstrapping, and he introduced it in his 1979 paper by saying it's a method

![](figs/efron-obv.png)

(Reader, the reasons did not become obvious.)

<br>

The internet's best guess: it has to do with the saying **"pull yourself up by your bootstraps"**.

- Previously, this saying meant "a pointless/useless act".
- These days, people use it to mean that 
  - you're independent, 
  - you're a self-starter, 
  - you're standing on your own two feet.
- We think that "bootstrapping" is used in that sense: **you can get estimates of uncertainty from just a single sample, so in a way, your sample is "pulling itself up by its bootstraps".**


# Confidence interval refresher

## Interpreting confidence intervals

<br>

It's common to imagine that a 95% CI means that there's a 95% probability that our true value is in the interval.

```{r echo=F, fig.width=7, fig.align='center', fig.asp = .7}
nx <- seq(-4, 4, length.out = 1000)
p_cri <- tibble(
  x = nx,
  n_dens = dnorm(nx)
) |>
  ggplot(aes(x = x, y = n_dens)) +
  geom_line() +
  geom_ribbon(
    aes(
      x = ifelse(x >= -2 & x <= 2, x, NA), 
      ymin = 0,  ymax = n_dens), 
    alpha = 0.3, fill = dapr2red) +
  labs(
    y = '',
    x = ''
  ) +
  geom_errorbar(aes(xmin = -2, xmax = 2, y = .45), colour = dapr2red, width = 0.05) +
  geom_label(x = 0, y = .45, label = '95%', colour = dapr2red, size = 10) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    panel.grid = element_blank()
  ) +
  ylim(0, .48) +
  # scale_x_continuous(breaks = seq(-6, 6, by=2)) +
  NULL
p_cri
```

:::{.hcenter}
**But this isn't what they really mean! :(**
:::

## So what _do_ confidence intervals mean?

<br>

Confidence intervals rely on the idea of "hypothetical repeated sampling".

- **hypothetical:** we're imagining something that isn't really happening
- **repeated:** we're imagining that we're doing something over and over again
- **sampling:** we're imagining that we're drawing samples from the same population over and over again


<br>

Side note: Hypothetical repeated sampling is _different_ from bootstrapping because



- hypothetical repeated sampling **imagines** repeatedly drawing samples **from the population**
- bootstrapping **really** repeatedly draws samples **from our original sample**
 

## We can simulate hypothetical repeated sampling by defining our own "reality"

<br>

Let's look at the association between **hours of sleep** and the **number of riddles solved in an hour.**

In the real true population (which we have defined, in order to illustrate how confidence intervals work), <br> the true association looks like this:

```{r echo=F, fig.asp = 0.5, fig.width = 14}
true_int <- 30
true_slope <- 4

ggplot() +
  geom_abline(intercept = true_int, slope = true_slope, linewidth = 2) +
  geom_text(
    aes(x = 0, y = 94), 
    label = "n_riddles = 30 + (4 * hrs_sleep)",
    hjust = 0,
    size = 12
  ) +
  ylim(0, 100) +
  labs(
    y = 'Number of riddles solved',
    x = 'Hours of sleep',
    title = 'Real actual truth in reality'
  ) +
  scale_x_continuous(
    breaks = seq(0, 12, 2),
    limits = c(0, 12)
  )
```


## Repeatedly draw samples from that true population and fit a linear model to each

```{r message=F, include = F}
get_ci <- function(df){
  m <- lm(n_riddles ~ hrs_sleep, data = df)
  return(confint(m)['hrs_sleep',] |> as.data.frame())
}

n_obs <- 20
n_rptd_samples <- 100

set.seed(2)

df_ci2 <- tibble(
  sample_idx = rep(1:n_rptd_samples, each = n_obs),
  hrs_sleep = rep(
    seq(0, 12, length.out = n_obs),
    n_rptd_samples
  ),
  n_riddles = true_int + (true_slope * hrs_sleep) + rnorm(n_obs*n_rptd_samples, 0, 10)
) |>
  mutate(
    n_riddles = case_when(
      n_riddles < 0 ~ 0,
      n_riddles > 100 ~ 100,
      .default = n_riddles
    )
  )

df_confints <- df_ci2 |>
  group_by(sample_idx) |>
  group_map(~ get_ci(.x)) |>
  bind_cols() |>
  t() |>
  as.tibble() |>
  rownames_to_column(var = 'sample_idx') |>
  mutate(
    true_val_in_ci = (`2.5 %` < true_slope) & (`97.5 %` > true_slope)
  )

# df_confints |>
#   summarise(
#     sum(true_val_in_ci)
#   )  # it's 95
```

```{r echo=F, fig.align = 'center'}
df_confints <- df_confints |>
  mutate(
    pretty_printed = paste0('95% CI [', round(`2.5 %`, 1), ', ', round(`97.5 %`, 1), ']')
  )

first_sample <- 10
last_sample <- first_sample + 5

p_samples <- df_ci2 |>
  filter(sample_idx %in% first_sample:last_sample) |>
  ggplot(aes(x = hrs_sleep, y = n_riddles)) +
  geom_abline(intercept = true_int, slope = true_slope, colour = 'black', linetype = 'dotted') +
  geom_point(alpha = 0.5, size = 3) +
  geom_smooth(method = 'lm', se = F, linewidth = 2) +
  facet_wrap(~ factor(sample_idx)) +
  ylim(0, 100) +
  theme(
    legend.position = 'none',
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.spacing = unit(2, "lines")
  ) +
  scale_colour_manual(values = c(dapr2red, '#000')) +
  scale_x_continuous(
    breaks = seq(0, 12, 2),
    limits = c(0, 12)
  ) +
  NULL
p_samples
```


## Some 95% CIs of the slope don't contain the true population value of 4

```{r echo=F, fig.align = 'center'}
p_samples +
  geom_label(data = filter(df_confints, sample_idx %in% first_sample:last_sample), 
            aes(label = pretty_printed, colour = true_val_in_ci), 
            x = 0, y = 95, hjust = 0,
            size = 8) +
  NULL
```


## How many CIs contain the true value?

<br> 

If we're using a **95% CI**, then we know that under hypothetical repeated sampling:

- **95** out of every 100 CIs will contain the true population value.
- **5** of every 100 CIs will not.

```{r echo=F, fig.align = 'center', fig.asp = .5, fig.width = 12}
df_confints |>
  mutate(true_val = true_slope) |>
  ggplot(aes(x = sample_idx, y = true_val, group = 1)) +
  geom_line(linewidth = 1) +
  geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`, colour = true_val_in_ci), linewidth = 2) +
  theme(
    legend.position = 'bottom',
    axis.text.x = element_blank(),
    panel.grid = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_colour_manual(values = c(dapr2red, '#000')) +
  labs(
    y = 'Slope',
    x = 'Sample',
    colour = 'True value in 95% CI?'
  ) +
  ylim(.5, 7.5)
```



## How many CIs contain the true value?

<br> 

If we're using a **60% CI**, then we know that under hypothetical repeated sampling:

- **60** out of every 100 CIs will contain the true population value.
- **40** of every 100 CIs will not.

```{r include=F, message = F}
get_ci60 <- function(df){
  m <- lm(n_riddles ~ hrs_sleep, data = df)
  return(confint(m, level = 0.6)['hrs_sleep',] |> as.data.frame())
}

set.seed(2)

df_ci60 <- tibble(
  sample_idx = rep(1:n_rptd_samples, each = n_obs),
  hrs_sleep = rep(
    seq(0, 12, length.out = n_obs),
    n_rptd_samples
  ),
  n_riddles = true_int + (true_slope * hrs_sleep) + rnorm(n_obs*n_rptd_samples, 0, 10)
) |>
  mutate(
    n_riddles = case_when(
      n_riddles < 0 ~ 0,
      n_riddles > 100 ~ 100,
      .default = n_riddles
    )
  )

df_confints60 <- df_ci60 |>
  group_by(sample_idx) |>
  group_map(~ get_ci60(.x)) |>
  bind_cols() |>
  t() |>
  as.tibble() |>
  rownames_to_column(var = 'sample_idx') |>
  mutate(
    true_val_in_ci = (`20 %` < true_slope) & (`80 %` > true_slope)
  )
```


```{r echo=F, fig.align = 'center', fig.asp = .5, fig.width = 12}
df_confints60 |>
  mutate(true_val = true_slope) |>
  ggplot(aes(x = sample_idx, y = true_val, group = 1)) +
  geom_line(linewidth = 1) +
  geom_errorbar(aes(ymin = `20 %`, ymax = `80 %`, colour = true_val_in_ci), linewidth = 2) +
  theme(
    legend.position = 'bottom',
    axis.text.x = element_blank(),
    panel.grid = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_colour_manual(values = c(dapr2red, '#000')) +
  labs(
    y = 'Slope',
    x = 'Sample',
    colour = 'True value in 60% CI?'
  ) +
  ylim(.5, 7.5)
```


## So what is "confidence"?

<br>

"Confidence" refers to the process that generates the intervals.

- **A 95% confidence interval is generated using a process that, in the long run, makes 95 intervals that contain the true population value for every 5 intervals that do not.**

- In general: a $n$% confidence interval is generated using a process that, in the long run, makes $n$ intervals that contain the true population value for every 100 $-~n$ that do not.

<br>

This is why it's not true to say "there's a 95% probability that the true value is in the 95% CI".

- We have **no idea** whether any given confidence interval contains the true population value.
- Either it does contain the true value (then the probability = 100%) or it does not (probability = 0%).
- **For a given confidence interval, we have no way of knowing how likely each of those two outcomes is.**




## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-09-0.svg){.fragment height="550" }

![](figs/block2-flowchart-09-1.svg){.fragment height="550" }
:::





## Revisiting this week's learning objectives

<br>

::: {}
::: {.dapr2callout}
**What is bootstrapping?**

- An alternative way of estimating the variability of a coefficient estimate (i.e., the standard error and confidence interval).
- Unlike regular linear models, bootstrapping doesn't require that errors be normally distributed or equal across the range of the predictor.

:::
:::


::: {}
::: {.dapr2callout}
**How does bootstrapping work?**

- It is based on repeatedly resampling from a given sample and then computing summary statistics on those samples.
- By combining those summary values, we get a sampling distribution: our best guess about how that statistic is distributed in the population.

:::
:::


## Revisiting this week's learning objectives

<br>

::: {}
::: {.dapr2callout}
**When would we bootstrap a linear model's estimates?**

- When a linear model's residuals don't fulfil the model's assumptions about normality of errors and equal variance of errors.
- Those assumptions are what makes the formulae for estimating standard error and confidence intervals work.
- When those assumptions aren't met, then we need a different way of estimating standard error and CIs: bootstrapping.

:::
:::

::: {.dapr2callout}
**What does "confidence" in "95% confidence interval" actually mean?**

- "Confidence" is a property of the process that generates confidence intervals.
- A 95% confidence interval is generated by a process that produces 95 intervals that contain the true value for every 5 that do not.
- **95% confidence interval $\neq$ 95% probability that the interval contains the true value!!** 
  - Either the 95% CI contains the true value or it doesn't. We have no idea how probable each of those scenarios is.
:::

## This week 

<br>

:::: {.columns}
::: {.column width="50%"}

### Tasks

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

:::
::: {.column width="50%"}

### Support

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

:::
::::

<br>

:::: {.columns}
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

:::
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

:::
::::


# Appendix {.appendix} 

## 95% CIs from the sampling distribution's quantiles

```{r}
confint(mod_boot, type = 'perc')
```


The CIs from `Boot()` are close to the 2.5th and 97.5th quantiles of our manually created sampling distribution:

```{r}
quantile(
  boot_coefs_df_1k$Intercept,
  probs = c(0.025, 0.975)
)

quantile(
  boot_coefs_df_1k$x,
  probs = c(0.025, 0.975)
)
```





<!-- :::: {.columns} -->
<!-- ::: {.column width="50%"} -->
<!-- a -->
<!-- ::: -->
<!-- ::: {.column width="50%"} -->
<!-- b -->
<!-- ::: -->
<!-- :::: -->

<!-- style="font-size: 125%;" -->
