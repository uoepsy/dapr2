---
title: "Manual contrast interactions and multiple comparisons"
editor_options: 
  chunk_output_type: console
format:
  revealjs:
    smaller: true
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(emmeans)
dapr2red <- "#BF1932" 

theme_set(
  theme_minimal(
    base_size = 28
  )
)

hosp <- read_csv("data/hospital.csv") |>
  mutate(
    Treatment = case_when(
      Treatment == 'TreatA' ~ 'Meds',
      Treatment == 'TreatB' ~ 'EMDR',
      Treatment == 'TreatC' ~ 'CBT',
    ),
    Treatment = factor(Treatment),
    Hospital = factor(Hospital),
  )

# get group means
mean_Meds_1 <- mean(filter(hosp, Treatment == 'Meds', Hospital == 'Hosp1')$SWB)
mean_EMDR_1 <- mean(filter(hosp, Treatment == 'EMDR', Hospital == 'Hosp1')$SWB)
mean_CBT_1  <- mean(filter(hosp, Treatment == 'CBT',  Hospital == 'Hosp1')$SWB)
mean_Meds_2 <- mean(filter(hosp, Treatment == 'Meds', Hospital == 'Hosp2')$SWB)
mean_EMDR_2 <- mean(filter(hosp, Treatment == 'EMDR', Hospital == 'Hosp2')$SWB)
mean_CBT_2  <- mean(filter(hosp, Treatment == 'CBT',  Hospital == 'Hosp2')$SWB)
```


# Course Overview

<br>

:::: {.columns}

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block1_name = "Introduction to linear Models"
block1_lecs = c("Intro to linear regression",
                "Interpreting linear models",
                "Testing individual predictors",
                "Model testing & comparison",
                "Linear model analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical predictors and dummy coding",
                "Effect coding and manual post-hoc contrasts",
                "Assumptions and diagnostics",
                "Bootstrapping and confidence intervals",
                "Categorical predictors: Practice analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week = 0)
```

:::

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block3_name = "Interactions"
block3_lecs = c("Mean-centering and numeric/categorical interactions",
                "Numeric/numeric interactions",
                "Categorical/categorical interactions",
                "Manual contrast interactions and multiple comparisons",
                "Interactions: Practice analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power analysis",
                "Binary logistic regression I",
                "Binary logistic regression II",
                "Logistic regression: Practice analysis",
                "Exam prep and course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block3_name,block4_name,block3_lecs,block4_lecs,week = 4)
```

:::

::::


## This week's learning objectives

<br>

::::: {style="font-size: 125%;"}

<!-- :::: {.fragment} -->
:::: {}
::: {.dapr2callout}
How do we test an interaction between two manually-coded post-hoc contrasts?
:::
::::

<!-- :::: {.fragment} -->
:::: {}
::: {.dapr2callout}
What's the main danger of running multiple statistical tests on the same data?
:::
::::

<!-- :::: {.fragment} -->
:::: {}
::: {.dapr2callout}
What are two common ways that we can correct for running multiple statistical tests on the same data?
:::
::::

:::::


## Where we are in the analysis workflow

![](figs/block3-flowchart-04.svg){height="925" fig-align="center"}

# Interactions between manual post-hoc contrasts

## Today's data

**The subjective wellbeing (`SWB`) of patients at two different hospitals:**


:::: {.columns}
::: {.column width="50%"}
`Hosp1`:
<br>
![](figs/Hosp1.jpg){height="300"}
:::
::: {.column width="50%"}
`Hosp2`:
<br>
![](figs/Hosp2.jpg){height="300"}
:::
::::


**who have undergone one of three different treatments for depression:**

:::: {.columns}
::: {.column width="33%"}
Cognitive Behavioural Therapy (`CBT`)
<br>
![](figs/CBT.jpg){height="300"}
:::
::: {.column width="33%"}
Eye Movement Desensitisation and Reprocessing therapy (`EMDR`)
<br>
![](figs/EMDR.jpg){height="300"}
:::

::: {.column width="33%"}
Antidepressant medication <br> (`Meds`):
<br>
![](figs/Meds.jpg){height="300"}
:::
::::


## Visualising the data


<br>

:::: {.columns}
::: {.column width="70%"}
```{r echo=F, fig.align = 'center', fig.dim = c(12, 8)}
hosp |>
  ggplot(aes(x = Treatment, y = SWB, colour = Treatment, fill = Treatment)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 5) +
  facet_wrap(~ Hospital) +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = F) +
  theme(legend.position = 'none') +
  NULL
```
:::

::: {.column width="30%"}

::::{ style="font-size:85%;"}
```{r}
hosp |>
  head(12)
```
::::

:::

::::

What we want to know: **Are the associations between SWB and talk therapy (both `CBT` and `EMDR`) and pharmaceutical therapy (`Meds`) different at the two different hospitals?**

- We'll manually chunk CBT and EMDR together in a post-hoc contrast, which we'll call `TrtmtType`.
- Then we'll make a **post-hoc interaction contrast** to test whether `Hospital` interacts with `TrtmtType`.


## Fit the model

<br>

We only care about the post-hoc contrasts, so it's fine to fit the model using R's default treatment coding.

<br>

Check reference levels, so we can write our predictors out correctly:

:::: {.columns}
::: {.column width="50%"}
```{r}
contrasts(hosp$Treatment)
```
:::
::: {.column width="50%"}
```{r}
contrasts(hosp$Hospital)
```
:::
::::

<br>

$$
\begin{align}
\text{SWB} ~=~& \beta_0 + 
(\beta_1 \cdot \text{Treatment}_{\text{EMDR}}) +
(\beta_2 \cdot \text{Treatment}_{\text{Meds}}) +
(\beta_3 \cdot \text{Hospital}) ~ + \\
& (\beta_4 \cdot \text{Treatment}_{\text{EMDR}} \cdot \text{Hospital}) +
(\beta_5 \cdot \text{Treatment}_{\text{Meds}} \cdot \text{Hospital}) +
\epsilon
\end{align}
$$
<br>

```{r}
m1 <- lm(SWB ~ Treatment * Hospital, data = hosp)
```


## Get estimated marginal means

<br>

```{r}
m1_emm <- emmeans(m1, ~ Treatment * Hospital)
m1_emm
```

```{r echo=F, fig.align = 'center', fig.dim = c(14, 6)}
plot(m1_emm) + 
  coord_flip() +
  theme(text = element_text(size=28))
```


# Defining post-hoc contrasts

## Define post-hoc contrast for `TrtmtType`

:::: {.columns}
::: {.column width="35%"}
:::{.hcenter}

| `Treatment`  | `TrtmtType` |
| :----------- | ------------: |
| `CBT`        |          1/2  |
| `EMDR`       |          1/2  |
| `Meds`       |           –1  |
|||

:::
:::
::: {.column width="65%"}

:::{.dapr2callout}
**Step 1:** "Chunk" together the two group(s) that the research question is comparing.

- Chunk 1: `CBT`, `EMDR`.
- Chunk 2: `Meds`.
:::

:::{.dapr2callout}
**Step 2:** Assign a 0 to any group(s) that aren't in one of the chunks from Step 1.
:::


:::{.dapr2callout}
**Step 3:** Assign a plus sign to every group in Chunk 1, and a minus sign to every group in Chunk 2.
:::

:::{.dapr2callout}
**Step 4:** Count the plus signs and minus signs.
:::

:::{.dapr2callout}
**Step 5:** To figure out the actual values for each cell, start with 1 and –1.
Divide 1 by $n_{plus}$, and divide –1 by $n_{minus}$.
:::

:::{.dapr2callout}
**Step 6:** In the coding matrix, replace the plus signs with the positive coding value from Step 5, and replace the minus signs with the negative coding value from Step 5. **Done!**
:::

:::

::::

## Define post-hoc contrast for `Hospital`


:::: {.columns}
::: {.column width="35%"}
:::{.hcenter}

| `Hospital`  | `Hosp` |
| :---------- | -----: |
| `Hosp1`     |   1    |
| `Hosp2`     |  –1    |
|||

:::
:::
::: {.column width="65%"}

:::{.dapr2callout}
**Step 1:** "Chunk" together the two group(s) that the research question is comparing.

- Chunk 1: `Hosp1`.
- Chunk 2: `Hosp2`.
:::

:::{.dapr2callout}
**Step 2:** Assign a 0 to any group(s) that aren't in one of the chunks from Step 1.
:::


:::{.dapr2callout}
**Step 3:** Assign a plus sign to every group in Chunk 1, and a minus sign to every group in Chunk 2.
:::

:::{.dapr2callout}
**Step 4:** Count the plus signs and minus signs.
:::

:::{.dapr2callout}
**Step 5:** To figure out the actual values for each cell, start with 1 and –1.
Divide 1 by $n_{plus}$, and divide –1 by $n_{minus}$.
:::

:::{.dapr2callout}
**Step 6:** In the coding matrix, replace the plus signs with the positive coding value from Step 5, and replace the minus signs with the negative coding value from Step 5. **Done!**
:::

:::

::::

## Visualising the interaction we'll test

We want to know: Is there a difference between hospitals in how different treatment types (talk therapy vs. pharmaceutical therapy) are associated with SWB?

```{r echo=F, fig.dim = c(14, 8), fig.align = "center"}
hosp |>
  mutate(
    TrtmtType = ifelse(Treatment == 'Meds', 'PharmaTherapy', 'TalkTherapy')
  ) |>
  ggplot(aes(x = TrtmtType, y = SWB)) +
  geom_violin() +
  geom_jitter(aes(colour = Treatment), alpha = 0.5, size = 5) +
  facet_wrap(~ Hospital) +
  NULL
```



## Post-hoc contrasts in R

<br>


:::: {.columns}
::: {.column width="50%"}
::::{.hcenter}

| `Treatment`  | `TrtmtType` |
| :----------- | ------------: |
| `CBT`        |          1/2  |
| `EMDR`       |          1/2  |
| `Meds`       |           –1  |
|||

::::
:::

::: {.column width="50%"}
::::{.hcenter}

| `Hospital`  | `Hosp` |
| :---------- | -----: |
| `Hosp1`     |   1    |
| `Hosp2`     |  –1    |
|||

::::
:::
::::

<br>

```{r}
TrtmtType_contr <- c(
  'CBT'  = 1/2,
  'EMDR' = 1/2,
  'Meds' =  -1
)

Hosp_contr <- c(
  'Hosp1' =  1,
  'Hosp2' = -1
)
```



## The interaction's contrast is the product of the two interacting contrasts

<br>

In other words: To get the interaction's contrast, we multiply together the two contrasts for the interacting predictors.

<br>

| Treatment | Hospital | TrtmtType | Hosp | TrtmtType:Hosp |
| --------- | -------- | ----------: | ---: | ---------------: |
| CBT       | Hosp1    | 0.5         | 1    | 0.5              |
| EMDR      | Hosp1    | 0.5         | 1    | 0.5              |
| Meds      | Hosp1    | –1          | 1    | –1               |
| CBT       | Hosp2    | 0.5         | –1   | –0.5             |
| EMDR      | Hosp2    | 0.5         | –1   | –0.5             |
| Meds      | Hosp2    | –1          | –1   | 1                |


## Put the interaction's contrast into R

**(1) The fancy way:**

- Use `outer()` to multiply everything together, then
- "flatten" the resulting table using `as.vector()`.


```{r}
(interaction_contr <- outer(TrtmtType_contr, Hosp_contr) |> as.vector())
```

- **IMPORTANT: Inside `outer()`, put TrtmtType first, then Hosp second,** because that's the order that `emmeans` uses for their original variables (we wrote `Treatment` first, `Hospital` second).

Then format the contrast the way that `emmeans` expects:

```{r}
interaction_contr <- list('TreatmentType:Hospital' = interaction_contr)
```


<br>

**(2) The manual way** (matching the order of items by hand!):

```{r}
interaction_contr <- list( "TreatmentType:Hospital" =
    c(
        0.5,    # CBT  Hosp1 
        0.5,    # EMDR Hosp1
         -1,    # Meds Hosp1
       -0.5,    # CBT  Hosp2
       -0.5,    # EMDR Hosp2
          1     # Meds Hosp2
    )
)
```


## Test the interaction contrast


<br>

Test the contrast (i.e., test the H0 that the estimate is different from 0):

```{r}
(m1_ixn_test <- contrast(m1_emm, interaction_contr))
```

<br>

Get the associated 95% CIs:

```{r}
(m1_ixn_confint <- confint(m1_ixn_test))
```

<br>

Can we reject the null hypothesis that in both hospitals, the associations between subjective wellbeing and talk therapy, and subjective wellbeing and pharmaceutical therapy, are the same?



## Where does this estimate of –3.73 come from?


It's the result of multiplying each group mean by the corresponding value from the interaction's contrast, and then taking the sum.

```{r echo=F}
(hosp_manual <- hosp |>
  group_by(Treatment, Hospital) |>
  summarise(mean_SWB = mean(SWB)) |>
  arrange(Hospital) |>
  ungroup() |>
  mutate(
    `TreatmentType:Hospital` = c(0.5, 0.5, -1, -0.5, -0.5, 1),
    product  = mean_SWB * `TreatmentType:Hospital`
  )) |>
  kableExtra::kable(digits = 2)
```

<br>

```{r}
sum(hosp_manual$product)
```

<br>

Can we give this number a more intuitive explanation?

Not really tbh... 
**Interaction coefficients are adjustments to other effects.
But we don't have any other coefficients to relate this adjustment to, so it's not very useful to interpret this single number on its own.**

The reason manual interactions are useful is essentially just the significance test.



<!-- | Treatment | Hospital | Group mean SWB  |  TrtmtType:Hosp | Product | -->
<!-- | --------- | -------- | --------------: |  ---------------: | ------: | -->
<!-- | CBT       | Hosp1    | 10.10           |  0.5              |    5.05 | -->
<!-- | EMDR      | Hosp1    | 9.43            |  0.5              |    4.72 | -->
<!-- | Meds      | Hosp1    | 10.80           |  –1               |  –10.80 | -->
<!-- | CBT       | Hosp2    | 7.98            |  –0.5             |   –3.95 | -->
<!-- | EMDR      | Hosp2    | 13.12           |  –0.5             |   –6.56 | -->
<!-- | Meds      | Hosp2    | 7.85            |  1                |    7.85 | -->
<!-- | | | | | **Sum = –3.69**| -->

<!-- (–3.69 is slightly different from –3.73 because of error introduced by rounding to two decimal places.) -->

## Extending the analysis workflow

![](figs/block3-flowchart-05.svg){height="925" fig-align="center"}



# Testing differences between _all_ pairs of groups

## Testing differences between _all_ pairs of groups

<br>

![](figs/pairwise.svg){fig-align="center"}

<br>

We call these "pairwise comparisons", because we're comparing groups _in all possible pairs_.


## To test all pairwise comparisons

```{r}
m1_pairwise <- pairs(m1_emm)
```

```{r}
m1_pairwise
```


<br>

What's this "P value adjustment: tukey method" thing at the bottom?
We'll get there soon, but first...


## Elizabeth has opinions

<br>

I don't think testing all pairwise comparisons is a very good thing to do in practice. 
I don't recommend you use this in your research.

Why?
**Testing all pairwise comparisons makes it very easy to do bad science, even if you have good intentions.**

<br>

- Most of the differences on the last slide are significant.
- After seeing these results, it's very tempting to make up a story about why each specific one might be different, even though we didn't have any hypotheses about them before.
- Making up a story _after_ seeing the results is poor research practice. In the Open Science community, it's known as HARKing: **Hypothesising After Results are Known**. 
- HARKing is one reason that many published results are not replicable: they aren't real effects, just something that was significant in one study, and the researchers spun a story around it after finding the result. 

<br>

So why are we showing you this method?

Because it raises an issue that you _should_ be aware of: **the multiple comparisons problem**.


# The multiple comparisons problem

## Let's roll some dice

On a twenty-sided die (a d20), the probability of rolling a 1 (or any other number) is 1/20 = 5%.

But if we roll the d20 over and over again, the probability that we'll roll a 1 **at least once** goes up and up and up.

:::: {.columns}
::: {.column width="30%"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}
:::

::: {.column width="20%"}
$0.05$
:::

::: {.column width="50%"}
$= 1 – 0.95$
:::
::::



:::: {.columns}
::: {.column width="30%"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}
:::
::: {.column width="20%"}
$0.0975$
:::
::: {.column width="50%"}
$= 1 – (0.95 \times 0.95)$
:::

::::

:::: {.columns}
::: {.column width="30%"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}
![](figs/arcticons--rpg-simple-dice.svg){width="100"}

:::
::: {.column width="20%"}
$0.1426$
:::
::: {.column width="50%"}
$= 1 – (0.95 \times 0.95 \times 0.95) = 1 - 0.95^3$
:::

::::

:::{.dapr2callout}

In general:

$$
P(\text{rolling a 1 in m rolls}) = 1 - (1 - 0.05)^m
$$

:::

:::{.dapr2callout}
**What's the probability that we'll roll a 1 at least once in twenty rolls?**
:::




## From dice to statistical tests

**Why did we spend time rolling those dice?**

- Think about what $\alpha = 0.05$ means.
  - 5% of the time, we'll reject the null hypothesis when actually we shouldn't.
  - (aka a false positive result, aka a Type I error)
- Every time a d20 rolls a 1, that happens with the same probability as us rejecting the H0 when actually we shouldn't.
- And if we run 20 tests, say, then even though we only want to incorrectly reject the H0 one time out of twenty (5% of the time), we would actually reject it with a much greater probability.





## relevant XKCD (<https://xkcd.com/882/>)

![](figs/xkcd-significant-1.png)

---

![](figs/xkcd-significant-2.png)

---

![](figs/xkcd-significant-3.png)

---

![](figs/xkcd-significant-4.png)




## The more tests you run, the more likely you are to see something unlikely just due to chance

- With $\alpha = 0.05$, we accept the risk that, we will incorrectly reject the null hypothesis.

And if we accept that positive result and ignore all the true negatives (green jelly bean style), then we run into **the multiple comparisons problem: we are more than 5% likely to see at least one false positive, the more tests (aka, the more comparisons) we run.**
  
  
## How much more likely are we to see a false positive and incorrectly reject the null?

<br>

Depends on the number of tests.

$$
\begin{align}
P(\text{incorrectly reject H0}) &= \alpha \\
P(\text{correctly reject H0}) &= 1 - \alpha \\
P(\text{correctly reject H0 in} ~m~ \text{tests}) &= (1 - \alpha)^m \\
P(\text{incorrectly reject H0 in} ~m~ \text{tests}) &= 1 - (1 - \alpha)^m \\
\end{align}
$$

<br>

In the scenarios with the d20 and the green jelly beans:

$$
\begin{align}
P(\text{incorrectly reject H0 in 20 tests}) &= 1 - (1 - 0.05)^{20} \\
 &= 1 - (0.95)^{20} \\
 &= 1 - 0.36 \\
 &= 0.64 \\
\end{align}
$$

How do we fix it?

Lower the $\alpha$ level (aka the false positive rate, aka the Type I error rate) to compensate for the increased probability of incorrectly rejecting H0.


# Back to our data

## Back to our data

![](figs/pairwise.svg){fig-align="center"}

There are fifteen possible pairwise comparisons for our six combinations of predictor levels.

If we did not account for running all these tests together, we'd incorrectly reject the H0 with a probability of 

$$
\begin{align}
P(\text{incorrectly reject H0 in 15 tests}) &= 1 - (1 - 0.05)^{15} \\
 &= 1 - (0.95)^{15} \\
 &= 1 - 0.46 \\
 &= 0.54 \\
\end{align}
$$


## `pairs()` automatically corrects the p-values

<br>

```{r}
m1_pairwise
```

<br>

The "P value adjustment: tukey method" thing is telling us _how_ `pairs()` has corrected the p-values to account for the multiple comparisons problem.





## Common methods to adjust p-values

<br>

- Tukey
- Bonferroni
- Šídák
- Scheffe
- Holm's step-down
- Hochberg's step-up
- ...

<br>

We'll only focus on Tukey and Bonferroni, the two most common.
We'll start with Bonferroni.


## The Bonferroni correction

Our goal is to lower the $\alpha$ level of our tests, to match our desired level of risk.

The Bonferroni correction adjusts our original $\alpha$ in a very simple way: 

$$
\alpha_{\text{Bonferroni}} = \frac{\alpha}{\text{number of tests}}
$$

In our fifteen-test scenario: 

```{r}
0.05 / 15
```

Our new $\alpha$ (aka our new $p$-value threshold for statistical significance) is about 0.003.


## Bonferroni in action

<br>

```{r}
pairs(m1_emm, adjust = "bonferroni")
```






## TODO Tukey's Honest Significant Differences (HSD)

<br>

At heart, Tukey's HSD is very similar to a t-test of whether two group means are significantly different.

The two groups being compared are the members of one pair in the pairwise comparison.

Instead of producing a t statistic, the difference between group means are adjusted to produce a q statistic, which is compared against what's called a "studentised range distribution".




You do not need to know how to calculate this adjustment by hand.

If you *want* to know, here's [a YouTube video that walks through the calculation](https://www.youtube.com/watch?v=EBGnAJHs3G4&t=218s).
(The main action is between 3:41–7:42.)

Where Bonferroni sets a single new significance value that applies to every test, Tukey's HSD compares every pair of groups separately, so there is no single new $\alpha$ value.


## Tukey in action

<br>

```{r}
pairs(m1_emm, adjust = "tukey")
```


## When to choose Bonferroni vs. Tukey

<br>

**When to choose Bonferroni:**

- When you're interested in a small number of comparisons.
- When you've got a set of p-values that come from the same data, regardless of what kind of test generated them.

<br>

**When to choose Tukey:**

- When you want to compare the means of levels of a categorical predictor.
- When you're interested in all/most pairwise comparisons between groups.

Check [flashcards](https://uoepsy.github.io/dapr2/2526/labs/1_b3_reading.html) for more details.


# TODO Testing simple effects

## Testing simple effects

retrieval activity: what's a simple effect? if I ask for the simple effect for `Hospital` at each level of `Treatment`, what do I want?

 <!-- the difference between groups at a specific level of another predictor -->

## Using `pairs()` to test simple effects

last week we saw how to compute simple effects by hand

Now we can use the machinery `pairs()` to test whether the simple effects are significantly different from zero.

```{r}
pairs(m1_emm, simple = "Treatment")
```


## Simple effects of Hospital for each level of Treatment

```{r}
pairs(m1_emm, simple = "Hospital")
```


## A new way to visualise categorical interactions

Now that we've got an `emmeans` object for our interaction model, there's one more kind of plot that we can make, using `emmip()`:


```{r}
emmip(m1_emm, Treatment ~ Hospital, CIs = TRUE)
```


Pretty similar to `cat_plot()` from the `interactions` package, so pick your poison!


# Back matter

## Revisiting this week's learning objectives

::: {.dapr2callout}
**How do we test an interaction between two manually-coded post-hoc contrasts?**

- Manually code the interaction between them, using the same procedure that we saw for treatment-coded predictors last week: 
  - get all combinations of levels, and
  - multiply each pair of coding values together.
  - The result is the contrast coding of the interaction.
- Test that contrast using our `emmeans()` toolkit.
:::

::: {.dapr2callout}
**What's the main danger of running multiple statistical tests on the same data?**

- We'll end up rejecting the H0 at least once in all those tests with a probability much higher than our desired $\alpha$ level.
- This phenomenon is called the "multiple comparisons problem".
- The solution: Make the $\alpha$ level smaller, to compensate for this increased likelihood of rejecting the H0.
:::

::: {.dapr2callout}
**What are two common ways that we can correct for running multiple statistical tests on the same data?**

- Bonferroni correction (a global adjustment to all $\alpha$s).
- Tukey's HSD (an individual adjustment to each comparison's $\alpha$ based on how different the two group means being compared are).
:::

## This week

<br>

:::: {.columns}
::: {.column width="50%"}

**Tasks:**

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises**

:::
::: {.column width="50%"}

**Support:**

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

:::
::::

<br>

:::: {.columns}
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

:::
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

:::
::::



# Appendix {.appendix} 

## Are we adjusting $\alpha$ or p?

If we compare `pairs()` with no correction and with the Bonferroni correction, it's only the p-values that change.

:::: {.columns}
::: {.column width="50%" style="font-size:70%"}
```{r}
pairs(m1_emm, adjust = "none")
```
:::
::: {.column width="50%" style="font-size:70%"}
```{r}
pairs(m1_emm, adjust = "bonferroni")
```
:::
::::

So technically, `pairs()` adjusts p, not $\alpha$.
But whichever one is adjusted, the results are equivalent!

**Adjusting $\alpha$ by dividing by 15 is the same as adjusting p by multiplying by 15** (and if the value ends up above 1, setting it equal to 1).

For example, the unadjusted p-value for `EMDR Hosp1 - Meds Hosp1` (the sixth row down), 0.0096, multiplied by 15 (the number of tests), is the Bonferroni-adjusted p-value, 0.1441.

```{r}
0.0096 * 15
```



<!-- :::: {.columns} -->
<!-- ::: {.column width="50%"} -->
<!-- a -->
<!-- ::: -->
<!-- ::: {.column width="50%"} -->
<!-- b -->
<!-- ::: -->
<!-- :::: -->
