---
title: "Categorical predictors and dummy coding"
editor_options: 
  chunk_output_type: console
format:
  revealjs:
    smaller: true
---

```{r setup, include=F}
library(tidyverse)
library(patchwork)
library(emmeans)
library(simglm)
library(latex2exp)  # for betas in ggplots
source('_theme/theme_quarto.R')

theme_set(theme_quarto(title_font_size=42))
theme_update(
  text = element_text(family = 'Source Sans 3'),
  axis.title.y = element_text(angle=0, 
                              vjust=0.5, 
                              hjust = 0,
                              margin = margin(t = 0, r = 10, b = 0, l = 0)
                              ),

  )

dapr2red <- "#BF1932" 
pal <- c("#3173c9", "#ff94b0", "#51b375")
```


```{r message=F, warning=F, include=F}
# simulate the data

set.seed(3119) 

sim_arguments <- list(
  formula = y ~ 1 + hours + motivation + study + method,
  fixed = list(hours = list(var_type = 'ordinal', levels = 0:15),
               motivation = list(var_type = 'continuous', mean = 0, sd = 1),
               study = list(var_type = 'factor', 
                            levels = c('alone', 'others'),
                            prob = c(0.53, 0.47)),
               method = list(var_type = 'factor', 
                            levels = c('read', 'summarise', 'self-test'),
                            prob = c(0.3, 0.4, 0.3))),
  error = list(variance = 20),
  sample_size = 250,
  reg_weights = c(0.6, 1.4, 1.5, 6, 6, 2)
)

df3 <- simulate_fixed(data = NULL, sim_arguments) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)

score_data <- df3 %>%
  dplyr::select(y, hours, motivation, study, method) %>%
  mutate(
    ID = paste("ID", 101:350, sep = ""),
    score = round(y+abs(min(y))),
    motivation = round(motivation, 2),
    study = factor(study),
    method = factor(method)
  ) %>%
  dplyr::select(ID, score, hours, motivation, study, method)

score_data <- score_data |>
  mutate(study_num = ifelse(study == 'alone', 0, 1))

# get group means
mean_alone <- filter(score_data, study == 'alone')$score |> mean()
sd_alone <- filter(score_data, study == 'alone')$score |> sd()
mean_others <- filter(score_data, study == 'others')$score |> mean()
sd_others <- filter(score_data, study == 'others')$score |> sd()
```

```{r}


```



# Course Overview 

<br>

:::: {.columns}

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week = 6)
```

:::

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block3_name,block4_name,block3_lecs,block4_lecs,week = 0)
```

:::

::::




## <img style="vertical-align: text-bottom; width: 1.5em;" src="figs/material-symbols--draw-outline.svg"> Warm-up activity: Line drawing
 
 


Draw each of the lines that's defined by the given intercept and slope.

<br>

:::: {.columns}
::: {.column width="33%"}
**Line 1:**

- Intercept is **0**
- Slope is **1**

<!-- Equation: y = b + mx -->

```{r echo=F, fig.asp=1, fig.width=6}
xy_plane <- tibble(
  x = seq(-2, 2),
  y = seq(-2, 2)
) |>
  ggplot(aes(x=x, y=y)) +
  # geom_blank()
  scale_x_continuous(limits = c(-2, 2), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-2, 2), expand = c(0, 0)) +
  geom_segment(x = -2, xend = 2, y = 0, yend = 0,
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black', linewidth = 1.5) +
  geom_segment(x = 0, xend = 0, y = -2, yend = 2, 
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black', linewidth = 1.5) +
  theme(plot.margin = margin(1,1,0,0, "cm")) +
  # geom_abline(intercept = 0, slope = 1, colour = 'blue', linewidth = 2) +
  NULL
xy_plane
```


:::
::: {.column width="33%"}
**Line 2:**

- Intercept is **1**
- Slope is **–1** 

```{r echo=F, fig.asp=1, fig.width=6}
xy_plane +
  # geom_abline(intercept = 1, slope = -1, colour = 'blue', linewidth = 2) +
  NULL
```

:::
::: {.column width="33%"}
**Line 3:**

- Intercept is **–1**
- Slope is **2** 

```{r echo=F, fig.asp=1, fig.width=6}
xy_plane +
  # geom_abline(intercept = -1, slope = 2, colour = 'blue', linewidth = 2) +
  NULL
```

:::
::::






## This week's learning objectives

<br>

::: {.fragment}
::: {.dapr2callout style="font-size: 125%;"}
How can we include categorical variables as predictors in a linear model?
:::
:::

::: {.fragment}
::: {.dapr2callout style="font-size: 125%;"}
When we use a categorical predictor, how do we interpret the linear model’s coefficients?
:::
:::

::: {.fragment}
::: {.dapr2callout style="font-size: 125%;"}
What hypotheses are tested by the default way that R represents categorical predictors?
:::
:::

::: {.fragment}
::: {.dapr2callout style="font-size: 125%;"}
If we wanted to test different hypotheses after fitting the model, how would we do that?
:::
:::




<!-- ======================================== -->

# Categorical predictors with two levels

## Categorical predictors with two levels <br> aka binary predictors

![](figs/binary1.svg){fig-align="center" height="600"}


## Categorical predictors with two levels <br> aka binary predictors

![](figs/binary2.svg){fig-align="center" height="600"}


## Categorical predictors with two levels <br> aka binary predictors

![](figs/binary3.svg){fig-align="center" height="600"}

:::{.fragment}

But: linear models can only deal with input in number form.

So we need a way to represent the levels of these variables as numbers.

The way we'll learn about today: **dummy coding**, aka **treatment coding**.

:::


## Dummy/treatment coding represents one level as 0, and the other level as 1

<br>




:::: {.columns}
::: {.column width="40%"}
:::{.fragment}

**Schematic:**

![](figs/trtmt.svg){fig-align="center"}

- Studying `alone` is coded as 0.

- Studying with `others` is coded as 1.


:::
:::
::: {.column width="10%"}
:::
::: {.column width="50%"}
:::{.fragment}
**In the data:**

:::{ style="font-size: 100%;" }

```{r df head}
score_data |>
  select(ID, study, study_num) |>
  head(10)
```

:::

:::
:::
::::



## Does it matter which level is coded as 0 and which level is coded as 1?

<br><br><br>

:::{.hcenter style="font-size: 150%;" .fragment}
Yes.
:::



## Why does it matter which level is coded as 0 <br> and which level is coded as 1?

:::{.fragment}

To illustrate: Here are test scores from students who studied either `alone` (blue) or with `others` (pink).
:::

:::: {.columns}
::: {.column width="50%"}
:::{.fragment}

```{r plot violin, echo=F, fig.width = 8, fig.asp=1, fig.align='center'}
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 5, width = 0.3) +
  theme(
    legend.position = 'none',
    panel.grid.minor = element_blank()
    ) +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +
  # stat_summary(colour = 'black', fun = mean, geom = 'point', size = 2) +
  # geom_segment(aes(x = 'alone', xend = 'others', y = mean_alone, yend = mean_others), colour = 'black') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  NULL
```
:::
:::

::: {.column width="50%"}
:::{.fragment}
```{r plot xy 1, echo=F, fig.width = 8, fig.asp=1, fig.align='center'}
xlim_lower <- -2.2
xlim_upper <-  2.2
ylim_lower <- -15
ylim_upper <-  55

set.seed(1)  # seed for constant jitter
p_xy_study <- score_data |>
  ggplot(aes(x = study_num, y = score)) +
  geom_jitter(aes(colour = study), alpha = 0.25, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0,
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, 
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  scale_colour_manual(values = pal) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = 'bottom'
  ) +
  labs(
    x = 'study (in numeric space)'
  ) +
 guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  NULL

p_xy_study
```

:::
:::
::::


## Why does it matter which level is coded as 0 <br> and which level is coded as 1?


A linear model will fit a line to this data.

Before I show you this line, I want you to make predictions about it.


:::: {.columns}

::: {.column width="50%"}

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Predict individually**: Write down your guesses: 

  1. The line's intercept will be the same as the mean of either **alone** or **others**. Which one? Why?
  2. Will the slope of the line be **positive** or **negative**? Why?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain in pairs/threes**: 
Why do you think your guesses are likely to be correct?

:::

::: {.column width="50%"}

```{r plot xy 3 app, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study
```

:::

::::



## How to choose appropriate reference level?

<br>

Each level of your categorical predictor will be compared against the reference level (aka the baseline).

<br>

**Useful reference levels might be:**

- The control group in a study
- The group expected to have the lowest score on the outcome
- The largest group

<br>

**The reference level should not be:**

- A poorly-defined level, e.g., category `Other` (because interpreting the coefficients will be weird/tricky)
- Much smaller than the other groups (because estimates based on smaller amounts of data tend to be less trustworthy)



# Testing differences between levels

## Testing differences between levels

<br>

We can find  parameters values all on our own, without a linear model.

But we need a linear model to answer research questions like:

**Do students who study with others score significantly better than students who study alone?**

<br>

To answer this question, we'll fit a model that predicts score as a function of study patterns: `score ~ study`.

<!-- my instinct is to want students to do some predictions here, see what param estimates would be consistent with each outcome. but I think they don't know yet what hyps are being tested by each param. so it might be premature. we can do it later.-->


<br>

```{r}
m1 <- lm(score ~ study, data = score_data)
```


## Modelling `score ~ study` 

```{r}
summary(m1)
```

<br>

:::{.fragment  style="font-size: 120%;" }
**Those coefficient estimates look familiar...**

:::: {.columns}
::: {.column width="50%"}
```{r}
mean_alone
```
:::
::: {.column width="50%"}
```{r}
mean_others - mean_alone
```

:::
::::

:::



## Model evaluation works the same as before

<br>

**$R^2$:**

- Still tells us how much variance in the data is accounted for by the model.

```{r echo=F}
cat(capture.output(summary(m1))[17])
```

<br>

**$F$-ratio (aka $F$-statistic):**

- Still tells us the ratio of explained variance to unexplained variance.
- Still tests the hypothesis that all regression slopes in the model = 0.

```{r echo=F}
cat(capture.output(summary(m1))[18])
```

<br>

(Revisit Lecture 4 for a refresher!)


## What hypotheses are being tested for each coefficient? 

```{r echo=F}
summary(m1)$coefficients
```

<br> 

:::{.fragment}


**`(Intercept)` aka $\beta_0$:**

- Null hypothesis (aka H0): the intercept is equal to zero (aka $\beta_0 = 0$).
- $p$-value (aka `Pr(>|t|)`): the probability of observing an intercept of `r mean_alone`, assuming that the true value of the intercept is zero.

:::

:::{.fragment}


**`studyothers` aka $\beta_1$:**

- Null hypothesis (aka H0): the difference between levels is equal to zero (aka $\beta_1 = 0$).
- $p$-value: the probability of observing a difference of `r round(mean_others-mean_alone, 2)`, assuming that the true value of the difference is zero.
:::

<br>

:::{.fragment}

Our research question was about how studying alone or with others is associated with test score.

**Are both of these hypothesis tests relevant to our research question?**

<!-- if time, could do a three-minute TPS here -->

:::

## Reporting the model's estimates

> Study habits significantly predicted student test scores, F(`r summary(m1)$fstatistic[['numdf']]`, `r summary(m1)$fstatistic[['dendf']]`) = `r summary(m1)$fstatistic[['value']]`, p < 0.001.

```{verbatim}
Study habits significantly predicted student test scores, F(`r summary(m1)$fstatistic[['numdf']]`, `r summary(m1)$fstatistic[['dendf']]`) = `r summary(m1)$fstatistic[['value']]`, p < 0.001.
```

> Study habits explained `r round( summary(m1)$r.squared * 100, 2)`% of the variance in test scores.

```{verbatim}
Study habits explained `r round( summary(m1)$r.squared * 100, 2)`% of the variance in test scores.
```

> Specifically, students who studied with others (M = `r round(mean_others, 2)`, SD = `r round(sd_others, 2)`) scored significantly higher than students who studied alone (M = `r round(mean_alone, 2)`, SD = `r round(sd_alone, 2)`), 
$\beta_1$ = `r round( summary(m1)$coefficients['studyothers', 'Estimate'], 2)`, 
SE = `r round( summary(m1)$coefficients['studyothers', 'Std. Error'], 2)`, 
t = `r summary(m1)$coefficients['studyothers', 't value']`, 
p < 0.001.

```{verbatim}
Specifically, students who studied with others (M = `r round(mean_others, 2)`, SD = `r round(sd_others, 2)`) scored significantly higher than students who studied alone (M = `r round(mean_alone, 2)`, SD = `r round(sd_alone, 2)`), $\beta_1$ = `r round( summary(m1)$coefficients['studyothers', 'Estimate'], 2)`, SE = `r round( summary(m1)$coefficients['studyothers', 'Std. Error'], 2)`, t = `r summary(m1)$coefficients['studyothers', 't value']`, p < 0.001.
```


## We always have time for your questions

<br>

:::{.fragment}

It's OK if your questions are only half-baked right now and you don't know exactly what to ask.
There's a lot of information here!

<br>

Here are some good starting points:

<br>


- **"I'm not certain about [keyword]. Can you go over that again?"**

- **"What does [keyword] have to do with [keyword]?"**

- **"Why would we even want to do [keyword]?"**

<br>

If you ask your questions, it's a good thing for all of us!

:::


# Visualising the model's predictions

## Visualising the model's predictions

<br>

:::{.r-stack}

:::{.fragment}
![](figs/nonref-ci1.svg){fig-align="center" height="500"}
:::

:::{.fragment}
![](figs/nonref-ci2.svg){fig-align="center" height="500"}
:::

:::{.fragment}
![](figs/nonref-ci3.svg){fig-align="center" height="500"}
:::

:::{.fragment}
![](figs/nonref-ci4.svg){fig-align="center" height="500"}
:::

:::

:::{.fragment}

We have almost all the pieces!

But finding the SE for the non-reference level is mathematically a little complicated.

A nice shortcut: **an R package called `emmeans`.**

:::

<!-- - The linear model gives us intercept and slope parameters. -->
<!-- - We have mean and 95% CI for the reference level (= the intercept). -->
<!-- - What if we want mean and 95% CI for the non-reference level? -->
<!-- - To get the mean, we can just add intercept and slope parameters together. -->
<!-- - But finding the CI is mathematically more complicated, so, here's a shortcut: -->


## `emmeans` = "Estimated Marginal Means"

<br>

::: {.incremental style="font-size: 125%;"}
- **"Estimated"** $\rightarrow$ based on a model's estimates
- **"Marginal"**  $\rightarrow$ for each level of a predictor, with the other predictors held constant
- **"Means"**  $\rightarrow$ averages (and confidence intervals!) of the estimated outcomes for each level of the predictor
:::



## `emmeans` = "Estimated Marginal Means"

<br>

:::{.fragment}

We have our model:

```{r}
m1 <- lm(score ~ study, data = score_data)
```

:::

<br> 

:::{.fragment}

Use `emmeans()` to get each group's estimated mean and the variability associated with it.

```{r}
m1_emm <- emmeans(m1, ~study)
m1_emm
```

<br>

`lower.CL` and `upper.CL` are our 95% confidence intervals.

:::

## Plotting marginal means and CIs

<br>

`emmeans` has a built-in plot style.

```{r fig.align='center', fig.asp = .5}
plot(m1_emm) + coord_flip()
```


It's OK, but a bit boring. We can definitely do better.


## Plotting marginal means and CIs

:::: {.columns}
::: {.column width="50%"}
```{r eval=F}
# Save EMMs in tibble format, 
# with compatible column names
m1_emm_df <- m1_emm |>
  as_tibble() |>
  rename(score = emmean)

# Plot the data like before, 
# and now overlay EMMs
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.3, size = 5) +
  theme(legend.position = 'none') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  geom_errorbar(
    data = m1_emm_df, 
    aes(ymin = lower.CL, ymax = upper.CL), 
    colour = 'black', 
    width = 0.2,
    linewidth = 2
  ) +
  geom_point(
    data = m1_emm_df, 
    colour = 'black', 
    size = 5
  )
```
:::
::: {.column width="50%"}
```{r echo=F, fig.width=8, fig.asp=1}
# Save EMMs in tibble format, 
# renaming "emmean" column so 
# it matches the original data
m1_emm_df <- m1_emm |>
  as_tibble() |>
  rename(score = emmean)

# Plot the data like before, 
# and now overlay EMMs
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.3, size = 5) +
  theme(legend.position = 'none') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  # The error bar with 95% CIs:
  geom_errorbar(
    data = m1_emm_df, 
    aes(ymin = lower.CL, ymax = upper.CL), 
    colour = 'black', 
    width = 0.2,
    linewidth = 2
  ) +
  # The point with group means:
  geom_point(
    data = m1_emm_df, 
    colour = 'black', 
    size = 5
  )
```
:::
::::

`emmeans` is an amazingly useful tool.

It can do LOTS more than we've seen here!

We'll see more examples in the coming days and weeks.


## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-06-0.svg){.fragment height="550" }

![](figs/block2-flowchart-06-1.svg){.fragment height="550" }

![](figs/block2-flowchart-06-2.svg){.fragment height="550" }
:::




## Extension: Changing the reference level 

:::: {.columns}
::: {.column width="50%"}

If reference level = `alone`:

```{r plot xy ref alone, echo = F, fig.width=8, fig.asp = 1}
set.seed(1)
p_xy_study +
  geom_abline(slope =4.73, intercept = mean_alone, colour = 'black', linewidth = 2) +
  NULL
```

:::
::: {.column width="50%"}

If reference level = `others`:

```{r plot xy ref others, echo=F, fig.width = 8, fig.asp = 1}
score_data |>
  mutate(study_num = ifelse(study == 'others', 0, 1)) |>
  ggplot(aes(x = study_num, y = score)) +
  geom_jitter(aes(colour = study), alpha = 0.25, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0,
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, 
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  scale_colour_manual(values = pal) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = 'bottom'
  ) +
  labs(
    x = 'study (in numeric space)'
  ) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  geom_abline(slope = -4.73, intercept = mean_others, colour = 'black', linewidth = 2) +
  NULL
```

:::
::::

When `others` is the reference level (that is, the level represented as 0):

- What does the intercept represent?
- What does the slope represent?
- Why is the slope negative now, when before it was positive?
- Challenge: What hypotheses would a linear model test about this data?


<!-- ======================================== -->

# Categorical predictors with >2 levels

## Examples of categorical predictors with >2 levels

<br>

![](figs/categ.png){fig-align="center" height="600"}


## Today's data: Study method

```{r include=F}
mean_read <- filter(score_data, method == 'read')$score |> mean()
sd_read <- filter(score_data, method == 'read')$score |> sd()
mean_self <- filter(score_data, method == 'self-test')$score |> mean()
sd_self <- filter(score_data, method == 'self-test')$score |> sd()
mean_summ <- filter(score_data, method == 'summarise')$score |> mean()
sd_summ <- filter(score_data, method == 'summarise')$score |> sd()
```

:::{.r-stack}
```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol <- score_data |>
  ggplot(aes(x = method, y = score, fill = method, colour = method)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.2, size = 5) +
  theme(legend.position = 'none') +
  scale_fill_manual(values = pal) +
  scale_colour_manual(values = pal) +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 5) +
  NULL

p_viol
```
:::{.fragment data-fragment-index="3"}
```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol +
  # line from read to self-test:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'self-test', y = mean_read, yend = mean_self), linewidth = 2) +
  # line from read to summarise:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'summarise', y = mean_read, yend = mean_summ), linewidth = 2) +
  NULL
```
:::
:::

:::{.fragment data-fragment-index="1"}
**How do we fit a line to data from _three_ groups?**
:::

:::{.fragment data-fragment-index="2"}
- It's impossible to draw one single straight line through all three group means.
:::
:::{.fragment data-fragment-index="3"}
- The smallest number of straight lines that connect all three group means is **two**.
:::
:::{.fragment data-fragment-index="4"}
- For this reason, we're going to use **two** predictors. These two predictors are called **"dummy variables"**.
:::
:::{.fragment data-fragment-index="5"}
- In general, for a categorical predictor with $k$ levels, we'll use $k-1$ dummy variables.
:::



## Dummy variables let us extend dummy/treatment coding to >2 levels

<br>

:::: {.columns}
::: {.column width="50%"}

```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol +
  # line from read to self-test:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'self-test', y = mean_read, yend = mean_self), linewidth = 2) +
  # line from read to summarise:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'summarise', y = mean_read, yend = mean_summ), linewidth = 2) +
  NULL
```


:::
::: {.column width="50%"}

<br>

Both dummy variables have the same reference level: `read`.

- The first dummy variable will compare `self-test` back to `read`.
- The second dummy variable will compare `summarise` back to `read`.

:::
::::

## Dummy variables let us extend dummy/treatment coding to >2 levels 

:::: {.columns}
::: {.column width="50%" .fragment}

First dummy variable: `self-test` vs. `read`.

```{r xy dummy 1, echo=F, fig.width=7, fig.asp=1}
xlim_lower <- -2.2
xlim_upper <-  2.2
ylim_lower <- -25
ylim_upper <-  55

set.seed(1)
p1 <- score_data |> 
  filter(method %in% c('read', 'self-test')) |>
  mutate(method_num = ifelse(method == 'read', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_self, colour = 'black') +
  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_read, colour = 'red', linewidth = 2) +
  # geom_segment(x = 1, xend = 1, y = mean_read, yend = mean_self, colour = 'red', linewidth = 2) +
  labs(
    x = 'method (in numeric space)'
  ) +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[1], pal[2])) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  geom_abline(slope = (mean_self - mean_read), intercept = mean_read, linewidth = 2) +
  NULL
p1
```
:::
::: {.column width="50%" .fragment}

Second dummy variable: `summarise` vs. `read`.

```{r xy dummy 2, echo=F, fig.width=7, fig.asp=1}
set.seed(1)
p2 <- score_data |> 
  filter(method %in% c('read', 'summarise')) |>
  mutate(method_num = ifelse(method == 'read', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_summ, colour = 'black') +
  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_read, colour = 'red', linewidth = 2) +
  # geom_segment(x = 1, xend = 1, y = mean_read, yend = mean_summ, colour = 'red', linewidth = 2) +
  labs(
    x = 'method (in numeric space)'
  ) +  
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[1], pal[3])) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  geom_abline(slope = (mean_summ - mean_read), intercept = mean_read, linewidth = 2) +
  NULL
p2
```

:::
::::

:::{.incremental}
1. Is the slope of the first dummy variable **positive or negative**?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
2. Is the slope of the second dummy variable **positive or negative**?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
3. Is the **slope** of the first dummy variable **bigger** than the slope of the second dummy variable?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
4. Is the **intercept** of the first dummy variable **bigger** than the intercept of the second dummy variable? <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::


## How do we know that a model will use *these* particular dummy variables? 

<br>

The function `contrasts()` shows us how a categorical predictor will be coded.

(You'll get error messages if the variable is not stored as a factor.)

```{r}
contrasts(score_data$method)
```

<br>

:::{.fragment}

**How to read this output:**

- Each **column** contains one dummy variable.
  1. `self-test` compares `self-test` (the 1 in that column) to `read`.
  2. `summarise` compares `summarise` (the 1 in that column) to `read`.

- We know that the reference level is `read` because **in BOTH dummy variables, `read` is coded as 0.**

:::

::: {.hcenter .fragment style="font-size:125%;"}
<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">  **Now is a good time for questions!** <img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">
:::


## Modelling `score ~ method` 

The linear expression:

$$
\text{score}_i = \beta_0 + (\beta_1 \cdot \text{method}_{\text{self-test}}) + (\beta_2 \cdot \text{method}_{\text{summarise}}) + \epsilon_i
$$

:::{.fragment}
In R:

```{r}
m2 <- lm(score ~ method, data = score_data)

summary(m2)
```
:::

## What does each coefficient mean? (1)

```{r echo = F}
cat(paste0(capture.output(summary(m2)), '\n')[9:13])
```

<br>

:::{.fragment}

**`(Intercept)` aka $\beta_0$:**
The mean score for the reference level (`read`).

```{r}
mean_read
```

:::

<br>

:::{.fragment}
**What hypothesis is the model testing here?**

- Null hypothesis: The mean score for the reference level (`read`) is equal to zero.
- $p$-value: the probability of observing an intercept of `r mean_read`, assuming that the true intercept is zero.
:::


:::{.fragment}
**Can we reject this null hypothesis?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::
:::{.fragment}
**If we care about differences between study methods, is this null hypothesis interesting?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::


## What does each coefficient mean? (2)

```{r echo = F}
cat(paste0(capture.output(summary(m2)), '\n')[9:13])
```

<br>

**`methodself-test` aka $\beta_1$:**
The difference between the mean score of `self-test` and the mean score of `read`.

```{r}
mean_self - mean_read
```

<br>

:::{.fragment}
**What hypothesis is the model testing here?**

- Null hypothesis: The difference between the mean score of `self-test` and the mean score of `read` is equal to zero.
- $p$-value: the probability of observing a difference of `r mean_self-mean_read`, assuming that the true difference is zero.
:::



:::{.fragment}
**Can we reject this null hypothesis?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::
:::{.fragment}
**If we care about differences between study methods, is this null hypothesis interesting?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::

## What does each coefficient mean? (3)

```{r echo = F}
cat(paste0(capture.output(summary(m2)), '\n')[9:13])
```

<br>


**`methodsummarise` aka $\beta_2$:**
The difference between the mean score of `summarise` and the mean score of `read`.

```{r}
mean_summ - mean_read
```


<br>

:::{.fragment}
**What hypothesis is the model testing here?**

- Null hypothesis: The difference between the mean score of `summarise` and the mean score of `read` is equal to zero.
- $p$-value: the probability of observing a difference of `r mean_summ-mean_read`, assuming that the true difference is zero.
:::


:::{.fragment}
**Can we reject this null hypothesis?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::
:::{.fragment}
**If we care about differences between study methods, is this null hypothesis interesting?**  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::

<br>

::: {.hcenter .fragment style="font-size:125%;"}
<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">  **Now is a good time for questions!** <img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">
:::


# What if we want to test other hypotheses?

## What if we want to test other hypotheses? 

<br>

For example, what if we want to know about how the two non-reference levels `self-test` and `summarise` are different from each other?

That hypothesis isn't one of the hypotheses tested by the model.

<br>

:::{.fragment}

The solution: use our old friend `emmeans`, "Estimated Marginal Means".

- **"Estimated"** $\rightarrow$ based on a model's estimates
- **"Marginal"**  $\rightarrow$ for each level of a predictor, with the other predictors held constant
- **"Means"**  $\rightarrow$ averages (and confidence intervals!) of the estimated outcomes for each level of the predictor

:::

## Intro to testing hypotheses with `emmeans` 

:::{.fragment}

1. Get estimated marginal means and 95% CIs for each level of the predictor. (This is as far as we got last time.)

```{r}
(m2_emm <- emmeans(m2, ~method))
```


:::
:::{.fragment}


```{r echo=F, fig.align = 'center'}
p_viol
```


:::

## Intro to testing hypotheses with `emmeans` 

2. Using these estimated marginal means and 95% CIs, we can **compare any two levels of the predictor that we want.**
(This comparison is one example of "testing our own contrasts"—more details next week!)

<br>

:::{.fragment}


2.1. We need to know the order of levels in the categorical predictor.

```{r}
levels(score_data$method)
```

:::

<br>

:::{.fragment}

2.2. For every contrast you want to test, assign `1` to the level you're interested in and `-1` to the level you want to compare it to, _using the level order above._

:::{.fragment}
:::

```{r}
m2_comparisons <- list(
  'self-test vs. read'      = c(-1, 1,  0), # read -1, self-test 1, summarise  0
  'summarise vs. read'      = c(-1, 0,  1), # read -1, self-test 0, summarise  1
  'self-test vs. summarise' = c( 0, 1, -1)  # read  0, self-test 1, summarise -1
)
```

These comparisons will compute the level coded as 1 **minus** the level coded as –1.

We'll test the significance of each of those differences.

:::


## Intro to testing hypotheses with `emmeans` 

2.3. Use `contrast()` to find the $p$-value for each contrast.

```{r}
(m2_emm_contr <- contrast(m2_emm, m2_comparisons))
```

<br>

:::{.fragment}

These `estimate`s look familiar:
:::

:::: {.columns}
::: {.column width="33%" .fragment}
```{r}
mean_self - mean_read
```

:::
::: {.column width="33%" .fragment}
```{r}
mean_summ - mean_read
```

:::

::: {.column width="33%" .fragment}
```{r}
mean_self - mean_summ
```
:::
::::

<br>

:::{.fragment}
Finally, use `confint()` to get the 95% CIs of each contrast:

```{r}
confint(m2_emm_contr)
```
:::



## When would you test your own contrasts using `emmeans`?

<br>

:::{.fragment}
**Testing our own contrasts using `emmeans` is not necessary for every analysis.**

But it is useful if the hypotheses we want to test are not the same hypotheses that our *a priori* contrast coding (e.g., our dummy coding) will test.
:::

<br>
<br>
<br>

::: {.hcenter .fragment style="font-size:125%;"}
<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">  **Now is a good time for questions!** <img style="vertical-align: text-bottom; width: 1.2em;" src="figs/mdi--frequently-asked-questions.svg">
:::



# Changing the reference level

## Changing the reference level 

<br>

**Another prediction activity:**

<br>

:::{.dapr2callout}

Before, the reference level of `method` was `read`, and the non-reference levels were `self-test` and `summarise`.

**Now, the reference level of `method` is `summarise`**, and the non-reference levels are `read` and `self-test`.

- Would you expect **the model coefficients** to be the same or different?
- Would you expect **the p-values of each model coefficient** to be the same or different?
- Would you expect **the estimated marginal means** to be the same or different?

:::

<br>

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Predict:**
Write down your guesses about each question.

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain:**
Why do you think your guesses are likely to be correct?

<br>

We'll work through the example and you can check your predictions after.


## How to change reference level in R?

:::{.fragment}

The original dummy variables:

```{r}
contrasts(score_data$method)
```

:::

<br>

:::{.fragment}
Re-code `method`, setting the `base` to our desired reference level (the third level of the variable).

```{r}
contrasts(score_data$method) <- contr.treatment(
  c('read', 'self-test', 'summarise'),  # could also just be "3", for 3 levels
  base = 3
)
```


<!-- ```{r} -->
<!-- score_data <- score_data |> -->
<!--   mutate( -->
<!--     method = factor(method, levels = c('summarise', 'read', 'self-test')) -->
<!--   ) -->
<!-- ``` -->
:::

<br>

:::{.fragment}
Now check the contrasts. **How can we tell that `summarise` is now the reference level?**

```{r}
contrasts(score_data$method)
```

:::


## Understanding the new dummy variables


:::: {.columns}
::: {.column width="50%" .fragment}

First new dummy variable:

`read` vs. `summarise`.


```{r xy dummy relevel 1, echo=F, fig.width=7.5, fig.asp=1}
set.seed(1)
p3 <- score_data |> 
  filter(method %in% c('read', 'summarise')) |>
  mutate(method_num = ifelse(method == 'summarise', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +
  labs(
    x = 'method (in numeric space)'
  ) +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[3], pal[1])) +
  guides(
    colour = guide_legend(override.aes = list(alpha = 1), nrow = 2,byrow = TRUE)
    ) + 
  geom_abline(slope = (mean_read - mean_summ), intercept = mean_summ, linewidth = 2) +
  NULL
p3
```
:::

::: {.column width="50%" .fragment}

Second new dummy variable:

`self-test` vs. `summarise`.

```{r xy dummy relevel 2, echo=F, fig.width=7.5, fig.asp=1}
set.seed(1)
p4 <- score_data |> 
  filter(method %in% c('self-test', 'summarise')) |>
  mutate(method_num = ifelse(method == 'summarise', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +
  labs(
    x = 'method (in numeric space)'
  ) +  
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[3], pal[2])) +
  guides(
    colour = guide_legend(override.aes = list(alpha = 1), nrow = 2,byrow = TRUE)
    ) +
  geom_abline(slope = (mean_self - mean_summ), intercept = mean_summ, linewidth = 2) +
  NULL
p4
```

:::
::::

:::{.incremental}

1. Is the slope of the first dummy variable **positive or negative**?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
2. Is the slope of the second dummy variable **positive or negative**?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
3. Will the coefficient estimates be **the same or different** compared to our previous model?  <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">

:::

## New model vs. old model

:::{.fragment}

```{r}
m3 <- lm(score ~ method, data = score_data)
```
:::

<br>

:::: {.columns}
::: {.column width="50%" .fragment}
Reference level = `summarise` (new).

::: {style="font-size:75%;"}
```{r}
summary(m3)
```
:::

:::
::: {.column width="50%" .fragment}
Reference level = `read` (old).

::: {style="font-size:75%;"}
```{r}
summary(m2)
```
:::

:::
::::

<br>

:::{.fragment}

In the `summarise` model (`m3`, on the left), **what hypotheses are being tested for each coefficient?**
:::


## Changing the reference level: `emmeans` 

:::{.fragment}
```{r}
m3_emm <- emmeans(m3, ~method)
```
:::

<br>

:::: {.columns}
::: {.column width="50%" .fragment}
Reference level = `summmarise` (new).

```{r}
plot(m3_emm) + coord_flip()
```

:::
::: {.column width="50%" .fragment}
Reference level = `read` (old).

```{r}
plot(m2_emm) + coord_flip()
```

:::
::::

<br>

:::{.fragment}
Are the estimated marginal means and 95% CIs from each model **the same or different**? <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsup-filled.svg"> <img style="vertical-align: text-bottom; width: 1em;" src="figs/codicon--thumbsdown-filled.svg">
:::

## What difference does it make to change the reference level?

<br>

:::{.dapr2callout .fragment}

When the reference level of `method` is `summarise`, instead of `read`:

- **The model coefficients** are different.
- **The set of hypotheses that the model tests** are different.
- **The estimated marginal means** are the same.

:::

<br>

:::{.fragment}

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Observe:**
Did your guesses match these results?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain:**
Why are the results the way they are?

:::




<!-- ======================================== -->

## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-06-0.svg){.fragment height="550" }

![](figs/block2-flowchart-06-1.svg){.fragment height="550" }

![](figs/block2-flowchart-06-2.svg){.fragment height="550" }
:::



<!-- ======================================== -->


## Revisiting this week's learning objectives

::: {.fragment}
::: {.dapr2callout}
**How can we include categorical variables as predictors in a linear model?**

- Represent the variable numerically, for example using dummy coding (also called treatment coding).
- In dummy coding, the reference level (also called baseline level) is represented ("coded") as 0, and the non-reference level is coded as 1.
- For categorical predictors with >2 levels, dummy coding uses "dummy variables" to individually compare each non-reference level to the same reference level.

:::
:::

::: {.fragment}
::: {.dapr2callout}
**When we use a categorical predictor, how do we interpret the linear model's coefficients?**

- Intercept (also written as $\beta_0$): The mean outcome for the reference level.
- Slope (also written as $\beta_1$, $\beta_2$, etc., or for short, $\beta_j$): The difference between (1) the mean outcome for the non-reference level and (2) the mean outcome for the reference level (when all other predictors are at zero).

:::
:::

## Revisiting this week's learning objectives

::: {.dapr2callout}
**What hypotheses are tested by the default way that R represents categorical predictors?**

- By default, R uses dummy coding/treatment coding. And by default, the reference level is the level that comes first in the alphabet.
- The intercept's hypothesis test: The mean outcome for the reference level is different from zero.
- The slope's hypothesis test: The difference between the (1) mean outcome for the non-reference level and (2) the mean outcome for the reference level is different from zero.
:::

::: {.fragment}
::: {.dapr2callout}
**If we wanted to test different hypotheses after fitting the model, how might we do that?**

- We can use a linear model to estimate what we'd expect our outcome variable to be for every value of our predictor(s).
- These expected outcome values are called "expected marginal means".
- By comparing expected marginal means after fitting the model, we can test basically any hypotheses we want.

:::
:::

<!-- ======================================== -->

## This week 

<br>

:::: {.columns}
::: {.column width="50%"}

### Tasks

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

:::
::: {.column width="50%"}

### Support

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

:::
::::

<br>

:::: {.columns}
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

:::
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

:::
::::


# Appendix {.appendix}


## Prediction equations: `score ~ study` <br> (two levels) 

The linear expression that the model assumes has generated each data point $i$ (the reference level is `alone`:

$$
score_i = \beta_0 + (\beta_1 \cdot study) + \epsilon_i
$$

A little hat over the variable means that that's an estimated value, so the model will estimate:

$$
\widehat{score} = \hat{\beta_0} + (\hat{\beta_1} \cdot study)
$$

With simple algebra, we can work out how the $\beta$ coefficients relate to the estimated score of each group.

:::: {.columns}
::: {.column width="50%"}
**Studying alone is represented as `study = 0`.**

$$
\begin{align}
\widehat{score}_{alone} &= \hat{\beta_0} + (\hat{\beta_1} \cdot study) \\
\widehat{score}_{alone} &= \hat{\beta_0} + (\hat{\beta_1} \cdot 0) \\
\widehat{score}_{alone} &= \hat{\beta_0} + 0 \\
\widehat{score}_{alone} &= \hat{\beta_0} \\
\end{align}
$$

:::
::: {.column width="50%"}

**Studying with others is represented as `study = 1`.**


$$
\begin{align}
\widehat{score}_{others} &= \hat{\beta_0} + (\hat{\beta_1} \cdot study) \\
\widehat{score}_{others} &= \hat{\beta_0} + (\hat{\beta_1} \cdot 1) \\
\widehat{score}_{others} &= \hat{\beta_0} + \hat{\beta_1} \\
\end{align}
$$

:::
::::




## Prediction equations: `score ~ method` <br> (three levels) 

The linear expression that the model assumes has generated each data point $i$ (the reference level is `read`):

$$
score_i = \beta_0 + (\beta_1 \cdot method_{self-test}) + (\beta_2 \cdot method_{summarise}) + \epsilon_i
$$

A little hat over the variable means that that's an estimated value, so the model will estimate:

$$
\widehat{score} = \beta_0 +  (\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise})
$$

With simple algebra and with the help of the `contrasts()` function, we can work out how the $\beta$ coefficients relate to the estimated score of each group.

:::: {.columns}
::: {.column width="40%"}

<br>

```{r include=F}
score_data <- score_data |>
  mutate(
    method = factor(method, levels = c('read', 'self-test', 'summarise'))
  )
```

```{r}
contrasts(score_data$method)
```


:::
::: {.column width="60%"}

First: **Studying by reading is represented as $method_{self-test} = 0$ and $method_{summarise} = 0$.**
We get these numbers from the `read` row of the contrast matrix from `contrasts()`.


$$
\begin{align}
\widehat{score}_{read} &= \hat{\beta_0} +(\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise}) \\
\widehat{score}_{read} &= \hat{\beta_0} +(\hat{\beta_1} \cdot 0) + (\hat{\beta_2} \cdot 0) \\
\widehat{score}_{read} &= \hat{\beta_0} + 0 + 0 \\
\widehat{score}_{read} &= \hat{\beta_0} \\
\end{align}
$$

:::
::::



## Prediction equations: `score ~ method` <br> (three levels) 

The linear expression that the model assumes has generated each data point $i$ (the reference level is `read`):

$$
score_i = \beta_0 + (\beta_1 \cdot method_{self-test}) + (\beta_2 \cdot method_{summarise}) + \epsilon_i
$$

A little hat over the variable means that that's an estimated value, so the model will estimate:

$$
\widehat{score} = \beta_0 +  (\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise})
$$

With simple algebra and with the help of the `contrasts()` function, we can work out how the $\beta$ coefficients relate to the estimated score of each group.

:::: {.columns}
::: {.column width="40%"}

<br>

```{r}
contrasts(score_data$method)
```


:::
::: {.column width="60%"}

Second: **Studying by self-testing is represented as $method_{self-test} = 1$ and $method_{summarise} = 0$.**
We get these numbers from the `self-test` row of the contrast matrix from `contrasts()`.


$$
\begin{align}
\widehat{score}_{self} &=  \hat{\beta_0} +(\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise}) \\
\widehat{score}_{self} &=  \hat{\beta_0} +(\hat{\beta_1} \cdot 1) + (\hat{\beta_2} \cdot 0) \\
\widehat{score}_{self} &=  \hat{\beta_0} + \hat{\beta_1} + 0 \\
\widehat{score}_{self} &=  \hat{\beta_0} + \hat{\beta_1}\\
\end{align}
$$

:::
::::




## Prediction equations: `score ~ method` <br> (three levels) 

The linear expression that the model assumes has generated each data point $i$ (the reference level is `read`):

$$
score_i = \beta_0 + (\beta_1 \cdot method_{self-test}) + (\beta_2 \cdot method_{summarise}) + \epsilon_i
$$

A little hat over the variable means that that's an estimated value, so the model will estimate:

$$
\widehat{score} = \beta_0 +  (\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise})
$$

With simple algebra and with the help of the `contrasts()` function, we can work out how the $\beta$ coefficients relate to the estimated score of each group.

:::: {.columns}
::: {.column width="40%"}

<br>

```{r}
contrasts(score_data$method)
```


:::
::: {.column width="60%"}

Third: **Studying by summarising is represented as $method_{self-test} = 0$ and $method_{summarise} = 1$.**
We get these numbers from the `summarise` row of the contrast matrix from `contrasts()`.


$$
\begin{align}
\widehat{score}_{summ} &=  \hat{\beta_0} +(\hat{\beta_1} \cdot method_{self-test}) + (\hat{\beta_2} \cdot method_{summarise}) \\
\widehat{score}_{summ} &=  \hat{\beta_0} +(\hat{\beta_1} \cdot 0) + (\hat{\beta_2} \cdot 1) \\
\widehat{score}_{summ} &=  \hat{\beta_0} + 0 + \hat{\beta_2} \\
\widehat{score}_{summ} &=  \hat{\beta_0} + \hat{\beta_2} \\
\end{align}
$$

:::
::::


## Calculating marginal means by hand

For the `score ~ method` model with `read` as the reference level, the linear expression our linear expression is

$$
\text{score}_i = \beta_0 + (\beta_1 \cdot \text{method}_{\text{self-test}}) + (\beta_2 \cdot \text{method}_{\text{summarise}}) + \epsilon_i
$$

<br>

With the $\beta$s that the model estimated for us, we can compute the model's predicted mean outcome for each group (aka, the estimated marginal means) by hand.

<br>

From the model summary, we know that

- $\beta_0$ = 23.4
- $\beta_1$ = 4.2
- $\beta_2$ = 0.8

Substituting those values in, we get:

$$
\widehat{\text{score}} = 23.4 + (4.2 \cdot \text{method}_{\text{self-test}}) + (0.8 \cdot \text{method}_{\text{summarise}})
$$

## Calculating marginal means by hand: <br> method = read

```{r}
contrasts(score_data$method)
```

For `method` = `read` (first row of contrast matrix), we know that

- $\text{method}_{\text{self-test}}$ = 0 (first column of the contrast matrix)
- $\text{method}_{\text{summarise}}$ = 0 (second column of the contrast matrix)

<br>

Substituting _those_ values in, we get:

$$
\begin{align}
\widehat{\text{score}}_{\text{read}} &= 23.4 + (4.2 \cdot \text{method}_{\text{self-test}}) + (0.8 \cdot \text{method}_{\text{summarise}})\\
\widehat{\text{score}}_{\text{read}} &= 23.4 + (4.2 \cdot 0) + (0.8 \cdot 0)\\
\widehat{\text{score}}_{\text{read}} &= 23.4\\
\end{align}
$$

Thus the mean score when method = read should be 23.4—and it is.

```{r}
mean_read
```



## Calculating marginal means by hand: <br> method = self-test

```{r}
contrasts(score_data$method)
```

For `method` = `self-test` (second row of contrast matrix), we know that

- $\text{method}_{\text{self-test}}$ = 1 (first column of the contrast matrix)
- $\text{method}_{\text{summarise}}$ = 0 (second column of the contrast matrix)

<br>

Substituting _those_ values in, we get:

$$
\begin{align}
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + (4.2 \cdot \text{method}_{\text{self-test}}) + (0.8 \cdot \text{method}_{\text{summarise}})\\
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + (4.2 \cdot 1) + (0.8 \cdot 0)\\
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + 4.2\\
\widehat{\text{score}}_{\text{self-test}} &= 27.6\\
\end{align}
$$

Thus the mean score when method = self-test should be 27.6—and it is.

```{r}
mean_self
```



## Calculating marginal means by hand: <br> method = summarise

```{r}
contrasts(score_data$method)
```

For `method` = `summarise` (third row of contrast matrix), we know that

- $\text{method}_{\text{self-test}}$ = 0 (first column of the contrast matrix)
- $\text{method}_{\text{summarise}}$ = 1 (second column of the contrast matrix)

<br>

Substituting _those_ values in, we get:

$$
\begin{align}
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + (4.2 \cdot \text{method}_{\text{self-test}}) + (0.8 \cdot \text{method}_{\text{summarise}})\\
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + (4.2 \cdot 0) + (0.8 \cdot 1)\\
\widehat{\text{score}}_{\text{self-test}} &= 23.4 + 0.8\\
\widehat{\text{score}}_{\text{self-test}} &= 24.2\\
\end{align}
$$

Thus the mean score when method = summarise should be 24.2—and it is.

```{r}
mean_summ
```




## Why bother with a reference level?  

Why can't we just compare each group individually to the overall mean? 

Such a model might look something like this:

$$
score_i = \beta_0 + (\beta_1 \cdot read) + (\beta_2 \cdot selftest) +  (\beta_3 \cdot summarise) + \epsilon_i
$$

Each score is given by an intercept plus the difference between that intercept and each group's scores.
The mean $\mu$ of each group would be:

$$
\begin{align}
\mu_{read} &= \beta_0 + (\beta_1 \cdot read)\\
\mu_{selftest} &= \beta_0 + (\beta_2 \cdot selftest)\\
\mu_{summarise} &= \beta_0 + (\beta_3 \cdot summarise)\\
\end{align}
$$

**For technical reasons, we cannot model this data this way.**

We are trying to estimate three group means: $\mu_{read}$,  $\mu_{selftest}$,  $\mu_{summarise}$.
And for mathematical reasons, that means we can only have three parameters in our model.
But the model above has four: $\beta_0$,  $\beta_1$, $\beta_2$, $\beta_3$.

In technical terms, we say the model above is **overparameterised**.

<br>

Dummy coding with a reference level is one way to achieve **three parameters for three group means**: one intercept for the reference level and two differences between groups for the other two levels.

And we use `emmeans` to get any comparisons we want, regardless of a priori coding scheme.


## Why does it matter which level is coded as 0 <br> and which level is coded as 1?

:::: {.columns}

::: {.column width="50%"}

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Predict individually**: Write down your guesses: 

  1. The line's intercept will be the same as the mean of either **alone** or **others**. Which one? Why?
  2. Will the slope of the line be **positive** or **negative**? Why?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain in pairs/threes**: 
Why do you think your guesses are likely to be correct?

:::


::: {.column width="50%"}
```{r plot xy 2 app, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study
```
:::

::::



## Why does it matter which level is coded as 0 <br> and which level is coded as 1?

:::: {.columns}

::: {.column width="50%"}

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Predict individually**: Write down your guesses: 

  1. The line's intercept will be the same as the mean of either **alone** or **others**. Which one? Why?
  2. Will the slope of the line be **positive** or **negative**? Why?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain in pairs/threes**: 
Why do you think your guesses are likely to be correct?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> **Observe:**
Did your guesses match these results?

<img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--group-outline-rounded.svg"> **Explain:**
Why are the results the way they are?

:::


::: {.column width="50%"}

```{r plot xy with line, echo=F, fig.width=8, fig.asp=1}
gm <-(mean_alone + mean_others)/2

set.seed(1)
p_xy_study <- p_xy_study + 
  geom_abline(slope = 4.73, intercept = mean_alone, colour = 'black', linewidth = 2) +
  NULL

p_xy_study
```

:::


::::


<!-- at Observe, show line, annotate it with Intercept and Slope. ask someone to articulate it in plenum (wait til several hands raised). -->

## How do we describe this line?


:::: {.columns}

::: {.column width="50%"}
Using our mathematical toolkit:

$$
\text{score}_i = \beta_0 + (\beta_1 \cdot \text{study}) + \epsilon_i
$$

<br>

**$\beta_0$:** intercept, mean of `alone` (the level coded as 0):

```{r}
mean_alone
```

<br>

**$\beta_1$:** slope, the difference between mean of `others` (coded as 1) and mean of `alone` (coded as 0):

```{r}
mean_others - mean_alone
```

<br>

**$\epsilon_i$:** error for each individual data point $i$

:::




::: {.column width="50%"}

:::{.r-stack}

```{r plot xy pre betas, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study
```

:::{.fragment}
```{r plot xy with betas, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study +
  # right triangle
  geom_segment(x = 0, xend = 1, y = mean_alone, yend = mean_alone,
               colour = dapr2red, linewidth = 2, linetype = 'dotted') +
  geom_segment(x = 1, xend = 1, y = mean_alone, yend = mean_others,
               colour = dapr2red, linewidth = 2) +
  # arrows
  geom_segment(x = -.8, xend = -.15, y = mean_alone+13, yend = mean_alone+3, arrow = arrow(), colour = dapr2red) +
  geom_segment(x = 1.7, xend = 1.1, y = gm, yend = gm, arrow = arrow(), colour = dapr2red) +
  # betas
  geom_label(x = -1, 
             y = mean_alone+13, 
             label = TeX('$\\hat{\\beta}_0$'), 
             size = 12, 
             col = dapr2red) +
  geom_label(x = 1.8, 
             y = gm, 
             label = TeX('$\\hat{\\beta}_1$'), 
             size = 12, 
             col = dapr2red) +
  NULL
```

:::
:::

:::

::::




<!-- :::: {.columns} -->
<!-- ::: {.column width="50%"} -->
<!-- a -->
<!-- ::: -->
<!-- ::: {.column width="50%"} -->
<!-- b -->
<!-- ::: -->
<!-- :::: -->



<!-- style="font-size: 70%;" -->

 <!-- <img style="vertical-align: text-bottom; width: 1.5em;" src="figs/material-symbols--draw-outline.svg"> -->
 <!-- <img style="vertical-align: text-bottom; width: 1.2em;" src="figs/material-symbols--person-rounded.svg"> -->