---
title: "<b>Introduction to the linear model (LM)</b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(kableExtra)

knitr::opts_chunk$set(out.width = '80%')

theme_set(theme_gray(base_size = 15))

baseColour <- "#BF1932"
```


```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

# Course Overview

.pull-left[

```{r echo = FALSE, results='asis'}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block1_name,block2_name,block1_lecs,block2_lecs,week=1)
```


]

.pull-right[


```{r echo = FALSE, results='asis'}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block3_name,block4_name,block3_lecs,block4_lecs,week=0)
```

]

---



# This Week's Learning Objectives
1. Understand the link between models and functions

2. Understand the key concepts (intercept and slope) of the linear model

3. Understand what residuals represent 

4. Understand the key principles of least squares

5. Be able to specify a simple linear model (labs) 

---
class: inverse, center, middle

# Part 1: Functions & Models

---
# What is a model?
+ Pretty much all statistics is about models

+ A model is a formal representation of a system

+ Put another way, a model is an idea about the way the world is

---
# A model as a function
+ We tend to represent mathematical models as functions

  + A **function** is an expression that defines the relationship between one variable (or set of variables) and another variable (or set of variables)
  
  + It allows us to specify what is important (arguments) and how these things interact with each other (operations)

+ This allows us to make and test predictions

---
# An Example
+ To think through these relations, we can use a simpler example

+ Suppose I have a model for growth of babies <sup>1</sup>

$$
Length = 55 + 4 * Age
$$
--

+ I'm using this model to formally represent the relationship between a baby's age and their length

.footnote[
[1] Length is measured in cm; Age is measured in months
]

---
# Visualising a model

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
fun1 <- function(x) 55 + (4*x)
m1 <- ggplot(data = data.frame(x=0), aes(x=x)) +
  stat_function(fun = fun1) +
  xlim(0,24) +
  ylim(0,150) +
  ylab("Length (cm)") +
  xlab("Age (months)") #+
  #geom_point(colour = "red", size = 3) #+
 # geom_segment(aes(x = x, y = fx, xend = x, yend = 0), 
  #             arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  #geom_segment(aes(x = x, y = fx, xend = 0, yend = fx), 
   #            arrow=arrow(type = "closed", length = unit(0.25, "cm")))
m1
```
]

.pull-right[

{{content}}
]

--
+ The x-axis shows `Age`
{{content}}

--

+ The y-axis shows `Length`
{{content}}

--

+ The black line represents our model: $y = 55+4x$
{{content}}

---
# Models as "a state of the world"
+ Let's suppose my model is true
  + That is, it is a perfect representation of how babies grow
  
+ What are the implications of this?

--
  + My models creates predictions
  
  + **IF** my model is a true representation of the world, **THEN** data from the world should closely match my predictions.


---
# Predictions and data

.pull-left[
```{r, warning=FALSE, message=FALSE, echo=FALSE}
m1+
  geom_segment(aes(x = 11, y = 99, xend = 11, yend = 0),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  geom_segment(aes(x = 11, y = 99, xend = 0, yend = 99),
               col = "red", lty = 2,
               arrow=arrow(type = "closed", length = unit(0.25, "cm")))
```
]

.pull-right[

```{r, echo=FALSE}
tibble(
  Age = seq(10, 12, 0.25)
) %>%
  mutate(
    PredictedLength = 55 + (Age*4)
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

+ Our predictions are points which fall on our line (representing the model, as a function)
+ The arrows are showing how we can use the model to find a predicted value




---
# Predictions and data

.pull-left[

+ Consider the predictions when the children get a lot older...

{{content}}

]


.pull-right[
```{r, echo=FALSE}
tibble(
  Age = seq(216,300, 12)
) %>%
  mutate(
    Year = Age/12,
    Prediction = 55 + (Age*4),
    Prediction_M = Prediction/100
  ) %>%
  kable(.) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

]

--
+ What does this say about our model?
{{content}}

--
+ If we were to collect actual data on height and age, will our observations fall on the line?
{{content}}

---

# Length & Age is non-linear

.pull-left[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
df <- read_csv("data/length_age.csv", col_names = c("Month", "Mean", "SD", "Lower", "Upper"))
m1 + 
  geom_line(data = df, aes(x=Month, y = Mean), colour = "red") + 
  geom_line(data = df, aes(x=Month, y = Lower), colour = "red", linetype = "dashed") +
  geom_line(data = df, aes(x=Month, y = Upper), colour = "red", linetype = "dashed")
```
]

.pull-right[

+ Our red line is plotted based on the mean length for different ages [real data](https://www.cdc.gov/growthcharts/who/boys_length_weight.htm)

]


---
# How good is my model?
+ How might we judge how good our model is?

--

  1. Model is represented as a function
  
  2. We see that as a line (or surface if we have more things to consider)
  
  3. That yields predictions (or values we expect if our model is true)
  
  4. We can collect data
  
  5. If the predictions do not match the observed data (observations deviate from our line), that says something about our model.


---
# Models and Statistics
+ In statistics we (roughly) follow this process:

  + We define a model that represents one state of the world (probabilistically)

  + We collect data to compare to it.

  + These comparisons lead us to make inferences about how the world actually is, by comparison to a world that we specify by our model. 




---
# Deterministic vs Statistical models

.pull-left[
A deterministic model is a model for an **exact** relationship:
$$
y = \underbrace{3 + 2 x}_{f(x)}
$$
```{r echo=FALSE, fig.align='center', out.width = '70%'}
df <- tibble(
  x = seq(-2.6, 2, length.out = 10),
  y = 3 + 2 * x
)
df_grid <- tibble(
  x = seq(-3, 3, length.out = 100),
  y = 3 + 2 * x
)

ggplot() +
  geom_point(data = df, aes(x = x, y = y)) +
  geom_line(data = df_grid, aes(x = x, y = y), color = 'blue') +
  labs(x = 'x', y = 'y') +
  ylim(-6, 11)
```

]

.pull-right[
A statistical model allows for case-by-case **variability**:
$$
y = \underbrace{3 + 2 x}_{f(x)} + \epsilon
$$
```{r echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', out.width = '70%'}
df <- tibble(
  x = rnorm(200),
  y = 3 + 2 * x + rnorm(200, sd = 2.2)
)

ggplot(data = df, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(x = 'x', y = 'y') + 
  ylim(-6, 11)
```
]

---
class: center, middle
# Questions?

---
class: inverse, center, middle

# Part 2: The Linear Model


---
# Linear model
+ For the majority of the course, we will focus on how we move from the idea of an association to estimating a model for the relationship.

+ We'll mostly look at the **linear model**

  + Assumes the relationship between the outcome variable and the predictor(s) is linear
  
  + Describes a continuous **outcome** variable as a function of one or more **predictor** variables
  
      + In other words, in using a linear model, we are typically trying to explain variation in an outcome (AKA $Y$ , dependent, response) variable, using one or more predictor ( $x$ , independent, explanatory) variable(s).


---
# Example

**Question: Do students who study more get higher scores on the test?**

--

.pull-left[

```{r, echo=FALSE}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)

kable(test)
```

]

.pull-right[

**Simple data**

+ `student` = ID variable unique to each respondent

+ `hours` = the number of hours spent studying. This will be our predictor ( $x$ ).

+ `score` = test score. This will be our outcome ( $y$ ).

]

---
# Scatterplot of our data

.pull-left[
```{r, echo=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  labs(x = "Hours Studied", y = "Test Score") +
  scale_x_continuous(limits=c(0, 5)) + 
  scale_y_continuous(limits=c(0, 8)) +
  geom_hline(yintercept = 0, colour = 'black') +
  geom_vline(xintercept = 0, colour = 'black') +
  theme(axis.text = element_text(size=14), 
        axis.title = element_text(size = 16, face = 'bold'))

```
]

.pull-right[

{{content}}

]

--

```{r, echo=FALSE, warning=FALSE, message=FALSE}

(studyPlot <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2)+
  stat_smooth(method="lm", se=FALSE, col="red") +
  labs(x = "Hours Studied", y = "Test Score") +
   geom_hline(yintercept = 0, colour = 'black') +
   geom_vline(xintercept = 0, colour = 'black') +
  scale_x_continuous(limits=c(0, 5)) + 
  scale_y_continuous(limits=c(0, 8)) +
  theme(axis.text = element_text(size=14), 
        axis.title = element_text(size = 16, face = 'bold')))

```



{{content}}


+ The line represents the best model

---
# Definition of the line

.pull-left[
+ The line can be described by two values:

  + **Intercept**: the point where the line crosses the $y$ -axis and $x = 0$

  + **Slope**: the gradient of the line, or rate of change
  
+ What do the intercept and slope stand for in our example?
]

.pull-right[
```{r, echo=FALSE, message = F}
studyPlot
```

]


---
# Intercept and slope

```{r, echo=FALSE, message=FALSE, warning=FALSE}

intercept <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 3, slope = .3) +
  geom_abline(intercept = 4, slope = .3) + 
  geom_abline(intercept = 5, slope = .3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Different intercepts, same slopes")

slope <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(alpha = .1) +
  geom_abline(intercept = 4, slope = .3) +
  geom_abline(intercept = 4, slope = 0) + 
  geom_abline(intercept = 4, slope = -.3) +
  ylab("Test Score") +
  xlab("Hours Studied") +
  ggtitle("Same intercepts, different slopes")

```

.pull-left[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
intercept

```

]

.pull-right[

```{r, echo=FALSE, message=FALSE, warning=FALSE}
slope

```

]


---
# Linear Model Equation

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$


+ $y_i$ = the outcome variable (e.g. `score`) 

+ $x_i$ = the predictor variable, (e.g. `hours`)

+ $\beta_0$ = intercept

+ $\beta_1$ = slope

+ $\epsilon_i$ = residual (we will come to this shortly)


---
# Linear Model Equation

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ **Why do we have $i$ in some places and not others?**


--

+ $i$ is a subscript to indicate that each participant has their own value.

+ So each participant has their own: 
    + score on the test ( $y_i$ )
    + number of hours studied ( $x_i$ ) and
    + residual term ( $\epsilon_i$ )

--
+ **What does it mean that the intercept ( $\beta_0$ ) and slope ( $\beta_1$ ) do not have the subscript $i$?**

--

+ It means there is one value for all observations.
    + Remember the model is for **all of our data**

---
# What is $\epsilon_i$?

.pull-left[
+ $\epsilon_i$, or the residual, is a measure of how well the model fits each data point.

+ It is the distance between the model line (on $y$-axis) and a data point.

+ $\epsilon_i$ is positive if the point is above the line (red in plot)

+ $\epsilon_i$ is negative if the point is below the line (blue in plot)

]


.pull-right[

```{r, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, col = c(rep("darkgrey", 5), "red", "blue", rep("darkgrey", 3)))+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = 3, y = 3.7, xend = 3, yend = 5.9),
               col = "red", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
    geom_segment(aes(x = 3.5, y = 4, xend = 3.5, yend = 3.15),
               col = "blue", lty = 2, 
               arrow=arrow(type = "closed", length = unit(0.25, "cm"))) +
  ylab("Test Score") +
  xlab("Hours Studied")


```

]

???
+ comment red = positive and bigger (longer arrow) model is worse
+ blue is negative, and smaller (shorter arrow) model is better
+ key point to link here is the importance of residuals for knowing how good the model is
+ Link to last lecture in that they are the variability 
+ that is the link into least squares

---
# How to find the line?

.pull-left[
+ The line represents a model of our data.
    + In our example, the model that best characterises the relationship between hours of study and test score

+ In the scatterplot, the data are represented by points

+ So a good line is a line that is "close" to all points

+ The method that we use to identify the best-fitting line is the **Principle of Least Squares**

]

.pull-right[
```{r, echo=F, message=F}
studyPlot
```

]

---
class: center, middle

# Questions?

---
class: inverse, center, middle

# Part 3: Principle of Least Squares

---

# Linear Model
+ Yesterday we left off having introduced the linear model:

$$y_i = \beta_0 + \beta_1 x_{i} + \epsilon_i$$

+ Where

  + $y_i$ is our measured outcome variable
  + $x_i$ is our measured predictor variable
  + $\beta_0$ is the model intercept
  + $\beta_1$ is the model slope
  + $\epsilon_i$ is the residual error (difference between the model predicted and the observed value of $y$)

--

+ The values of $y$ and $x$ come from the observed data. 
+ We'll now go through calculating $\beta_0$ and $\beta_1$

---
# Principle of least squares

.pull-left[

+ The values $\beta_0$ and $\beta_1$ are typically **unknown** and need to be estimated from our data. 

  + We denote the "best" estimated values as $\hat \beta_0$ and $\hat \beta_1$

+ We find the values of $\hat \beta_0$ and $\hat \beta_1$ (and thus our best line) using **least squares**
    
+ Least squares minimises the distances between the actual values of $y$ and the model-predicted values of $\hat y$
  + That is, it minimises the residuals for each data point (the line is "close")

]


--

.pull-right[
```{r, echo = F, message = F}
test$predVals <- predict(lm(score~hours, test), test)
ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, colour = baseColour)+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  geom_segment(aes(x = hours, y = predVals, xend = hours, yend = score),
               col = baseColour, lty = 2) +
  labs(x = "Hours Studied", y = "Test Score")
```

]

---
# Principle of least squares

+ Formally, least squares minimises the **residual sum of squares**

--

.pull-left[
+ Essentially:

  + Fit a line
]

.pull-right[
```{r, echo = F, message = F}
(basePlot <- ggplot(test, aes(x=hours, y=score)) +
  geom_point(size = 2, colour = baseColour)+
  stat_smooth(method="lm", se=FALSE, col = "black") +
  labs(x = "Hours Studied", y = "Test Score") +
   scale_x_continuous(limits=c(0, 5.25)) +
   theme(axis.text = element_text(size=14),
         axis.title = element_text(size = 16, face = 'bold')))
```
]

---
count: false

# Principle of least squares

+ Formally, least squares minimises the **residual sum of squares**

.pull-left[
+ Essentially:

  + Fit a line
  + Calculate the residuals
]


.pull-right[
```{r, echo=F, message=F}
test$resids <- round(test$score-test$predVals, 2)

basePlot + geom_segment(aes(x = hours, y = predVals, xend = hours, yend = score),
               col = baseColour, lty = 2) +
  annotate(geom='text', label=test$resids, x=test$hours+.25, y = test$score-test$resids*.5, size=5)
```

]

---
count: false

# Principle of least squares

+ Formally, least squares minimises the **residual sum of squares**

.pull-left[
+ Essentially:

  + Fit a line
  + Calculate the residuals
  + Square them
]

.pull-right[
```{r, echo = F, message = F}
basePlot + geom_segment(aes(x = hours, y = predVals, xend = hours, yend = score),
               col = baseColour, lty = 2) +
  annotate(geom='text', label=round(test$resids^2, 2), x=test$hours+.25, y = test$score-test$resids*.5, size=5)
```

]
---
count: false

# Principle of least squares

+ Formally, least squares minimises the **residual sum of squares**

.pull-left[
+ Essentially:

  + Fit a line
  + Calculate the residuals
  + Square them
  + Sum up the squares
  
]

.pull-right[
```{r, echo = F, message = F}
basePlot + geom_segment(aes(x = hours, y = predVals, xend = hours, yend = score),
               col = baseColour, lty = 2) +
  annotate(geom='text', label=round(test$resids^2, 2), x=test$hours+.25, y = test$score-test$resids*.5, size=5) +
  annotate(geom='text', label = round(sum(test$resids^2), 2), x = 4, y = 1.5, colour = baseColour, fontface = 'bold', size = 6)
```

]

---
count: false

# Principle of least squares

+ Formally, least squares minimises the **residual sum of squares**

.pull-left[
+ Essentially:

  + Fit a line
  + Calculate the residuals
  + Square them
  + Sum up the squares
  
+ **Why do you think we square the deviations? **
]

.pull-right[
```{r, echo = F, message = F}
basePlot + geom_segment(aes(x = hours, y = predVals, xend = hours, yend = score),
               col = baseColour, lty = 2) +
  annotate(geom='text', label=round(test$resids^2, 2), x=test$hours+.25, y = test$score-test$resids*.5, size=5) +
  annotate(geom='text', label = round(sum(test$resids^2), 2), x = 4, y = 1.5, colour = baseColour, fontface = 'bold', size = 6)
```

]

---

# Residual Sum of Squares

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

---
count: false

# Residual Sum of Squares

$$SS_{Residual} = \sum_{i=1}^{n}(\color{#BF1932}{y_i} - \hat{y}_i)^2$$
+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

---
count: false

# Residual Sum of Squares

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \color{#BF1932}{\hat{y}_i})^2$$
+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example: given you study for 4 hours, what test score does our model predict you will get?


---
count: false

# Residual Sum of Squares

$$SS_{Residual} = \sum_{i=1}^{n}(\color{#BF1932}{y_i - \hat{y}_i})^2$$
+ Data = $y_i$
    + This is what we have measured in our study. 
    + For us, the test scores.

+ Predicted value = $\hat{y}_i = \hat \beta_0 + \hat \beta_1 x_i$ 
    + Or, the value of the outcome our model predicts given someone's values for predictors.
    + In our example: given you study for 4 hours, what test score does our model predict you will get?

+ Residual = Difference between $y_i$ and $\hat{y}_i$.


---
# Key Point

+ It is worth a brief pause as this is a very important point

> The values of the intercept and slope that minimise the sum of square residual are our estimated coefficients from our data

--

> Minimising the $SS_{residual}$ means that across all our data, the predicted values from our model are as close as they can be to the actual measured values of the outcome


---
# Calculating the slope

+ Calculations for slope:

$$\hat \beta_1 = \frac{SP_{xy}}{SS_x}$$

.pull-left[
+ $SP_{xy}$ = sum of cross-products:


$$SP_{xy} = \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$$


+ $SS_x$ = sums of squared deviations of $x$:


$$SS_x = \sum_{i=1}^{n}(x_i - \bar{x})^2$$

]

.pull-right[

+ $x_i$ = predictor data (in our example, `hours`)

+ $y_i$ = outcome data (in our example, `scores`)

+ $\bar{y}$ = mean of $y$

+ $\bar{x}$ = mean of $x$

+ $n$ = total number of observations

+ $\Sigma$ = sum it all up


]

---
# Calculating the intercept

+ Calculations for intercept:

$$\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$$

+ $\hat \beta_1$ = slope estimate

+ $\bar{y}$ = mean of $y$

+ $\bar{x}$ = mean of $x$


---
class: center, middle

## Questions?

---
class: center, middle

## Time for a little R and to look at an example hand calculation


---
# `lm` in R
+ We do not generally calculate our linear models by hand.

+ In R, we use the `lm()` function.

```{r, eval=FALSE}
lm(DV ~ IV, data = datasetName)
```

+ The first bit of code is the model formula:
  + The outcome or DV appears on the left of ~
  + The predictor(s) or IV appear on the right of ~

+ We then give R the name of the data set
  + This set must contain variables (columns) with the same names as you have specified in the model formula.

---
# `lm` in R

.pull-left[
```{r}
test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)
```
]

.pull-right[
```{r}
head(test, 4)
```
]

--

```{r}
lm(score ~ hours, data = test)
```

---
class: center, middle

## We've just run our first linear model! 

## Questions?

---

# Summary
+ Take home points...

  1. In statistics, we are building models that describe how a set of variables relate
  2. The **linear model** is one such model we will use in this course
  3. The linear model describes our data based on an intercept and a slope(s)
  4. From this model (line) we can make predictions about peoples scores on an outcome
  5. The degree to which our predictions differ from the observed data = residual = error = how good (or bad) the model is
  6. We find our model coefficients based on least squares, which are the coefficients that minimise the sum of squared residuals

+ The majority of this course is going to revolve around getting a deeper understanding of these points.

---


## This week 

.pull-left[

### Tasks

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

]

.pull-right[

### Support

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

]


---
class: center, middle
# Thanks for listening!

