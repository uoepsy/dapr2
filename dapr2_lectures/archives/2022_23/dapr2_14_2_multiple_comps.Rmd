---
title: "<b> Multiple Comparisons </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(emmeans)

hosp_tbl <- read_csv("./hospital.csv", col_types = "dff")
```

# Lecture Objectives
1. Understand post-hoc pairwise comparisons and when they are useful 
2. Apply pairwise testing with `emmeans`
3. Understand the multiple testing problem
4. Be able to apply and interpret results with adjustments for multiple comparisons


---
# Models for today
```{r}
m1 <- lm(SWB ~ Treatment + Hospital + Treatment*Hospital, data = hosp_tbl,
         contrasts = list(Treatment = contr.sum, 
                          Hospital = contr.sum)) # you can code contrasts within lm

m1_emm <- emmeans(m1, ~Treatment*Hospital)
```


---
# Pairwise comparisons
+ So far we have been discussing tests which move from the very general to the specific:
  + Overall model F
  + Incremental F/ F per condition
  + Contrasts and codes
  + Simple effects
  
+ But we have one more layer that more closely aligns to looking at the $\beta$ coefficients, namely a fully exploratory pairwise analysis.

---
# Pairwise comparisons
+ As the name suggests, pairwise comparisons compare all levels of a given predictor variable with all levels of the other.

```{r}
pairs_res <- pairs(m1_emm, adjust = "none")
```

---
# Pairwise comparisons

```{r, echo=FALSE}
pairs_res
```

---
# Why do pairwise comparisons?

+ Sometimes we do not have a concrete hypothesis to test.

+ Sometimes we do, but the exploratory analysis is still useful information for the field.

+ Pairwise comparisons throws up a statistical issue, namely the problem of multiple comparisons.
  + When we do lots and lots of tests, the chances of Type I error (false-positives) increase.

+ We will move on to how we can adjust our inferences to deal with this next week, along with a quick revisit of assumption checks.

---
# Types of errors

+ Type I error = False Positive
  + Reject the null when the null is true. 
  + $\alpha = P(\text{Type I Error})$

+ Type II error = False negative
  + Fail to reject the null when the null is false. 
  + $\beta = P(\text{Type II Error})$
  
---
# A single test
+ If we perform a single test, our Type I error rate is $\alpha$.
  + So if we set $\alpha = 0.05$, the probability of a false positive is 0.05

+ However, what if we do multiple tests ( $m$ ) each with the same $\alpha$ level?

+ What is the probability of a false positive among $m$ tests?

---
# Multiple tests

$$P(\text{Type I error}) = \alpha$$
$$P(\text{not making a Type I error}) = 1 - \alpha$$

$$P(\text{Not making a Type I error in m tests}) = (1 - \alpha)^m$$

$$P(\text{Making a Type I error in m tests}) = 1 - (1-\alpha)^m$$

---
# P(Making a Type I error in m tests)

+ Suppose $m=2$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^2)
```

+ Suppose $m=5$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^5)
```

+ Suppose $m=10$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^10)
```

---
# Why does this matter?

+ The $P(\text{Making a Type I error in m tests}) = 1 - (1-\alpha)^m$ is referred to as the family-wise error rate. 

+ A "family" is a set of related tests. 

+ When we analyse an experimental design, and we look at lots of specific comparisons, we can think of all these tests as a "family". 

+ The larger the family, the more likely we are to find a false positive (see previous slide). 

---
# Corrections
+ There are various methods designed to control for the number of tests.
  + Here control means to keep the Type I Error rate at an intended $\alpha$. 

+ Many options. Some of most common:
  + Bonferroni
  + Sidak
  + Tukey
  + Scheffe

+ Others you may see:
  + Holm's step-down
  + Hochberg's step-up


---
# Bonferroni & Sidak
+ Both are considered "conservative" adjustments.

+ Each treats individual tests within the family as if they are independent.
  + Consider an $\alpha = 0.05$ and $m=\text{number of tests}=15$

+ **Bonferroni**: $\alpha_{Bonferroni} = \frac{\alpha}{m}$

```{r}
0.05/15
```


+ **Sidak**: $\alpha_{Sidak} = 1 - (1- \alpha)^{\frac{1}{m}}$

```{r}
1-((1-0.05)^(1/15))
```

---
# Adjusting $p$ not $\alpha$ 
+ On the previous slide, we adjusted the $\alpha$ level.
  + To use this value, we would compare the exact $p$-value of a particular test to the adjusted $\alpha$

+ Alternatively, we can adjust the $p$-value itself, and then compare the adjusted $p$-value to the original $\alpha$
  + This is what `emmeans` does.
  
+ **Bonferroni**: $p_{Bonferroni} = p*m$


---
# No adjustments

```{r, warning = FALSE, eval=TRUE}
pairs(m1_emm, adjust="none")
```

---
# Bonferroni in action: `emmeans`

```{r, echo=TRUE}
pairs(m1_emm, adjust="bonferroni")
```

---
# Sidak with `emmeans`

```{r}
pairs(m1_emm, adjust = "sidak")
```

---
# What about if there were less tests?

```{r}
pairs(m1_emm, simple="Treatment", adjust="bonferroni")
```


---
# Scheffe
+ **Scheffe procedure** calculates $p$ value from the $F$ distribution.

+ The Scheffe critical values is calculated as:

$$ \sqrt{rF(\alpha; r; d)}$$
+ Where 
  + $r$ = number of tests (levels - 1)
  + $d$ = residual degrees of freedom
  + $F$ = unadjusted $F$ critical value

+ Essentially makes the critical value of $F$ larger for a fixed $\alpha$, dependent on the number of tests. 

+ The square-root of the adjusted F provides and adjusted $t$. 

---
# Tukey Honest Significant Differences
+ **Tukey's HSD**
  + Compares all pairwise group means. 
  + Each difference is divided by the $SE$ of the sum of means. 
  + This produces a $q$ statistic for each comparison. 
  + And is compared against a studentized range distribution. 


---
# With `emmeans`

```{r}
pairs(m1_emm, adjust = "tukey")
```


---
# With `emmeans`

```{r}
pairs(m1_emm, adjust = "scheffe")
```


---
# Summary
+ We are know finished with discussing testing effects within `lm`.

+ This week we have looked at ways we can code categorical data to test experimental effects.

+ In the next block, we will consider:
  + Non-continuous **dependent** variables
  + Approaches to modelling and modelling issues
  + Power

+ Last week of the course will be all Q&A and discussing exam.


---
class: center, middle
# Thanks for listening!
