---
title: ' Lecture 15: Logistic Regression 1 '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today 

+ Logistic regression
	+ Why do we need logistic regression?
	+ The logistic regression model
	+ Overall model evaluation
	+ Logistic regression in R


---
#  Why do we need logistic regression? 



---
#  Binary outcomes 

+ Thus far we have discussed:
	+ linear regression with a continuous DV
	+ linear regression with categorical (including binary) predictors
+ What if we have a binary outcome variable?
	+ E.g.,:
		+ Healthy vs diseased
		+ Died vs survived
		+ Hired vs not hired
		+ Correct vs incorrect
![](assets/img/image1.jpeg)


---
#  Binary outcomes 

+ When we have binary outcome variable, linear regression is no longer appropriate
	+ Let’s see what happens when we fit a linear regression model with a binary outcome variable…
+ x
![](assets/img/image2.png)


---
#  Applying linear regression to binary outcomes 

+ We can code our outcome in terms of whether or not an event happened
	+ Y=1 for a job offer
		+ Job offer event occurred
	+ Y= 0 for no job offer
		+ Job offer event did not occur
+ If we then fit a linear regression model, our model predicts the probability of the event occurring P(Y=1)  or


---
#  Some example data 

+ Imagine we’re interested in predicting hiring decisions . We collect data on n=242 job-seekers
+ We collect information on:
	+ Age
	+ Effort put into job application
+ Our variables:
	+ DV: “work” (0 = did not get job; 1 = did get job)
	+ IV1: “age” (in years)
	+ IV2: “ msrch ” (effort into job application, 0=low effort, 1 = high effort)
![](assets/img/image3.jpeg)

---
#  Our outcome variable 

.pull-left[![](assets/img/image5.png)]

.pull-right[![](assets/img/image4.png)]

---
#  Linear probability model 

+ We’ll begin by predicting the probability of a job offer from age using the lm ( ) function to fit a linear regression model
![](assets/img/image6.png)

---
#  Linear probability model 

+ Nothing  looks too amiss here
+ But let’s look at our assumptions checks…
![](assets/img/image6.png)


---
#  Violated assumptions 


These assumptions checks do not look good
![](assets/img/image7.png)

---
#  Impossible probabilities 

+ And there is a further issue…
+ By definition, probabilities must be between 0 and 1
+ But our model predicts some P(Y=1) values >1 within the plausible range of age values


---
#  Something is not right… 

![](assets/img/image8.png)


---
#  The problem with linear regression 

+ In general, when we apply linear regression to binary outcomes:
	+ The distribution of the residuals is bimodal (not normal)
	+ The variance of the residuals is not constant
	+ The relation between X and Y is not linear
	+ Probabilities are not constrained to be between 0 and 1
+ The logistic regression model solves these issues with linear regression

---
#  The LOGISTIC REGRESSION MODEL 



---
#  The logistic regression model 

+ In logistic regression, we predict the probability that Y=1, P( ), from our Xs , using:
+ e = exponential
+ form a linear combination with:
	+ a constant 
	+  capturing the effect of on the DV


---
#  The logistic regression model 

![](assets/img/image9.png)


---
#  Logistic models with different constants 

.pull-left[![](assets/img/image12.png)]

.pull-right[![](assets/img/image11.png)]

---
#  Logistic models with different slopes 

.pull-left[![](assets/img/image13.png)]

.pull-right[![](assets/img/image11.png)]

---
#  Multiple logistic regression 

+ As in linear regression, we can expand the model to include more predictors:


---
#  Estimating logistic regression coefficients 

+ Linear regression models can be estimated using least squares estimation
+ Logistic regression models are estimated using maximum likelihood estimation (MLE)
+ MLE  finds the logistic regression coefficients that *maximise the likelihood of the observed data having occurred*
	+ While least squares estimation minimises the SSE to find the coefficients for the line of best, MLE minimises the log-likelihood
	+ Larger log-likelihood values indicate poorer fitting models


---
#  OVERALL MODEL evaluation 



---
#  Overall model evaluation 

+ To evaluate overall model fit in logistic regression we:
	+ Compare our model to a baseline model with no predictors  (null model)
	+ Assess the improvement in fit
+ The baseline model is where the predicted values for the DV are based on the most frequent value of the DV (0 or 1)
	+ Our best guess of the DV value in the absence of informative predictors
	+ Analogous to using the mean DV value as the baseline in linear regression


---
#  Overall model evaluation 

+ We compare our model with the baseline model using deviance :
+ Deviance often denoted ‘ -2LL ’
+ We calculate the -2LL differences between our model and the baseline model :

---
#  Overall model evaluation 

+ We assess the statistical significance of the -2LL difference to see if our model significantly improves on the baseline model
	+ We compare our -2LL difference to a chi-square distribution with df = k
		+ k= number of predictors in model
+ Significant p-value indicates that our model improves on the baseline model
+ Assessing the -2LL differences in this way is called a likelihood ratio test or chi-square difference test


---
#  Logistic regression in r 



---
#  The glm( ) function 

+ In R, we conduct linear regression using the glm ( ) fuction
+ ‘ glm ’ stands for ‘generalised linear model’
+ Very similar in structure  to the lm( ) function


---
#  Run our model: a little more code 

+ glm () ; R function for running generalised linear models.
	+ Provide a formula in the same style as lm()
	+ Provide the name of the dataset
+ The new bit.
	+ We need to start what family of probability distributions we want for our DV.
	+ This relates to the type of variable our DV is.
	+ We will take a little more about this next lecture.
	+ For now, for a binary variable, we want “family = binomial”

glm (work ~ age + msrch , data = job, family = "binomial")

---
#  Run our model: a little more code 

+ glm () ; R function for running generalised linear models.
	+ Arguments are structured almost identically to lm().
	+ Provide a formula in the same style as lm()
	+ Provide some data
+ We also need to state what family of probability distributions we want for our DV
	+ Relates to the type of variable our DV is
	+ For logistic regression we choose “family = binomial”
	+ For other types of regression (not covered in RMS2) we would choose a different option here

glm(work ~ age + msrch, data = job, family = "binomial")


---
#  glm( ) output 

![](assets/img/image14.png)


---
#  Overall Model Test 

+ Steps:
	+ Calculate the difference in model deviance
+ Calculate the difference in degrees of freedom
+ Calculate the exact p-value based on
+ Is this less than alpha?
---
class: inverse
background-image: url('assets/img/image16.png')
background-size: cover

---
class: inverse
background-image: url('assets/img/image19.png')
background-size: cover

---
class: inverse
background-image: url('assets/img/image18.png')
background-size: cover

---
class: inverse
background-image: url('assets/img/image17.png')
background-size: cover



---
#  Take home messages 

+ For binary outcomes we use logistic rather than linear regression
+ Logistic regression predicts the probability of Y=1 from our Ivs
+ Logistic regression is estimated using maximum likelihood estimation
+ Implemented using glm ( ) function in R
+ The model can be evaluated by comparison with a baseline model using a likelihood ratio  test
![](assets/img/image20.png)


---
#  Tasks for this week… 

+ **Problem** **set & Lab:** Worked example of logistic regression.
	+ It is also now time to start thinking about your notes for the exam. It is quite early in the block this time round, so do not leave this until the last minute.
+ **Reading** **:**
	+ Field, chapter 8.
	+ Other reading, see LEARN page for week 8.
+ **Homework** **:** Categorical*categorical interactions.
	+ Open now, closes 17:00 Sunday.

