---
title: ' Lecture 5: Categorical Predictors '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Overview 

+ Recap: Categorical variables
+ Regression with binary IVs
+ Regression with categorical IVs with more than two levels
	+ Dummy coding
	+ Effects coding
	+ Standardised regression coefficients for categorical IVs

---
#  Recap: categorical variables 



---
#  Categorical variables 

+ Up until now we’ve focused on continuous IVs
	+ Continuous variables can take any value within a given range
		+ E.g., height in cm
+ Regression can also accommodate categorical IVs
	+ Categorical variables can only take discrete values
		+ E.g., animal type: 1= duck, 2= cow, 3= wasp
	+ They are mutually exclusive
		+ No duck-wasps or cow-ducks!
![](assets/img/image1.png)

---
#  Categorical variables in R 

+ In R:
	+ Continuous variables should be of class ‘numeric’
	+ Categorical variables (including binary variables) should be of class ‘factor’
+ It is possible to convert a numeric variable to a factor:
+ Here each unique number become a level of a categorical variable
.pull-left[![](assets/img/image2.emf)]

.pull-right[![](assets/img/image3.emf)]


---
#  Categorical variables in R 

+ Similarly, character variables can be converted to factors
+ Here each unique character string becomes a level of a categorical variable
.pull-left[![](assets/img/image4.emf)]

.pull-right[![](assets/img/image5.emf)]


---
#  Regression with binary IVs 



---
#  Regression with binary IVs 

+ Binary variables are categorical variables with two levels
+ Returning to our revision hours and test score example, suppose we now have a sample of 100 students who come from 2 classes
	+ We have our continuous outcome (Score, Y)
	+ We have a binary predictor (Class, X)
		+ We could code our binary predictor:
			+ Class A = 0
			+ Class B = 1
	+ We would have dataset that looks something like
![](assets/img/image6.emf)


---
#  Regression with binary IVs 

+ We could include the ‘ study.class ’ variable in a simple regression, just like any other IV:
+ We use ‘ ’ rather than to represent the fact that the variable codes ** category membership

---
#  Including a binary IV in a simple regression 


In R:
+ The regression coefficient for study class is *the difference between the mean score for class A and class B*
	+ As we coded class A=0 and class B=1, class A serves as the baseline
	+ If we had coded them the other way round the coefficient would have the same magnitude but opposite sign
+ The intercept is the *mean test score for the group coded 0*
![](assets/img/image7.emf)


---
#  Equation for the two levels of binary predictors 

+ We can verify this by writing the regression equation in terms of separate equations for our two groups:
+ For class A, = 0, giving us:
+ That is, the expected score for a person in class A is equal to the intercept


---
#  Equations for the two levels of a binary predictor 

+ For class B, =1, giving us:
+ The expected score for a person in class B is the expected score for a person in class A plus the regression coefficient .
+ The difference between the expected scores in classes A and B is :


---
#  Overall model evaluation for categorical IVs 

+ R 2 and F-ratio interpretation are identical to their interpretation in models with only continuous IVs
![](assets/img/image7.emf)

---
#  Statistical significance of categorical IVs 

+ We assess the statistical significance of regression coefficients for categorical IVs in exactly the same way as we did for continuous IVs:
+ We use the standard error of the coefficient to construct:
	+ *t-* tests and associated *p-* values  for the coefficient
	+ Confidence intervals around the coefficient
![](assets/img/image7.emf)

---
#  Statistical significance of categorical IVs 

+ Here we can say that there was a mean difference in test scores between class A and class B of 1.67 points on the test and this difference was statistically significant ( t **=4.22,** ***p*** **<.001** )
![](assets/img/image7.emf)

---
#  Including a binary IV in a multiple regression 

+ We can also include the binary IV in a multiple regression:

This let’s us compute the mean difference in test scores across our two groups controlling for other IVs

---
#  Class differences controlling for study hours 

+ For example, we likely want to know whether the two classes differ after we take into account potential differences in hours studying:
+ The intercept is now the expected score when *both* IVs are 0
	+ The expected score for a person in class A who spent 0 hours studying
+ The regression coefficient for study class is now the difference between the mean score for class A and class B *controlling for class differences in study hours*
![](assets/img/image8.emf)

---
#  Class differences controlling for study hours 

+ For example, we likely want to know whether the two classes differ after we take into account potential differences in hours studying:
+ Likewise, the regression coefficient hours is the effect of hours studied controlling for differences between  class A and B
![](assets/img/image8.emf)

---
#  Categorical IVs with other IVs in the model 

+ The mean difference between two groups estimated  in a simple versus multiple regression will differ unless group membership is completely uncorrelated with all the other IVs in the model
+ Otherwise:
	+ Overall model evaluation (R 2 and F-ratio) interpretation is the same as a multiple regression with continuous predictors
	+ Assessment of the statistical significance of the regression coefficients is the same as in a multiple regression with continuous predictors

---
#  Regression with categorical IVs with more than 2 levels 



---
#  Including categorical IVs with >2 levels in a regression 

+ When we have a categorical IV with k levels, we need to convert this to k-1 regressors
	+ The regressors are the variables that go into the regression model to represent the categorical IV
		+ Study class (2 levels) required 1 regressor
	+ We convert the categorical IV to regressors using a ‘coding scheme’
	+ Two common coding schemes are:
		+ Dummy coding
			+ One group is selected as the baseline and all others are compared against this
		+ Effects coding
			+ Each group is compared against the average of all the groups

---
#  Dummy coding 

+ Dummy coding uses 0s and 1s to represent group membership
	+ One level is chosen as a baseline
	+ All other levels are compared against that baseline
+ The steps in dummy coding are:
	+ Create k-1 variables (your regressors or ‘dummy variables’)
	+ Choose a baseline level
	+ Assign everyone in the baseline  group ‘0’ for all k-1 dummy variables
	+ Assign everyone in the next group a ‘1’ for the first dummy variable and a ‘0’ for all the other dummy variables
	+ Assign everyone in the next again group a ‘1’ for the second dummy variable and a 0 for all the other dummy variables
	+ Repeat step 5 until all k-1 dummy variables have had ‘0’s and ‘1’s assigned
	+ Enter the k-1 dummy variables into your regression
![](assets/img/image11.png)

---
#  Choosing a baseline? 

+ Each level of your categorical IV will be compared against the baseline, therefore, it’s important to choose a baseline wisely
+ Good baseline levels could be:
	+ The control group in an experiment
	+ The group expected to have the lowest score on the DV
	+ The largest group
+ It is best the baseline is not:
	+ A poorly defined level, e.g. an ‘Other’ group
	+ Much smaller than the other groups

---
#  Dummy coding 

+ Imagine another 100 students took an exam but were assigned each to use one of three ‘study methods’
	+ Notes re-reading=1, Notes summarising=2, Self-testing=3*
+ We could use dummy coding to convert our ‘study methods’ variable into k-1 regressors:
+ *See: https://www.psychologicalscience.org/publications/journals/pspi/learning-techniques.html

```{r tbl23, echo = FALSE}
tbl23 <- tibble::tribble(
~`Level`, ~`D1`, ~`D2`,
"Notes re-reading","0","0",
"Notes summarising","1","0",
"Self-testing","0","1"
)

kableExtra::kable_styling(knitr::kable(tbl23), font_size = 18)
```
![](assets/img/image12.jpeg)

---
#  Dummy coding 

+ We start out with  a dataset that looks like:
+ And end up with one that looks like:
.pull-left[![](assets/img/image13.emf)]

.pull-right[![](assets/img/image14.emf)]


---
#  Dummy coding with lm( ) 

+ lm ( ) automatically applies dummy coding when you include a variable of class ‘factor’ in a regression model
+ It selects the first group as the baseline group
+ We write:
+ And lm ( ) does all the dummy coding work for us
![](assets/img/image15.emf)

---
#  Dummy coding with lm( ) 

+ The intercept is the mean of the baseline group (notes re-reading)
+ The coefficient for study.method2 is the mean difference between the ‘notes summarising’ group and the baseline group
+ The coefficient for study.method3 is the mean difference between the ‘self-test’ group and the baseline group
![](assets/img/image16.png)

---
#  Changing the baseline group  

+ The level that lm ( ) chooses as it’s baseline may not always be the best choice
	+ You can change it using:
+ Updates the variable with the new coding scheme
+ Specifies that you want dummy coding
+ No. of levels of your IV
+ Group number of your new baseline
![](assets/img/image17.emf)

---
#  Results using the new baseline 

+ The intercept is the now the mean of  the second group (‘Notes summarising’)
+ The two regression slopes now compare ‘Notes re-reading’ and ‘Self-testing’ against this new baseline
+ Note that the choice of baseline does not affect the R^2 or F-ratio
![](assets/img/image18.png)


---
#  Effects coding 

+ Used to compare each level (group) of a categorical IV with the mean of all the groups
+ Steps in effects coding:
	+ Create k-1 variables (your regressors or ‘effect variables’)
	+ Choose a base level
	+ Assign everyone in the base  group ‘-1’ for all k-1 effect variables
	+ Assign everyone in the next group a ‘1’ for the first effect variable and a ‘0’ for all the other effect variables
	+ Assign everyone in the next again group a ‘1’ for the second effect variable and a 0 for all the other effect variables
	+ Repeat step 5 until all k-1 effect variables have had ‘-1’s, ‘0’s and ‘1’s assigned
	+ Enter the k-1 effect variables into your regression

```{r tbl29, echo = FALSE}
tbl29 <- tibble::tribble(
~`Level`, ~`E1`, ~`E2`,
"Notes re-reading","1","0",
"Notes summarising","0","1",
"Self-testing","-1","-1"
)

kableExtra::kable_styling(knitr::kable(tbl29), font_size = 18)
```

---
#  Effects coding 

+ We start out with  a dataset that looks like:
+ And end up with one that looks like:
.pull-left[![](assets/img/image19.emf)]

.pull-right[![](assets/img/image14.emf)]


---
#  Effects coding with lm( ) 

+ lm( ) defaults to dummy coding
+ To do effects coding we need to go through the extra step of setting the coding scheme to effects coding:
+ Updates the variable with the new coding scheme
+ Specifies that you want effects
+ coding
+ No. of levels of your IV
![](assets/img/image20.emf)

---
#  Effects coding with lm( ) 

+ The intercept is the now the mean score across all the groups
+ The coefficient for study.method1 is the difference between the ‘notes re-reading’ group mean and the overall mean across the groups
+ The coefficient for study.method2 is the difference between the ‘notes summarising’ group mean and the mean across all the groups
![](assets/img/image21.emf)

---
#  Effects coding with lm( ) 

+ The intercept is the now the mean score across all the groups
+ The coefficient for study.method1 is the difference between the ‘notes re-reading’ group mean and the overall mean across the groups
+ The coefficient for study.method2 is the difference between the ‘notes summarising’ group mean and the mean across all the groups
![](assets/img/image21.emf)

---
#  What about the self-test group? 

+ Whatever coding scheme we use, we will have k-1 variables for an IV with k levels
+ In effects coding, this implies that one group’s comparison with the mean will always be missing from the output
	+ By default, the last group will be coded ‘-1’ across all the effect variables
	+ Therefore, by default, the last group will be the missing group
+ The easiest way to obtain the missing comparison is to re-run lm( ), coding a different group as the last group:
+ Swaps the codes for group 2 and 3 around
![](assets/img/image22.emf)

---
#  lm( ) with the missing comparison  

+ The coefficient for study.method2 is now the difference between the ‘self-test’ group mean and the overall mean across all the groups
![](assets/img/image23.emf)


---
#  Model evaluation and statistical significance 

+ For dummy and effects coding:
	+ Overall model evaluation (R 2 and F-ratio) interpretation is the same as a multiple regression with continuous predictors
		+ And the same irrespective of which coding scheme is chosen
	+ Assessment of the statistical significance of the regression coefficients is the same as in a multiple regression with continuous predictors

---
#  Standardised coefficients in dummy and effects coding 

+ Last week we discussed standardised regression slopes for continuous IVs
	+ We discussed how we can standardise the IV and DV to get a standardised beta
+ For categorical IVs, WE SHOULD NOT DO THIS
+ Instead, we should STANDARDISE ONLY THE DV
+ The interpretation is then:
	+ For dummy codes, the difference between each  group and the reference group in SD units
	+ For effect codes, the difference between each group and the mean in SD units

---
#  Extensions of dummy and effects coding 

+ Dummy and effects coded categorical IVs can also be included in a multiple regression with other IVs. This gives:
	+ For dummy coding, the difference between each group and baseline controlling for the other IVs
		+ E.g., the difference between notes re-reading and self-test controlling for number of hours spent studying
	+ For effects coding, the difference between each group and the mean controlling for the other IVs
		+ E.g. the difference between self-test and average score controlling for number of hours spent studying
+ They can also interact with other IVs in the model (lectures 11 & 12)
+ We can test the effect of a categorical IV as a whole, rather than just specific contrasts (tomorrow’s lecture)

---
#  Summary 

+ Regression can accommodate categorical IVs
+ Achieved via:
	+ Dummy coding
		+ Each regression slope represents a comparison against a baseline group mean

OR
+ Effects coding
	+ Each regression slope represents a comparison against the overall mean
+ In either case, testing the statistical significance of the coefficients and interpreting the R 2 and F-ratio proceeds in the same way as for regression with continuous predictors

---
#  Tasks for this week… 

+ **Problem** **set:** Categorical variable coding.
+ **Lab** **:** Big (multiple week) regression lab.
+ **Reading** **:**
	+ See reading list on LEARN.
	+ No additional readings this week.
+ **Homework:**  Multiple regression – week 2 content.
	+ Live now, closes Sunday 17:00.
