---
title: ' Lecture 8:Regression Assumptions '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Overview 

+ What are the assumptions of regression and how can we test them?
	+ Normally distributed errors
	+ Homoscedasticity
	+ Linearity
	+ Independence of errors
	+ (Lack of extreme multi-collinearity)

---
#  Regression assumptions 

+ So far, we have discussed evaluating regression models with respect to:
	+ Overall model fit (F-ratio, R^2)
	+ Individual predictors
	+ Individual cases
+ However, the regression model is also built on a set of assumptions
+ If these assumptions are violated, the model will not be very accurate
+ Thus, we also need to test these assumptions

---
#  Example: test scores and sleep 

+ Imagine we collected data on n=100 students who took a statistics exam
+ We are now interested in the effect of the previous night’s sleep on test scores
	+ Our DV is test score
	+ Our IVs are hours spent studying, sex, and hours of sleep the previous night
![](assets/img/image1.jpeg)

---
#  The lm( ) results for test scores and sleep 

+ *We do all our assumption testing after fitting the lm( ) model, using the lm( ) object (‘M1’) we save from the analysis*
![](assets/img/image2.emf)

---
#  Normally distributed errors 



---
#  Normally distributed errors 

+ It is assumed that the residuals (differences between predicted and observed scores) in a linear regression are normally distributed around each predicted DV score
+ Normality of residuals can be tested via a range of methods:
+ Graphical methods
	+ QQ-plots, Histograms
+ Statistical tests:
	+ Shapiro-Wilk test
+ In general, graphical methods are often more useful
	+ Easier to see the nature and magnitude of the assumption violation
![](assets/img/image3.png)

---
#  Q-Q Plots 

+ Quantile comparison plots (Q-Q plots)
	+ Plot the studentised residuals from the model against their theoretically expected values
	+ If the residuals are normally distributed, the points should fall neatly on the diagonal of the plot
	+ Non-normally distributed residuals cause deviations of points from the diagonal
		+ The specific shape of these deviations are characteristic of the distribution of the residuals
![](assets/img/image4.png)

---
#  qqPlot( ) 

+ In R, we can do this using the qqPlot ( ) function from the ‘car’ package:
.pull-left[![](assets/img/image6.png)]

.pull-right[![](assets/img/image5.emf)]

---
#  hist( ) 

+ Histograms plot the frequency distribution of the residuals
+ In R, we can plot a histogram of the residuals using the hist ( ) function
+ > hist (M1$residuals)
![](assets/img/image7.png)

---
#  shapiro.test( ) 

+ The Shapiro-Wilk test provides a significance test on the departure from normality
+ A non-significant *p* -value suggests that the residuals don’t deviate from normality
![](assets/img/image8.emf)

---
#  Homoscedasticity 



---
#  Homoscedasticity 

+ Homoscedasticity is the assumption that error variance is constant and not related to the values of the predictors or the predicted Y values
	+ Heteroscedasticity refers to when this assumption is violated (non-constant variance)
+ Graphical methods
	+ Plot residual values against each predictor and the predicted Y values
+ Statistical tests
	+ Breusch-Pagan test
.pull-left[![](assets/img/image10.png)]

.pull-right[![](assets/img/image9.png)]

---
#  Residual-vs-predicted values plot 

+ In R, we can plot the Pearson residuals against the predictor values and predicted y-values using the residualPlots ( ) function from the car package
![](assets/img/image11.emf)

---
#  residualPlots( ) output 

+ Categorical predictors should show a similar spread of residual values across their levels
+ The plots for continuous predictors should look like a random array of dots
	+ The solid line should follow the dashed line closely
![](assets/img/image12.png)

---
#  Breusch-Pagan test 

+ Also called the ‘non-constant variance test’
+ Tests whether residual variance depends on the predicted values
+ Implemented using the ncvTest ( ) function in R
+ Non-significant *p* -value suggests homoscedasticity assumption holds
![](assets/img/image13.emf)

---
#  Linearity 



---
#  Linearity 

+ Linearity is the assumption that each IV has a straight line relation with the DV
+ Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation
.pull-left[![](assets/img/image15.png)]

.pull-right[![](assets/img/image14.png)]

---
#  Non-linearity in simple regression 

+ In simple regression, plotting the relation between X and Y and looking for curvature can often identify non-linearity
	+ Adding a lowess line and comparing it with the line of best fit from an lm ( ) can highlight departures from linearity
+ > plot( test.sleep$score~test.sleep$sleep )
+ > abline ( lm ( score~sleep , data= test.sleep ))
+ > lines( lowess ( test.sleep$sleep , test.sleep$score ))
![](assets/img/image16.png)

---
#  Non-linearity in multiple regression 

+ In multiple regression, we need to know whether the relations are linear between each IV and DV, controlling for the other IVs
	+ This can be done using component-residual plots
		+ Also known as partial-residual plots
	+ Component-residual plots have the X values on the X-axis and partial residuals on the Y axis
	+ *Partial residuals* for each X variable are:
+ Where :
	+  is the regression residual from the regression model including all the predictors
	+ is the partial (linear) relation between and Y

---
#  crPlots( ) 

+ Component-residual plots can be obtained using the crPlots ( ) function in R
+ The plots for continuous IVs show a linear (dashed) and lowess (solid) line
+ The lowess line should follow the linear line closely, with deviations suggesting non-linearity
	+ Here we have mild non-linearity in the relation between sleep and test scores.
.pull-left[![](assets/img/image17.emf)]

.pull-right[![](assets/img/image18.png)]


---
#  What to do about non-normality of residuals, heteroscedasticity and non-linearity 



---
#  Non-linear transformations 

+ Often non-normal residuals, heteroscedasticity and non-linearity can be ameliorated by a non-linear transformation of the DV and/or IVs
+ This involves applying a function to the values of a variable to change its values and overall shape of its distribution
	+ For non-normal residuals and heteroscedasticity, skewed DVs can be transformed to normality
	+ Non-linearity may be helped by a transformation of both IV and DV

---
#  Transforming variables to normality 

+ Original variable, skew=0.85
+ Log-transformed, skew=0.27
+ Positively skewed data can be made more normally distributed using a log-transformation
.pull-left[![](assets/img/image20.png)]

.pull-right[![](assets/img/image19.png)]

---
#  Log-transformations 

+ Log-transformations can be implemented in R using the log( ) function
+ If your variable contains zero or negative values, you need to first add a constant to make all your values positive
	+ A good strategy is to add a constant so that your minimum value is one
	+ E.g., if your minimum value is -1.5, add 2.5 to all your values
![](assets/img/image21.png)

---
#  Transforming variables to normality 

+ Original variable, skew=-0.99
+ Reflected and log-transformed variable, skew=0.30
+ Negatively skewed data can be made more normally distributed using same procedure but first reflecting the variable (make biggest values the smallest and smallest the biggest) and then applying the log-transform
.pull-left[![](assets/img/image23.png)]

.pull-right[![](assets/img/image22.png)]

---
#  Other potential solutions 

+ Adding higher-order terms to the model (lectures 11 and 12)
+ Non-linear link functions (lectures 15 and 16)
+ Bootstrapping (lectures 17 and 18)

---
#  Independence of errors 



---
#  Independence of errors 

+ Independence of errors is the assumption that residual values are not correlated with one another
+ Difficult to test unless we know the potential source of correlation between cases
+ We can test a limited form of the assumption by testing for autocorrelation between errors
	+ We can test the correlation between each case an adjacent cases in the dataset
	+ Achieved using the Durbin-Watson test

---
#  Durbin-Watson test 

+ Durbin-Watson test implemented in R using the durbinWatsonTest () function:
+ The D-W statistic can take values between 0 and 4
	+ 2= no autocorrelation
+ Therefore, we ideally want D-W values close to 2 and a non-significant *p-* value
+ Values <1 or >3 may indicate problems
![](assets/img/image24.emf)

---
#  Lack of extreme multi-collinearity 



---
#  Multi-collinearity 

+ Multi-collinearity refers to the correlation between IVs
+ We saw this in the formula for the standard error of regression slopes in multiple regression
+ depends on the multiple correlation between a predictor and all the others,
+ When there are large correlations between IVs, the standard errors are increased
	+ Therefore, we don’t want our predictors to be too correlated

---
#  Variance Inflation Factor 

+ The ‘Variance Inflation Factor’ or ‘VIF’ quantifies the extent to which standard errors are increased by IV inter-correlations
+ It can be obtained in R using the vif ( ) function:
+ The function gives a VIF value for each IV
+ Ideally, we want values to be close to 1
+ VIFs> 10 indicate a problem
![](assets/img/image25.emf)

---
#  What to do about multi-collinearity 

+ In practice, multi-collinearity is not often a major problem
+ When issues arise, consider:
	+ Combining highly correlated IVs into a single composite
		+ E.g. create a sum or average of the two IVs
	+ Dropping an IV that is obviously statistically and conceptually redundant with another from the model

---
#  Summary  

+ The accuracy of regression results depends on a number of assumptions:
	+ Normality of residuals
		+ QQ-plots, histograms of studentised residuals, Shapiro Wilk
	+ Homoscedasticity
		+ Plots of residuals against predicted and IV values, non-constant variance test
	+ Linearity
		+ Component-residual plots
	+ Independence of errors
		+ Durbin-Watson test
	+ Multi-collinearity
		+ VIF
+ Non-linear transformations of DV and/or IVs can often resolve issues with the first three assumptions
