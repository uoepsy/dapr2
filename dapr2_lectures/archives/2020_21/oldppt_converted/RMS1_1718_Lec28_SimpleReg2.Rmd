---
title: ' Lecture 28:Simple Regression with a continuous predictor '
subtitle: 'Research Methods & Statistics 1 '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today’s lecture 

+ Today we are going to talk about:
	+ Evaluating the amount of variance in the outcome accounted for by our predictors
		+ Or r-square, coefficient of determination
	+ Evaluating the significance of the slope
	+ Briefly introduce the F-test for the overall significance of the model.
+ Collectively we are answering the question:

*Is our model useful?*


---
#  Quality of the overall model 

+ The aim of our linear model is to build a model which describes Y as a function of X.
	+ That is we are trying to explain Y using X.
+ This model gives us predicted values for Y, and the difference between these an the actual values is:
+ SSE captures the misfit of our model.
	+ If SSE is small, then our model is good.

---
#  Quality of the overall model 

+ But how do we quantify small?
+ Well if the total variation is given by:
+ Then we might quantify small as the proportion of residual variance as compared to total variance.

---
#  Coefficient of determination 

+ R 2 = coefficient of determination
+ Quantifies the amount of variability in the outcome accounted for by the predictors.
+ Can be written as:

Or:

---
#  Our example 


![](assets/img/image1.emf)

---
#  Significance of individual effects 

+ A general way to ask this question would be to state: *“Does our model capture an informative relationship between X and Y?”*
+ In the context of our example from last lecture, we could ask, *“Is study time a useful predictor of test score?”*
+ The above is a research question/hypothesis. As we have been doing throughout, we need to turn this into a testable statistical hypothesis.

---
#  Significance of individual effects 

+ Conceptually:
	+ If x yields no information on y, then
	+ Why?
		+ Beta gives the predicted change in y for a unit change in x.
		+ If y does not change as x changes, then beta = 0.
+ We can state this formally as a null and alternative:

---
#  Significance of individual effects 

+ So we have a null, what we need to do now is construct a test statistic and the associated sampling distribution.
+ For beta’s, we use a t-statistic (again!):

where

And


---
#  Significance of individual effects 

+ This can be evaluated against a t-distribution with n-2 degree for a model with one predictor.
+ More generally this is written as N-K-1
	+ Where K is the number of predictors, and the additional -1 catches the intercept.


---
#  Significance of individual effects 

+ For our example:

So

And…

---
#  Significance of individual effects 

+ For our example:
![](assets/img/image1.emf)

---
#  A decision about the null 

+ So we have a t-value associated with our beta coefficient.
	+ t = 3.656
+ And we know we will evaluate it against a t-distribution with df = n-2 = 3.
+ As with all tests we need to set our alpha.
	+ Let’s take 0.05 two tailed.
+ This gives us a critical value of 3.11.
	+ What is our decision?

---
#  Decision about the null 

+ For our example:
![](assets/img/image1.emf)

---
#  Significance of the overall model 

+ The test of the individual predictors (IVs, or X’s) does not tell us if the overall model is significant or not.
	+ Neither does R-square
	+ But both are indicative
+ To test the significance of the model as a whole, we conduct an F-test.
	+ You will talk much more about F-tests in the context of ANOVA so this is just a brief intro

---
#  F-statistic 

+ Model variation is regression, is a little more complex than in ANOVA. So we will wait to discuss this in detail.
+ In short:
	+ If there is equivalent residual to model variation, F=1
	+ If there is more model than residual F > 1

---
#  Null hypothesis for F 

+ In words, the null hypothesis for the model says that the best guess of any individuals Y value is the mean of Y plus error.
	+ Or, that the X variables carry no information collectively about Y.
+ The alternative is that the best prediction of an individual Yi value includes the predictors.

---
#  F-distribution 

+ The *F-* distribution is a continuous probability (sampling) distribution used when we want to compare two chi-square distributions.
	+ Why is that relevant here?
+ Chi-square distributions are used when we have “sums of squared” quantities.
	+ Think about the calculation of the chi-square statistic.
+ And what is the *F-* statistic…
	+ Ratio of variances (which are sums of squares)
+ The parameters of the F-distribution are the degrees of freedom association which each of the variance estimates (model and residual).

---
#  Decision about the null (model) 

+ For our example:
![](assets/img/image1.emf)

---
#  Tasks for this week… 

+ **Problem set 14** : Understanding and interpreting regression.
+ **Lab 14** : Simple regression in R.
+ **SWIRL:** 18
+ **Reading:**
	+ **OIS:** 7.1-7.4
	+ **Navarro:** 15.0-15.2
	+ **Field:** 7.1-7.5.3 (not R commander)
+ **Homework:** Questions on correlation.
	+ Live now, closes Sunday 17:00.
