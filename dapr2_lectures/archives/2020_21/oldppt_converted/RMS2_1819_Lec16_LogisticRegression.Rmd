---
title: ' Lecture 16: Logistic Regression 2 '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today 

+ More logistic regression
	+ The effects of individual IVs
	+ Model selection
	+ Other issues


---
#  Evaluating and Interpreting IV effects 



---
#  The regression coefficients in our job-seeking example 


![](assets/img/image1.png)


---
#  The logistic regression equation for our job-seeking example 



---
#  The regression coefficients in logistic regression 

+ In linear regression, the b coefficients for each IV are the unit increase in Y for every unit increase in X (holding other IVs constant)
+ In logistic regression, the b coefficients for each IV are the ***change in log odds of Y for every unit increase in X*** ** (holding other IVs constant)


---
#  What are log odds? 


b = the ***change in*** ***log odds*** ***of Y for every unit increase in X***
+ The odds of an event occurring (e.g., a job offer; Y=1) is defined as the ratio of the probability of the event occurring to the probability of the event not occurring :
+ P is the same as calculated  in the logistic regression model:


---
#  What are log odds?  


b = the ***change in*** ***log odds*** ***of Y for every unit increase in X***
+ Log odds are then the natural logarithm of the odds:

---
#  Probabilities, odds and log-odds 

![](assets/img/image8.png)


---
#  For our job-seekers example 

+ ‘ *for every additional year of age, there was a decrease in the log odds of a job offer of* *0.105* *’*
+ *‘those who showed high effort in their application had a* *1.88* *greater log odds of a job offer than those who showed low effort’*

---
#  Odds ratio 

+ log odds don’t provide an easily interpretable way of understanding how the DV changes with the Ivs
+ The b coefficients from logistic regression are thus often converted to odds ratios
	+ Odds ratios are a bit easier to interpret…
	+ Odds ratios are obtained by exponentiating the b coefficients:

*‘Raise e to the power of b’*


---
#  Exponentiating b coefficients 

+ In R, we exponentiate coefficients using the exp( ) function:
![](assets/img/image10.png)

---
#  Interpreting odds ratios  

+ When the coefficients are converted to odds ratios, they represent the *change in odds with a unit increase in X*
	+ Specifically the *ratio of odds* at X=x and X=x+1
+ An odds ratio of 1 indicates no effect
+ An odds ratio < 1 indicates a negative effect
+ An odds ratio of >1 indicates a positive effect


---
#  Interpreting odds ratio 

+ Let’s illustrate using a model with effort as the only predictor of job offer
![](assets/img/image11.png)

---
#  Interpreting odds ratios 

+ Our prediction equation for this model is :
+ We can calculate for X=0 (low effort) and X=1(high effort)
+ We can use these values to calculate the odds at X=0 and X=1
+ We can then find their ratio

---
#  Odds of a job offer for low effort 

+ For X=0

---
#  Odds of a job offer for low effort 

+ For X=0, =0.329

---
#  Odds of a job offer for high effort 

+ For X=1

---
#  Odds of a job offer for high effort 

+ For X=1, =0.788

---
#  The odds ratio for effort 

+ The ratio of the odds for high versus low effort is :
+ Exactly what we get if we do:
![](assets/img/image11.png)

---
#  Statistical significance of the effects of individual IVs 

+ We can also evaluate the statistical significance of the predictors
+ To do this we can use a z-test :
+ However , we should be aware that the z-test is a little prone to type II errors
	+ We can supplement it using model selection procedures (see later)
+ The z-test and associated *p* -value is provided as part of the summary output for glm ( )

---
#  The z-test 


![](assets/img/image11.png)

---
#  Confidence intervals  

+ We can also compute confidence intervals for our coefficients and associated odds ratios
	+ For odds ratios, 1= no effect
	+ The question is, therefore, whether the confidence interval includes 1 or not

---
#  95% confidence intervals for our job-seekers example 

+ We can use the confint ( ) function to compute confidence intervals
+ We can embed this in the exp( ) function to convert our coefficients to odds ratios
+ Neither 95% CI includes 1, therefore, both predictors are significant at *p* <.05.
![](assets/img/image19.png)

---
#  MODEL SELECTION 



---
#  Model selection 

+ Just as in linear regression, we can compare logistic models differing in their predictors to choose a ‘best fitting’ model
+ Methods we can use:
	+ Likelihood ratio test
	+ AIC
	+ BIC


---
#  Likelihood ratio test 

+ We already encountered this yesterday when we compared our model to a baseline model with no predictors
+ We can compare any set of  **nested**  models using the likelihood ratio test
	+ Including models differing in one predictor
	+ This tests the statistical significance of the effect of that predictor
	+ Provides an alternative to the z-test

---
#  Likelihood ratio test in R 


m2a <- glm (work ~ age, data = job, family = "binomial")

m2b <- glm (work ~ age + msrch , data = job, family = "binomial")
+ *Our second model is a significant improvement on our first model*
![](assets/img/image20.png)

---
#  AIC and BIC  

+ We met AIC and BIC in the model selection section in linear regression
	+ Can be used to compare either nested or non-nested models
	+ Smaller (more negative) AIC and BIC indicate better fitting models
	+ BIC, in the context of regression, penalises extra predictors more heavily
	+ BIC differences >10 indicate that one model is better than another to a practically significant extent
+ B IC
+ AIC

---
#  AIC and BIC 

+ When we use maximum likelihood estimation (as we do in logistic regression), AIC is calculated as:
+ BIC is calculated as:
+ k is the number of predictors, n is the sample size

---
#  AIC and BIC in R 

+ We can compute them in R using the AIC( ) and BIC( ) functions:
+ *Both AIC and BIC suggest that the second model is better fitting*

m2a <- glm (work ~ age, data = job, family = "binomial")

m2b <- glm (work ~ age + msrch , data = job, family = "binomial")
![](assets/img/image20.png)


---
#  Other Issues 



---
#  A brief note on diagnostics and assumptions 

+ As in linear regression it is important to check:
	+ There are no unusual cases with undue influence on the results
	+ The assumptions of the logistic model are met
+ Logistic regression assumptions:
	+ Linear relation between the predictors and the log-odds
	+ Independence of errors

---
#  Other regression models 

+ The logistic regression model can be extended to multi-category outcomes
	+ multinomial logistic regression
+ It is just one example of alternative forms of regression to the linear regression model subsumed under the ‘generalised linear model’ e.g.,
	+ Probit regression
	+ Poisson regression
+ The choice of which regression model to use depends on the nature of the data

---
#  Take home messages 

+ Logistic regression coefficients are converted to  odds ratios to make them more interpretable
	+ Odds ratios tell us how the odds of the event change with a unit increase in X
		+ 1 is no effect
		+ <1 is a negative effect
		+ >1 is a positive effect
+ Statistical significance of predictors can be assessed via:
	+ z-test
	+ Confidence intervals
	+ Likelihood ratio test
+ Model selection uses
	+ Likelihood ratio test
	+ AIC and BIC
![](assets/img/image21.png)

---
#  Tasks for this week… 

+ **Problem** **set & Lab:** Worked example of logistic regression.
	+ It is also now time to start thinking about your notes for the exam. It is quite early in the block this time round, so do not leave this until the last minute.
+ **Reading** **:**
	+ Field, chapter 8.
	+ Other reading, see LEARN page for week 8.
+ **Homework** **:** Categorical*categorical interactions.
	+ Open now, closes 17:00 Sunday.

