---
title: ' Lecture 6: Model Selection '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Overview 

+ The model selection challenge
+ Statistics for model selection
	+ Incremental F-test
	+ AIC and BIC
+ Some brief comments on automated selection methods.

---
#  The model selection challenge 



---
#  Model selection 

+ Model selection refers to choosing between competing regression models
+ An important aspect of model selection is choosing which IVs out of all of those that you have collected should be included in your model
+ Challenge is to strike a balance and avoid:
	+ Over-fitting
		+ Too many IVs that contribute little to the model
	+ Under-fitting
		+ Key IVs have been missed out
+ No hard and fast rules for model selection
	+ Requires using your judgement
![](assets/img/image1.jpeg)


---
#  Why not include them all? 

+ Theoretical: The principle of parsimony
	+ Occam’s razor
	+ All else being equal, simpler models are better
+ Practical: Impact on precision and power
+ Having large numbers of IVs increases standard errors and reduces the power to detect the effects of individual IVs
+ ‘ *Entia non sunt multiplicanda praeter necessitatem’*
![](assets/img/image2.png)


---
#  Model selection criteria 

+ Model selection might be based on a number of considerations:
	+ **Theoretical:** The theory you are testing implies that a certain IV or set of IVs must be in the model
		+ e.g., you are testing the theory of planned behaviour, therefore, must include ‘ *attitudes* ’, ‘ *subjective norms* ’ and ‘ *perceived behavioural control* ’ as IVS
	+ **Covariate control:** A certain IV or set of IVs is/are known to be a potential confound and must be included in the model to control for it
		+ e.g., you are interested in the effects of personality on health but must control for the potential confounds of *‘age’* and ‘ *sex’* .
	+ **Statistical:** The IVs or set of IVs contributes in a statistically or practically significant sense to improving variance explained in the DV


---
#  Statistics for model selection 



---
#  Statistical criteria for model selection 

+ For a single IV, to decide whether we want it in the model we could look at the magnitude and statistical significance of its regression slope
+ We can also consider model selection at the level of sets of IVs
+ Sets of IVs could include:
	+ A *structural set* which together encode the effects of a single construct
		+ E.g., dummy regressors for a categorical predictor
	+ A *functional set* which represents a block of conceptually related IVs
		+ E.g., set of personality variables
+ For a single IV or set of IVs, we can compare the fits of two models: 1 with and 1 without the IV/ set of IVs to decide whether we want it in the model:
	+ Incremental F-test
	+ AIC
	+ BIC

---
#  Example: Model selection in ‘study method’ example 

+ Returning to our example from yesterday
	+ Predicting test scores from study method
	+ `Notes re-reading vs Notes summarising vs Self-testing
+ Let’s imagine we weren’t sure whether we wanted to include the set of effects codes representing the effect of study method
+ We could use an incremental F-test, AIC and BIC to help us decide
![](assets/img/image3.jpeg)


---
#  Incremental F-test 

+ Recall, the F-ratio for a single model tests the statistical significance of a regression model
+ The incremental F-test evaluates the statistical significance of the improvement in DV variance explained with the addition of an IV or set of IVs
+ It is based on the difference in F-values between two models
+ **F**

---
#  Incremental F-test calculation 

+ We call the model with the IV/IV set model 1
+ We call the model without the IV/IV set model 0
+ We can write the change in F between model 0 and model 1, as:
+ is the number of IVs in model 1
+ is the change in the value between model 0 and model 1
+ is the difference in the number of IVs between model 0 and model 1
+ is the value for model 1

---
#  Example incremental F calculation 

+ = 0.7271
+  0.7271- 0.5625 = 0.1646
+ =3
+ =3-1=2
+ N= 100
.pull-left[![](assets/img/image4.emf)]

.pull-right[![](assets/img/image5.emf)]


---
#  Example incremental F calculation 

+ = 0.7271
+  0.7271- 0.5625 = 0.1646
+ =3
+ =3-1=2
+ N=100

---
#  Significance of incremental F 

+ Having calculated , we then compare it against an F-distribution with and ) degrees of freedom to get the *p* -value for the change in variance explained by model 1 versus model 0
+ In our test score/revision example:
+ *The addition of study method to the model resulted in a statistically significant model improvement [F(2,96)=28.951, p<.001].*
![](assets/img/image6.emf)

---
#  Incremental F-test in R 

+ We can conduct the incremental F-test in R using the anova ( ) function:
+ Name of model with fewer IVs
+ Name of model with added IVs
![](assets/img/image7.emf)

---
#  anova( ) output 


![](assets/img/image8.emf)

---
#  Nested models 

+ The F-ratio depends on the models being compared being **nested**
+ Nested means that the IVs in one model are a subset of the IVs in the other
+ We also require the models to be computed on the same data
	+ Need to be careful with missing values.
![](assets/img/image9.jpeg)

---
#  Nested versus non-nested 

+ Nested models
+ Non-nested models
+ These models are nested because all IVs in Model 1 appear in Model 0
+ These models are non-nested because IV5 appears in model 0 but not in model 1, therefore, as well as adding two IVs (IV3, IV4), model 1 has also dropped 1 (IV5)

```{r tbl17, echo = FALSE}
tbl17 <- tibble::tribble(
~` `, ~` `,
"Model 0","DV~IV1 +IV2+IV3",
"Model 1","DV~IV1+IV2+IV3+IV4",
" "," ",
"Model 0","DV~IV1 +IV2+ IV5",
"Model 1","DV~IV1+IV2+IV3+IV4"
)

kableExtra::kable_styling(knitr::kable(tbl17), font_size = 18)
```
.pull-left[![](assets/img/image15.png)]

.pull-right[![](assets/img/image14.png)]

---
#  Nested or not nested? 



```{r tbl18, echo = FALSE}
tbl18 <- tibble::tribble(
~` `, ~` `,
"Model 0","Health~Conscientiousness+Neuroticism",
"Model 1","Health~Conscientiousness+ Extraversion"
)

kableExtra::kable_styling(knitr::kable(tbl18), font_size = 18)
```


---
#  Nested or not nested? 



```{r tbl19, echo = FALSE}
tbl19 <- tibble::tribble(
~` `, ~` `,
"Model 0","Health~Age+Sex+Conscientiousness+Neuroticism",
"Model 1","Health~Age + Sex+Conscientiousness+ Neuroticism+Agreeableness"
)

kableExtra::kable_styling(knitr::kable(tbl19), font_size = 18)
```

---
#  Nested or not nested? 



```{r tbl20, echo = FALSE}
tbl20 <- tibble::tribble(
~` `, ~` `,
"Model 0","Health~Age+Sex+Conscientiousness+Neuroticism",
"Model 1","Mortality~Age + Sex+Conscientiousness+ Neuroticism+Agreeableness"
)

kableExtra::kable_styling(knitr::kable(tbl20), font_size = 18)
```

---
#  AIC for model comparisons 

+ The Akaike Information Criterion (AIC) is another index of model fit we can use to compare two models
+ Unlike the incremental F-test it does not require two models to be nested
+ Smaller (more negative) values of AIC indicate better fitting models
+ Therefore, we can compare the AIC values of two models and choose the model with the smaller (more negative) AIC as our preferred model
+ **AIC**


---
#  AIC calculation 

+ AIC for a model can be calculated as:
+ N is the samples size
	+ ln refers to the natural log function
	+ SSE is the sum of squared error
	+ k is the number of IVs  in the model

---
#  AIC parsimony correction 

+ AIC for a model can be calculated as:
+ Main point to note is that the term ‘ **’** applies a penalty for having more predictors
+ When you add more IVs, fit will improve ( will get smaller)
+ The decrease is partially offset by the +2k
	+ This makes AIC a *parsimony-corrected statistic*
	+ Parsimony-corrected statistics help us avoid over-fitting

---
#  AIC in R 

+ We can compare the AIC for two regression models using:
![](assets/img/image16.emf)

---
#  AIC( ) output 


*Model 1 has the smaller AIC value and is, therefore, the better model*  *according to AIC.*
![](assets/img/image17.emf)

---
#  BIC for model comparisons 

+ The Bayesian Information Criterion (BIC) is another parsimony-corrected fit statistic
	+ Correction = k*log(N)
+ Like AIC, it doesn’t require nested models
+ Like AIC, smaller (more negative) BIC values mean better models
+ Like AIC, we can compare the BICs for two models and choose the one with the smaller BIC as the better model
+ **BIC**

---
#  BIC in R 

+ *Model 1 has the smaller BIC value and is, therefore, the better model*  *according to BIC.*
![](assets/img/image18.emf)

---
#  AIC and BIC considerations 

+ The AIC and BIC for a model are not meaningful on their own
	+ They only make sense for model comparisons
+ For AIC, there are no cut-offs to suggest how big a difference in two models is needed to conclude that one is substantively better than the other
+ For BIC, a difference of 10 can be used as a rule of thumb to suggest that one model is substantively better than another
+ It is important to make sure the two models being compared with AIC and BIC are fit to the same cases (participants)
	+ Beware when you have missing data for your IVs

---
#  AIC versus BIC versus F-test 

+ AIC and BIC sometimes tell a different story…
	+ AIC says M1 is better, BIC says M0 is better
+ BIC has a ‘harsher’ parsimony penalty for the kinds of sample sizes used in regression than AIC
	+ This means it is more likely to lead you to select a ‘simpler’ model (i.e., a model with fewer IVs) than AIC
	+ Requires you to think about whether you are more concerned about avoiding over-fitting or under-fitting
+ (Similarly, both AIC and BIC can tell a different from the incremental F-test)

---
#  Model selection across several models 

+ The logic of model comparisons of AIC and BIC for two models differing by k IVs can be extended to comparisons across a whole set of models
.pull-left[![](assets/img/image20.emf)]

.pull-right[![](assets/img/image19.emf)]

---
#  Interim Summary 

+ ‘Model selection’ as discussed here refers to choosing which IVs should be included in a multiple regression model
+ The F-test provides a test of whether an IV/set of IVs improves the model to a statistically significant extent
	+ Requires nested models
+ AIC and BIC provide a parsimony-corrected test of whether an IV/set of IVs improves the model
+ It is informative to consider all three in making model selection decisions

---
#  Automated methods of variable selection 

+ Forward & Backward selection (stepwise methods).
+ **Forward (incremental predictive power):**
	+ Start with the variable which has the highest association with the DV.
	+ Next, add the variable which increases r-squared most of those which remain.
	+ Continue until no variables improve model r-square.
+ **Backwards:**
	+ Start with all variables in the model.
	+ Remove the predictor with the highest p-value.
	+ Run the model again and repeat.
	+ Stop when all p-values for predictors are less than the a priori set critical p-value.

---
#  Oh no, we have gone…. 

![](assets/img/image25.png)

---
#  All-possible-regressions 

+ Start with a model with only the intercept (“empty model”, best guess with no predictors).
	+ Then run all possible models with 1 variable.
	+ Then run all possible models with 2 variables.
	+ Then run all possible models with 3 variables.
	+ And so on until you reach the maximum number of predictors.
+ Results in lots of models.
	+ 2 k where k=number of predictors.
	+ E.g. 12 predictors = 2 12 = 4096 equations

---
#  Now, we are really…. 

![](assets/img/image26.png)

---
#  All-possible-subsets 

+ An essentially identical procedure is all-possible-regressions.
	+ Here select a subset of key variables
		+ Note if you have designed your study well this will be **all** your variables.
	+ Run all models with 1 predictor, 2 predictors etc.
+ Select and present the **best** model for each number of predictors.
	+ Thus if we had 8 predictors, we would evaluate 8 possible models (+empty model).

---
#  Some consideration in using automated selection 

+ Atheoretical .
	+ Solely for predictive purposes.
	+ A solution may be numerically better than others, but it may not be interpretable.
+ General issues with multiple comparisons.
+ Quality of the model is still dependent on the design and inclusion of good variable sets.
+ The selection of the “best” model may also be influenced by the criteria you use to define what is best.

---
#  Tasks for this week… 

+ **Problem** **set:** Categorical variable coding.
+ **Lab** **:** Big (multiple week) regression lab.
+ **Reading** **:**
	+ See reading list on LEARN.
	+ No additional readings this week.
+ **Homework:**  Multiple regression – week 2 content.
	+ Live now, closes Sunday 17:00.
