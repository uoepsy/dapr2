---
title: ' Lecture 4: Multiple Regression '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today 

+ Continuing from last lecture…
	+ Evaluating individual IVs in multiple regression
	+ Types of sums of squares in multiple regression


---
#  Quick recap on multiple regression 

+ Multiple regression extends simple regression to several IVs
+ What are the implications?
	+ Y is now a function of multiple variables
	+ This gives us a multi-dimensional regression surface
	+ The coefficients  are now ‘partial coefficients’ capturing the effect of each IV with the others held constant
	+ and F-ratio can still be used for overall model evaluation, with adjusted- correcting for the fact that  overestimates the corresponding population value


---
#  Evaluating individual predictors in multiple regression 



---
#  Evaluating individual predictors in multiple regression 

+ Broadly follows the same procedure as in simple regression:
	+ Standard errors (SEs) for each regression slope are computed
		+ SE gives a measure of the sampling variability of a regression coefficient
	+ *t* -tests and confidence intervals evaluate the statistical significance of regression slopes

---
#  Standard errors in multiple regression 

+ For simple regression, we computed the standard error of beta using:
+ Where SSE is the sum of squared error (i.e.,
	+ N is the sample size
	+  k is the number of predictors (=1 for simple regression )
+ We noted that SE will be smaller when residual variance (SSE) is smaller and sample size (N) is larger


---
#  Standard errors in multiple regression 

+ SEs in multiple regression are very similar…
+ However, in multiple regression , for each IV *j,*  we also need to account for the **correlation between** **predictors**
+ Thus , we have an additional term, capturing the correlation between the IV *j* and all the other IVs


---
#  What affects the size of the standard error in multiple regression? 

+ Examining the above formula we can see that:
	+ SE is smaller when residual variance ( SSE ) is smaller
	+ SE is smaller when sample size ( N ) is larger
	+ SE is larger when the number of IVs ( k ) is larger
	+ SE is larger when an IV is strongly correlated with other IVs in the model ( )
		+ We’ll return to this later when we discuss multi-collinearity issues

---
#  Significance of multiple regression coefficients 

+ The significance tests for the individual coefficients otherwise follow the same procedure as in simple regression
+ A *t* -test tests the null hypothesis that the partial regression coefficient (
+ The *t* -value is compared to a *t-* distribution with N-k-1 degrees of freedom to assess statistical significance.

---
#  Returning to our academic performance example… 

+ *Both cognitive ability [* *t(197)=14.35, p<.001* *] and self-control [* *t(197)=9.87, p<.001* *] were statistically significant predictors of academic performance.*
![](assets/img/image5.emf)


---
#  Confidence intervals for multiple regression coefficients  

+ Like in simple regression, we can also compute confidence intervals for slopes in multiple regression:
+ The 100(1-alpha) confidence interval for the slope is :

---
#  For our academic performance example… 

![](assets/img/image6.emf)

---
#  Standardising multiple regression coefficients 

+ As in simple regression, we can standardise the IVs and the DV prior to running the regression to obtain the standardised slopes
+ E quivalently, we can use the standard deviations of the IV and DV to convert the unstandardised coefficients to standardised coefficients:
+ However, unlike in simple regression, the standardised regression coefficients are not equal to the Pearson correlation between that IV and Y.
	+ Rather, they are equal to the *partial* correlation coefficient; the correlation between an IV and Y holding all the other IVs constant


---
#  Types of sums of squares 



---
#  Types of sums of squares  

+ In multiple regression, the ‘ **type of sums of squares** ’ becomes relevant
+ Up until this point, we have discussed assessing the effect of different IVs holding *all others in the model* constant
+ However, this is not the only way to assess individual IVs
+ How we assess the effects of individual IVs depends on which type of sums of squares we use
![](assets/img/image9.jpeg)


---
#  Sums of squares 

+ **Type** **I** : Sequential sums of squares
	+ Effect of X on Y, *holding all* *previous* ** *X* constant
	+ Order of variables into the model matters
+ **Type** **III** : Simultaneous sums of squares
	+ Effect on X on Y *holding all X* *constant*
	+ Order of variables into the model does not matter
+ Note: There are also type II sums of squares but we’ll get to that in the section on interactions…


---
#  For example 

+ Let’s say we had three predictors of aggression:
	+ Age (IV1)
	+ Gender (IV2)
	+ Anger-proneness ( IV3)
+ In type I sums of squares:
	+ T he effect of age would be evaluated holding no other variables constant
	+ The effect of gender would be evaluated holding age constant
	+ The effect of anger-proneness would be evaluated holding age and gender constant
+ In type III sums of squares:
	+ The effects of age, gender and anger-proneness would all be evaluated holding every other IV constant

---
#  Sums of squares 

+ Type I:
	+ SS(b1)		: Sums of squares for b1
	+ SS(b2|b1)	: Sums of squares for b2, given b1
	+ SS(b3|b1, b2)	: Sums of squares for b3, given b1 and b2
+ Type III:
	+ SS(b1|b2, b3)	: Sums of squares for b1, given b2 and b3
	+ SS(b2|b1, b3)	: Sums of squares for b2, given b1 and b3
	+ SS(b3|b1, b2)	: Sums of squares for b3, given b1 and b2

---
#  Sums of squares (SS) 

+ **X1**
+ **X2**
+ **X3**

---
#  Type I (sequential) SS 

+ **X1**
+ **X2**
+ **X3**
+ **X1**

---
#  Type I (sequential) SS 

+ **X1**
+ **X2**
+ **X3**
+ **X1**
+ **X2**
+ **X1**

---
#  Type I (sequential) SS 

+ **X1**
+ **X2**
+ **X3**
+ **X1**
+ **X2**
+ **X1**
+ **X3**
+ **X2**
+ **X1**

---
#  Type I (sequential) SS – order matters 

+ **X1**
+ **X2**
+ **X3**
+ **X3**

---
#  Type I (sequential) SS – order matters 

+ **X1**
+ **X2**
+ **X3**
+ **X3**
+ **X2**
+ **X3**

---
#  Type I (sequential) SS – order matters 

+ **X1**
+ **X2**
+ **X3**
+ **X3**
+ **X2**
+ **X3**
+ **X1**
+ **X2**
+ **X3**

---
#  Type I (sequential) SS – order matters 

+ **X1**
+ **X2**
+ **X3**
+ **X1**
+ **X2**
+ **X3**
+ **X3**
+ **X2**
+ **X1**

---
#  Type III (simultaneous) SS 

+ **X1**
+ **X2**
+ **X3**
+ **X3**
+ **X2**
+ **X1**
+ ****
+ **X1**
+ **X2**

---
#  Equivalence of SS 

+ If **all IVs are all uncorrelated** , then type I and type III SS will give the same significance tests:
+ **X1**
+ **X2**
+ **X1**
+ **X2**
+ **X3**
+ **X3**


---
#  Type I vs III sums of squares in R 

+ Different R functions use different types of sums of squares differently
	+ lm() uses type III sums of squares
	+ anova () applied to lm object gives us type I sums of squares


---
#  anova ( ) 

+ Effect of cognitive ability
+ Effect of self-control, *holding cognitive ability constant*
+ The first row shows the significance test for c ognitive ability (not holding self-control constant)
+ The second row shows the significance test for self-control holding the previous IV (cognitive ability) constant
![](assets/img/image10.emf)


---
#  Summary 

+ Evaluation of the statistical significance of multiple regression coefficients is similar to simple regression
	+ E xcept that standard errors are also affected by the correlation between an IV and all the other IVs in the model
+ Significance tests can be done via type I or type III sums of squares
	+ Type I evaluates each IV with previous IVs held constant, making the tests dependent on the order in which IVs are listed
	+ Type III evaluates each IV all other IVs held constant, irrespective of the order in which they are listed

---
#  Tasks for this week… 

+ **Problem** **set:** Multiple regression interpretation.
+ **Lab** **:** Multiple regression.
+ **Reading** **:**
	+ See reading list on LEARN.
	+ No additional readings this week.
+ **Homework:**  Simple regression
	+ Live now, closes Sunday 17:00.
