---
title: ' Lecture 11: Interactions (1) '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Next two lectures… 

+ Specifically, defining, estimating and interpreting:
	+ **Continuous*categorical interactions**
	+ Continuous*continuous interactions
+ Discuss the important issue of variable centering in interactions.
+ Probing, plotting and evaluating interactions.
+ Discuss the different types of interaction.

---
#  Lecture notation 

+ For the next two lectures, we will work with the following equation and notation:
+ Y is a continuous outcome
+ X will always be the continuous predictor
+ Z will be either the continuous or binary predictor
	+ Dependent on the type of interaction we are discussing.
+ XZ is their product or interaction predictor

---
#  General definition 

+ When the effects of one predictor on the DV differ across levels of another predictor.
+ Note interactions are symmetrical. We can talk about interaction of X with Z, or Z with X.

---
#  General definition 

+ Categorical*categorical interaction:
	+ There is a difference in the differences between groups across levels of a second factor.
	+ Discussed this in the context of ANOVA
+ Categorical*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the DV is different across levels of a categorical predictor.
+ Continuous*continuous interaction:
	+ The slope of the regression line between a continuous predictor and the DV changes as the values of a second continuous predictor change.
	+ May have heard this referred to as moderation.

---
#  Interpretation: Categorical*Continuous 

+ Where z is a binary predictor
+ = Value of Y when X and Z are 0
+ = Effect of X (slope) when Z = 0 (reference group)
+ = Difference intercept between Z=0 and Z=1, when X=0.
+ = Difference in slope across levels of Z

---
#  Example: Categorical*Continuous 

+ Conducting a study on how years of service within an organisation predicts salary in two different departments, accounts and store managers.
+ Y = salary (unit = thousands of pounds)
+ X = years of service
+ Z = Department (0=Store managers, 1=Accounts)

---
#  Example: Categorical*Continuous 

.pull-left[![](assets/img/image4.png)]

.pull-right[![](assets/img/image3.png)]

---
#  Example: Categorical*Continuous 

+ Intercept:
	+ Predicted salary for a store manager ( dept =0) with zero years of service is 16.90.
	+ £16,900
+ Service (b1):
	+ For each additional year of service for a store manager ( dept = 0), salary increases by 2.73.
	+ £2,730
![](assets/img/image5.png)

---
#  Example: Categorical*Continuous 

+ Dept (b2):
	+ Difference in salary between store managers ( dept =0) and accounts ( dept =1) with zero years of service is 4.54.
	+ £4,540
+ Service:dept (b3):
	+ The difference in slope. For each year of service, those in accounts ( dept =1) increase by an additional 3.11.
	+ £3,110
![](assets/img/image5.png)

---
#  Plotting interactions 

+ Simple slopes:
	+ *Regression of the outcome Y on a predictor X at specific values of an interacting variable Z.*
+ So specifically for our example:
	+ Regression slopes for salary on years of service at specific values of department.
	+ As department is binary, it takes only two values (0 & 1)
	+ So here we can plot two simple slopes, one for store managers and one for accounts.

---
#  Example: Categorical*Continuous 

![](assets/img/image6.png)

---
#  Plotting interactions 

+ Simple slopes:
	+ *Regression of the outcome Y on a predictor X at specific values of an interacting variable Z.*
+ When calculating simple slopes, the regression equation is re-ordered to more accurately represent the above definition:
+ Where;
	+ captures the simple slope of Y on X and is dependent on the value of Z.

---
#  Example: Categorical*Continuous 

+ So in our example (for Dept. = 1, accounts):

---
#  Example: Categorical*Continuous 

+ So in our example (for Dept. = 0, store managers):

---
#  Example: Categorical*Continuous 

.pull-left[![](assets/img/image6.png)]

.pull-right[![](assets/img/image5.png)]

---
#  Interactions without main effects 

+ When we include a higher-order term/interaction in our models, it is critical we include the main effects.
+ If we do not include the main effects, our interaction term includes **all** effects (main and interaction) of our predictors.
	+ This is impossible to reasonably interpret.

---
#  Interactions without main effects 

+ Original model
+ No main effects
.pull-left[![](assets/img/image8.png)]

.pull-right[![](assets/img/image7.png)]

---
#  Specifying Interactions in R 

+ How we specify the interactions in R impacts whether it defaults to giving conditional main effects.
+ These provide full model results:

lm(salary ~ service + dept +service* dept , data = df )

lm(salary ~ service* dept , data = df )
+ This does not:

lm(salary ~ service:dept , data = df )

---
#  Centering predictors 

+ Note that much of the interpretation of models with interactions involves evaluation when other variables = 0.
+ This makes it quite important that 0 is meaningful in some way.
	+ Note this is simple with categorical variables.
	+ We code our reference group as 0 in all dummies.
+ For continuous variables, we need a meaningful 0 point.

---
#  Centering: An example with age 

+ Suppose I have age as a variable in my study with a range of 30 to 85.
+ Age = 0 is not that meaningful.
	+ Essentially means all my parameters are evaluated at point of birth.
+ So what might be meaningful?
	+ Average age – mean centering
	+ A fixed point – e.g. 65 if studying retirement

---
#  Why centre? 

+ Meaningful interpretation.
+ Reduce multi-collinearity.
	+ X and Z are by definition correlated with the product term XZ.
	+ Multi-collinearity (we know) is an issue within linear models.
	+ Centering helps to reduce this in models with interactions.
	+ Always some proportion of correlation, term essential collinearity, which can not be removed.

---
#  Our Example: Center on minimum observed value 

+ Original Model
+ Scaled so minimum service = 0
.pull-left[![](assets/img/image11.png)]

.pull-right[![](assets/img/image5.png)]

---

.pull-left[![](assets/img/image13.png)]

.pull-right[![](assets/img/image12.png)]

---

.pull-left[![](assets/img/image11.png)]

.pull-right[![](assets/img/image13.png)]

---
#  Our Example: Mean center 

+ Original Model
+ Mean center service = 0
.pull-left[![](assets/img/image5.png)]

.pull-right[![](assets/img/image14.png)]

---

.pull-left[![](assets/img/image15.png)]

.pull-right[![](assets/img/image12.png)]

---

.pull-left[![](assets/img/image14.png)]

.pull-right[![](assets/img/image15.png)]

---
#  Tasks for this week… 

+ **Problem** **set & Lab:** Interactions
	+ **PS:** Introduce jtools for plotting and probing.
	+ **Lab:** Worked example
+ **Reading** **:**
	+ See LEARN and next slide for further reading.
+ **Homework** **:** Synoptic on weeks 1-5
	+ Open now, closes 17:00 Sunday.

---
#  Additional Interactions Reading(not compulsory!!) 

+ Aiken, L. S., & West, S. G. (1991). *Multiple regression: Testing and interpreting interactions* . Newbury Park, CA: Sage .
+ McClelland , G. H., & Judd, C. M. (1993). Statistical difficulties of detecting interactions and moderator effects. *Psychological bulletin* , *114* (2), 376 .
+ Preacher, K. J. (2015). Advances in mediation analysis: A survey and synthesis of new developments. *Annual Review of Psychology* , *66* , 825-852.
