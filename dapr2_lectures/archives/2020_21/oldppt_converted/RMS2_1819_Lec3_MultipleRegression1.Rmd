---
title: ' Lecture 3: Multiple Regression 1 '
subtitle: 'Research Methods & Statistics 2  Aja Murray: aja.murray@ed.ac.uk, F16, 7 George Square Tom Booth: tom.booth@ed.ac.uk, F17, 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today 

+ Simple regression (continued)
	+ Evaluating individual predictors
+ Introduction to multiple regression
	+ Extending the simple regression model
	+ Overall model evaluation for multiple regression


---
#  Evaluating individual Independent variables 

+ SIMPLE REGRESSION CONTINUED…

---
#  Evaluating individual IVs 

+ As well as evaluating overall model fit and significance, we can evaluate individual IVs
+ Last lecture we discussed the interpretation of regression slopes
+ We also need to look at:
	+ The sampling variability of (its standard error)
	+ The statistical significance of 


---
#  Standard error of <U+0001D44F>1 

#    

+ The standard error (SE) provides a measure of how different values would be across repeated samples
+ Smaller SEs suggest is estimated more precisely
	+ Smaller SEs are better!


---
#  Formula for the standard error of <U+0001D44F>1 

#    

+ The formula for the standard error of is:
+ Where:
	+ SSE is the sum of squared error (i.e., )
	+ N is the sample size
	+ k is the number of predictors (=1 for simple regression)
+ From this formula, we can see that the SEs for regression slopes will be smaller when:
	+ Residual variance ( is smaller
	+ Sample size, N is larger


---
#  Significance of  <U+0001D44F>1 

#    

+ The SE is important in significance tests for
+ We can think of testing the significance of as follows:
	+ If an IV yields no information about the DV then
	+ Why?
		+ gives the predicted change in Y for a unit change in X
		+ If Y does not change as X changes, then = 0
+ We can state this formally as a null and alternative hypothesis:

---
#  Significance of  <U+0001D44F>1 

#    

+ For , the appropriate test statistic is a t-statistic:
+ In regression, the degrees of freedom for *t* is N-k-1
	+ In simple regression with only one predictor, this gives us N-2 df
+ We compare our t-value against a t-distribution with N-2 df to assess statistical significance


---
#  Significance of  <U+0001D44F>1 

#    

+ Applying this to our exam revision and test score example from last week where we had SSE=21.26 , 10 participants , = 20.625 and =1.055 gives us:

So;
+ We compare our t-value against a t-distribution with df=8 to assess statistical significance, giving us *p=* 0.019
+ We can conclude that *revision is a statistically significant predictor of test scores at alpha=0.05*


---
#  Confidence intervals for  <U+0001D44F>1 

#    

+ We can also compute confidence intervals for
+ The 100(1-alpha), e.g., 95%, confidence interval for the slope is:
+ E.g., the 95% confidence interval for in our revision and test score example would be:

The confidence interval of 0.229 to 1.881 does not include zero, therefore, we can conclude that *revision is a statistically significant predictor of test scores* ( *p<.* 05).


---
#  Standardised regression coefficients 

+ Unstandardised regression coefficients are often more useful when the variables are on  meaningful scales
	+ E.g. X additional hours of exercise per week adds Y years of healthy life
+ Sometimes it’s useful to obtain standardised regression coefficients
	+ When the scales of variables are arbitrary
	+ When there is a desire to compare the effects of variables measured on different scales
+ For continuous IVs, transforming both the IV and DV to z-scores (mean=0, SD=1) prior to fitting the regression yields standardised betas.
+ Alternatively, unstandardised betas can be converted to standardised betas using:


---
#  Standardised regression coefficients 

+ For our revision and test score example, we can compute the SDs for IV and DV:
	+ SD for revision hours (X) = 1.513
	+ SD for test scores (Y) = 2.213
+ Using these to compute the standardised regression coefficient:
+ *For every SD unit increase in hours revised, test scores are expected to increase by* *0.72* *SD units*

---
#  Interpreting standardised regression coefficients  

+ R 2 , *F* and *t-test* remain the same for the standardised coefficients as for unstandardised coefficients
+ The intercept becomes zero when all slopes are standardised
+ The interpretation of the coefficients becomes the increase in Y in standard deviation units for every standard deviation increase in X
+ Standardised betas in simple regression are equal to the Pearson correlation between X and Y
+ Cautions
	+ Just because you can put regression coefficients on a common metric doesn’t mean they can be meaningfully compared.
	+ The SD is a poor measure of spread for skewed distributions, therefore, be cautious of their use with skewed variables

---
#  Simple regression in R 

+ In R we use the lm( ) function to conduct a linear regression
+ DV
+ IV
+ Name of dataset that contains IV and DV
![](assets/img/image14.emf)

---
#  Output from lm( ) 

+ **R** **2**
![](assets/img/image15.emf)

---
#  Output from lm( ) 

+ *F-* test
![](assets/img/image15.emf)

---
#  Output from lm( ) 

+ Regression coefficient for study hours
+ Standard error of regression coefficient
+ *t-* test for regression coefficient **
![](assets/img/image15.emf)

---
#  Confidence intervals 

+ The default is for confint ( ) to give 95% confidence intervals but this can be changed using the ‘level’ argument to give e.g., 99% confidence intervals
![](assets/img/image16.emf)

---
#  Standardised regression coefficients 

+ The scale( ) function can be used to standardise variables
![](assets/img/image17.emf)


---
#  Simple regression summary 

+ Simple regression models estimate the linear increase in a DV for every unit increase in a single IV
+ The method of **least squares** allows us to calculate the intercept and slope of the line that best characterises this linear relation
+ **R-squared** and **F-ratio** provide overall measures of model fit
+ **t-tests** or **confidence intervals** assess the significance of a slope coefficient
	+ For simple regression (with one IV), t-tests for the IV and the F-ratio are equivalent
+ **Standardised regression coefficients** can be used to put simple regression coefficients on a common metric


---
#  EXTENDING THE SIMPLE REGRESSION MODEL 



---
#  Multiple regression 

+ The aim of a regression model is to explain variance in a DV
+ In contrast to simple regression which uses only one IV, multiple regression uses multiple IVs
+ However, the multiple IVs are likely to be correlated with one another
+ Thus, multiple regression finds the optimal prediction of a DV from several IVs, *taking into account their redundancy with one another*


---
#  Uses of multiple regression 

+ ***For prediction*** **:** multiple IVs may allow us to better predict the DV than simple regression because different IVs might explain different parts of the variance in the DV
+ ***For theory testing*** **:** often our theories suggest that multiple variables together contribute to variation in an outcome
+ ***For covariate control:*** we might want to assess the effects of certain IVs controlling for the effects of other influences.
	+ E.g., effects of personality on health after removing the effects of age and sex

---
#  Multiple regression 

+ Example:
	+ In our simple regression example we were seeking to explain test performance (DV)
	+ Our IV was revision time
	+ But other things may help explain test score:
		+ Motivation
		+ Previous experience of subject matter
		+ Hours of sleep prior to the test
		+ Etc…
	+ If we were to gather data on each of these variables for our sample, we could use them to together explain test score variation in a multiple regression
![](assets/img/image18.jpeg)

---
#  Extending the regression model 

+ Our model for a single predictor:

… is extended to include additional Xs :
+ For each X, we have an additional b
	+  is the coefficient for the 1 st predictor, for the second etc.


---
#  Interpreting coefficients in multiple regression 

+ Given that we have additional variables, our interpretation of the regression coefficients changes a little
	+ is still the y-intercept
		+ But now it is the y-value when **all** IVs are 0.
	+ Each is now a **partial regression coefficient**
	+ It captures the change in Y for a one unit change in , **when all other IVs are held constant**
		+ ‘Holding constant’ refers to finding the effect of the IV when the values of the other IVs are fixed
		+ It may also be expressed as the effect of ‘controlling for’, ‘ partialling out’ , ‘residualizing for’ the other IVs
		+ As such, multiple regression isolates the effects of IVs from one another and estimates the ‘unique’ contributions of each IV to the outcome over and above the effects of the others


---
#  Least squares in multiple regression 

+ In simple regression, we could visualise the model as a straight line in 2D space
	+ Least squares finds the coefficients that produces the *regression line* that minimises the vertical distances of the observed y-values from the line
+ In a regression with  2 predictors, this becomes a regression plane in 3D space
	+ The goal now becomes finding the set of coefficients that minimises the vertical distances between the *regression*  *plane* and the observed y-values
+ The logic extends to any number of predictors
	+ (but becomes very difficult to visualise!)
+ Thus, least squares regression  can be used to find regression coefficients in multiple regression too.
![](assets/img/image20.png)


---
#  Example: multiple regression with 2 predictors 

+ Imagine we were interested in examining the effects of  cognitive ability and self-control on academic performance in school children
+ We collect data on a sample of n=200 5 th graders and fit a multiple regression model
+ We’ll fit the model to z-scores for the IVs and DV
![](assets/img/image21.jpeg)

---
#  Multiple regression using lm( ) 


> M1<- lm ( academic~Cog+SC , data= academic.data ) 
+ Multiple IVs are separated by ‘+’

---
#  Multiple regression coefficients 

+ *Controlling for self-control, for every SD unit increase in cognitive ability, there is a* ***0.57*** *SD unit increase in academic performance*
![](assets/img/image22.emf)


---
#  Multiple regression coefficients 

+ *Controlling for cognitive ability, for every SD unit increase in self-control, there is a* ***0.38*** *SD unit increase in academic performance*
![](assets/img/image22.emf)


---
#  OVERALL MODEL EVALUATION IN MULTIPLE REGRESSION 



---
#  R2 

+ Like in simple regression, we use R 2 for overall model evaluation.
+ The sums of squares used to calculate R 2 are defined in the same way as for simple regression.

or
+ The only difference is that is based on multiple IVs.

---
#  R2 

+ R 2 is then calculated in the same way as in simple regression:

Or:

---
#  R2 interpretation 

+ Like in multiple regression R 2 represents the proportion of variation in the DV accounted for by the model (all the IVs)
+ Its square root is now the multiple correlation coefficient between IVs and DV
+ The multiple correlation coefficient summarizes the shared relationship between a variable (Y) and a set of variables ( Xs ).
	+ It is the squared correlation between the observed Y and predicted Y values.

---
#  Adjusted R2 

+ We can also compute an adjusted R 2 for multiple regression.
+ This is needed because R 2 is an inflated estimate of the corresponding population value
+ Due to random sampling fluctuation, even when R 2 is zero in the population, its value in the sample will fluctuate from this
	+ However, because R 2 can’t be negative it only fluctuates in the positive direction
+ In **smaller samples** , the fluctuations from zero will be larger on average
+ With **more IVs** , there are more opportunities to add to the positive fluctuation
+ Therefore, an adjusted R 2 is available that adjusts for both sample size (N) and number of predictors (k):
+ It shrinks the original  R 2 to better reflect the corresponding population value


---
#  In our academic performance example… 

+ *Based on adjusted R-squared, our cognitive ability and self-control together explain* ***59%*** *of the variance in academic performance*
![](assets/img/image22.emf)


---
#  In our academic performance example… 

+ *As the sample size is large and the number of predictors small, adjusted and unadjusted R-squared are similar*
![](assets/img/image22.emf)


---
#  F-ratio 

+ Like in simple regression, the F-ratio is used to test the null hypothesis that *all* regression slopes are zero.
+ It is calculated in exactly the same way as in simple regression:
+ Where
	+ df model = k
	+ df resid . = N – k – 1
		+ N = sample size
		+ k = number of predictors

---
#  In our academic performance example… 

+ *F(2,197)=142.7, p<.001*
![](assets/img/image22.emf)

---
#  Multiple regression: interim summary 

+ Multiple regression extends simple regression to several, possibly correlated predictors
+ It accounts for the correlation between predictors and estimates the contribution of each IV controlling for the others
+ Like in simple regression, the intercept and slopes can be computed using the least squares method
+ R 2 and F-ratio are used for overall model evaluation
+ Adjusted R 2 shrinks the original R 2 to make it a more accurate reflection of the corresponding population value

---
#  Tasks for this week… 

+ **Problem set:** Multiple regression interpretation
+ **Lab:** Multiple regression
+ **Reading:**
	+ See reading list on LEARN
	+ No additional readings this week
+ **Homework:** Simple regression
	+ Live now, closes Sunday 17:00.
