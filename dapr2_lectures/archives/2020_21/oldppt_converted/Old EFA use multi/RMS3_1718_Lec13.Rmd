---
title: ' Lecture 13:Principal components analysis (PCA) '
subtitle: 'Research Methods & Statistics 3  Alex Weiss: alex.weiss@ed.ac.uk, B18 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today’s lecture 

+ Today we will look at PCA. We will cover:
	+ The basic overview of the procedure.
	+ Discuss some common uses in psychology.
	+ Outline some technical aspects of the procedure.

---
#  Scenario 

+ In our LBC data set, we have 5 variables that concern the health behaviors'. These are:
	+ Average alcohol intake per week.
	+ Pack years – how many packets of cigarettes smoked per year.
	+ Minutes exercise per week.
	+ Minutes sedentary per week.
	+ Rating of diet quality.
+ I am conducting a much bigger analysis, and I am not sure I want to use all 5 variables individually.
+ So I need a method to produce a summary for me.

---
#  What is PCA? 

+ PCA is a statistical procedure for data reduction.
+ PCA seeks to explain as much of the **total variance** present in a data set as possible .
+ It does this by calculating sets of linear composites of the original variables.
	+ PCA begins with our original data.
	+ Next, we calculate the covariance (correlations) between our variables.
	+ We then use a mathematical procedure called eigen -decomposition to calculate the composites.

---
#  What is PCA? 

+ If the variables are very closely related to one another (large covariances /correlations), then they can be well represented by fewer composites .
+ If the variables are not very closely related (low covariances /correlations), then a single linear composite will not do a good job of representing them .
+ To get to grips with this, we need to look a little bit at the eigen -decomposition.

---
#  Eigen decomposition 

+ Each principal component ( y i ), is a weighted composite of the original variables (our health behaviors'):
+ Where a ij ’s are weights relating the original variables (x’s) to the components.

---
#  Eigenvectors and Eigenvalues 

+ Each component has an eigenvector  and an eigenvalue.
+ Eigenvector:
	+ Contains the weights relating each variable to each component .
	+ There will be as many elements in this vector as you have variables.
		+ So for our example, each eigenvector will have 5 weights.
	+ Visually, the eigenvectors provide a direction in dimensional space.
+ Eigenvalue .
	+ This can be thought of as the “scale” of the component .
	+ Or in more familiar terms, the variance of the component.

---
#  Visualizing PCA: Data 


```{r tbl7, echo = FALSE}
tbl7 <- tibble::tribble(
~` `, ~`Diet`, ~`Exercise`,
"Diet","1.00"," ",
"Exercise","0.00","1.00",
" ","Diet","Exercise",
"Diet","1.00"," ",
"Exercise","0.80","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl7), font_size = 18)
```
.pull-left[![](assets/img/image3.png)]

.pull-right[![](assets/img/image2.png)]

---
#  Visualizing PCA: Eigenvectors 

.pull-left[![](assets/img/image5.png)]

.pull-right[![](assets/img/image4.png)]

---
#  Visualizing PCA: Eigenvectors 

.pull-left[![](assets/img/image7.png)]

.pull-right[![](assets/img/image6.png)]

---
#  Visualizing PCA: Eigenvalues 

.pull-left[![](assets/img/image9.png)]

.pull-right[![](assets/img/image8.png)]

---
#  Visualizing PCA: Eigenvalues 

+ Eigenvalue 1 = 1
+ Eigenvalue 2 = 1
.pull-left[![](assets/img/image11.png)]

.pull-right[![](assets/img/image10.png)]

---
#  Visualizing PCA: Eigenvalues 

.pull-left[![](assets/img/image13.png)]

.pull-right[![](assets/img/image12.png)]

---
#  Visualizing PCA: Eigenvalues 

+ Eigenvalue 1 = 1.8
+ Eigenvalue 2 = 0.2
.pull-left[![](assets/img/image15.png)]

.pull-right[![](assets/img/image14.png)]

---
#  Eigenvalues & variance 

+ Consider the eigenvalues for the two variables in the situations just discussed.
	+ In both cases, the sum of the eigenvalues was equal to 2 – which happens to be the number of items we have.
		+ When r = 0; 1+1 = 2
		+ When r = 0.8; 1.8 + 0.2 = 2
+ This is a general rule for eigenvalues.
	+ Sum of the eigenvalues = the number of variables in the data set ( *p* ).
	+ The reason for this is to do with the underlying matrix algebra of the eigen -decomposition.
		+ You do not need to know details, but if you would like to know more, we can provide reading.

---
#  Eigenvalues & variance 

+ The eigen -decomposition has transformed the original data, but the variance remains the same.
	+ The sum of the variances of each variable = total variance.
	+ So the sum of the eigenvalues must also = total variance.
+ So if we want to know the amount of variance any individual component accounts for :

---
#  Eigenvalues & variance 

+ Again in our two variable example:
	+ When r = 0.00;
		+ Component 1 = 1/2 = 0.50 = 50%
		+ Component 2 = 1/2 = 0.50  = 50%
	+ When r = 0.80;
		+ Component 1 = 1.8/2 = 0.90 = 90%
		+ Component 2 = 0.2/2 = 0.10 = 10%
+ So when we have a correlations present, a smaller subset of components can account for a majority of the variance .
	+ And this is where the **reduction** comes in.

---
#  Features of PCA solution 

+ There are as many components as original variables.
	+ Doesn’t make sense right? I thought it was data reduction?
	+ PCA seeks to e xplain ***total amount of variability.***
		+ Logically this will require as many components as variables .
	+ So our question becomes: What number of components accounts for ***enough variance*** in the original data such that it is a good reduction ?
+ Each component is variance maximizing
	+ Component 1 will account for more variance than component 2; component 2 more than component 3 etc.
	+ Thus they are sequentially decreases with respect to explained variance .
+ Uncorrelated with one another (orthogonal )

---
#  Eigenvectors and PCA loadings 

+ PCA loadings give the strength of relationship between each variable and each component.
	+ Loadings range from -1 to 1.
	+ The higher the loading, the more strongly associated with the component the variable is.
	+ The sum of the squared loadings for any variable on all components will equal 1.
		+ All variance in the variable is explained if we retain all components.

---
#  Eigenvectors and PCA loadings 

+ If the eigenvector weights are labelled a ij
	+ Where i = variable, and j = component
+ Loadings are derived from the eigenvectors as:
+ Essentially we are scaling the loadings so that the components with the highest eigenvalues have the highest loadings.

---
#  PCA Scores 

+ PCA scores are calculated in the same way as we produce predicted values from a regression equation.
	+ The a set of weights are calculated.
	+ These weights are multiplied by the observed value for an individual
	+ And the sum of these provides an individuals score on the component.

---
#  PCA Scores 

+ The weights are again derived directly from the eigenvectors.
+ So here we are standardizing the weights for the scores by dividing the weight by the square-root of the eigenvalue.
	+ This results in the scores having a unit variance.

---
#  Tasks for this week… 

+ **Problem** **set:** Glossary of key terms & PCA interpretation.

****
+ **Lab:** Walk through PCA with example to complete yourself.
	+ **Extra exercise:** Linking PCA to eigen decomposition.

****
+ **Reading:** See list on LEARN.
+ **Homework:** Homework is live now. Survey design.
	+ Closes Sunday at 17:00
