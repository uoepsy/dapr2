---
title: ' Lecture 19:Reliability '
subtitle: 'Research Methods & Statistics 3  Alex Weiss: alex.weiss@ed.ac.uk, F17 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Problem set reading 

+ Kleinheksel , A. J., & Ritzhaupt , A. D. (2017). Measuring the adoption and integration of virtual patient simulations in nursing education: An exploratory factor analysis. *Computers & Education* .

---
#  Todays lecture 

+ Measurement, reliability and the idea of a true score.
+ Types of reliability
	+ Over time
	+ Over alternative forms
	+ Over items
		+ Internal consistency
		+ Structural measures of reliability
	+ Over raters

---
#  Global credit/reference 

+ These slides heavily influenced by work of Bill Revelle .
	+ Psych package information.
	+ Chapter reference on the reading list
	+ Papers from the reading list

---
#  Measurement 

+ In psychology we aim to develop and use measures of constructs to test theories.
+ In doing so, we want the best measures possible.
	+ But data is always a combination of “true” measurement (or signal) of the intended construct.
	+ And “error” (or noise). The measurement of unintended aspects.

---
#  True score theory 

+ Suppose we have a test.
	+ We assume that this test measures some ability.
	+ We assume that in the world, there is a true value or score for this test for individuals.
	+ Reliability can then be thought of as how well our test score reflects this true score.
+ Spearman was the first to note that under certain assumptions, the correlation between two parallel tests of the same underlying construct provides an estimate of reliability.
	+ As we correlate more tests, we can begin to make fewer assumptions.

---
#  True score and parallel tests 

+ What are the assumptions?
	+ Parallelism
		+ Same relation to true score, same error variance .
	+ Tau equivalence
		+ Same relation to true score, error variance can differ.
	+ Congeneric
		+ 4+ tests, we can relax the relation assumption and just assume each is an imperfect measure.
+ So where do parallel tests come from?
	+ In the previous definitions this is abstract.
	+ But parallel can come from a number of sources:
		+ Time, raters or items

---
#  Alternative forms 

+ Correlation between two variants of a test.
	+ Same items given in a different order (randomization of experimental stimuli fits here).
	+ Tests with similar but not identical content.
		+ E.g. design a test which has one basic summation, subtraction, multiplication and division question.
		+ Ideally tests will have equal means and variances.
	+ Assumption may be such tests correlate perfectly.
		+ They will not, and to the extent they do not, we have a measure of reliability.

---
#  Alternative forms 

+ With 4 + tests, we can use the factor loadings from a factor model to give us the estimate of each tests relation to the latent variable.
	+ As in congeneric tests .
	+ This also provides us with information on reliability.
	+ It also links back to one reason we prefer to have more than 3 items relating to each factor!

---
#  Reliability over alternative forms 

+ Developing alternative forms has become increasingly easy.
	+ Write a large item bank.
	+ Get many respondents.
	+ Use Item Response Theory methodologies to assess item difficulty.
	+ Match based on this.
+ Core procedure in Computer Adaptive Testing (CAT).

---
#  Reliability over time: Test-retest  

+ Correlation between the same test taken at two (or more) points in time.
+ This is a corner-stone of test assessment and appears in many test manuals, but it poses some tricky questions:
	+ What is the appropriate gap between measurements?
	+ How stable should my construct be across time?
		+ Trait versus state debate.
		+ Note: Mean change may occur, but if rank order is maintained, correlations stay high.

---
#  Split-half reliability 

+ Steps:
	+ Randomly split a test into equal sub-sets of items.
	+ Score the items.
	+ Correlate the two scores.
+ Issue:
	+ Huge number of potential splits: 
	+ This number gets very big, very quickly.

---
#  Reliability over items 

+ If we take the idea of correlating subsets to its logical end, we reach the idea of reliability as the relation among items forming the composite.
	+ Sometimes termed internal consistency.
	+ Most well know estimate is Cronbach’s alpha

---
#  Reliability over items 

+ Comments on alpha:
	+ Sometimes used to suggest a set of items measure one unidimensional construct. This is not the case.
	+ Will increase as you add items and/or item correlation.
	+ Enter Spearman-Brown Prophecy formula:
+ Can be high for sets of items with no underlying factor.
	+ Very high if you simply repeat items.

---
#  Reliability over items 

+ Other measures of internal consistency:
	+ Omega hierarchical ( )
		+ Items may measure multiple things.
		+ We can call these general and group factors.
		+ Omega hierarchical estimates the amount of item variance which is general.
+ Omega total ( )
	+ Given different things are measured, we can think about estimating the total amount of reliable variance.

---
#  Reliability over raters 

+ Suppose we have a study where we ask a series of judges to rate a set of targets.
	+ We could ask family members to rate the personality of a target family member.
	+ We could ask your class mates to estimate your IQ.
+ In such situations we may want to know how consistent the raters are.
	+ i.e. how reliable their estimates, or the combined estimates are.
	+ This we can do with the intraclass correlation coefficient (ICC)

---
#  Reliability over raters 

+ To calculate ICC we can think of breaking variance in ratings into:
	+ Variance between subjects (across targets)
	+ Variance within subjects (across raters , same target)
	+ Variance due to raters (across targets, same rater ).
+ Dependent on our intention, we can calculate ICCs based on these breakdowns of variance.

---
#  Types of ICC 


```{r tbl17, echo = FALSE}
tbl17 <- tibble::tribble(
~`ICC`, ~`Description`,
"ICC(1,1)","Targets rated by a different set of randomly selected raters and reliability is based on one measurement",
"ICC(1,k)","As (1,1), but with reliability calculated as the average of k raters’ measurements.",
"ICC(2,1)","Each target measured by each rater. Raters are considered representative of larger pool of raters. Reliability calculated from a single measurement.",
"ICC(2,k)","As (2,1), but with reliability calculated as the average of k raters’ measurements.",
"ICC(3,1)","Each target measured by each rater. Raters only raters of interest. Reliability calculated from a single measurement.",
"ICC(3,k)","As (3,1), but with reliability calculated as the average of k raters’ measurements."
)

kableExtra::kable_styling(knitr::kable(tbl17), font_size = 18)
```

---
#  Reliability in R 


```{r tbl18, echo = FALSE}
tbl18 <- tibble::tribble(
~`Reliability over`, ~`Reliability estimate`, ~`R function`,
"Forms","Alternate forms","cor (X,Y)",
"Time","Test-retest","cor (T1,T2)",
"Split halves","Random split","splitHalf",
" ","Worst split","splitHalf",
" ","Best split","splitHalf",
"Items","General factor","omega",
" ","Average","alpha",
"Raters","All variants","ICC"
)

kableExtra::kable_styling(knitr::kable(tbl18), font_size = 18)
```

---
#  Uses of reliability 

+ Generally, it is good to know how reliable a measure is.
	+ It has implications for validity we will discuss next lecture.
+ But knowing reliability also allows us to calculate a correction for attenuation due to less than perfect reliability.
	+ Relationship between X and Y attenuated (reduced) by reliability of X and Y.

---
#  Tasks for this week… 

+ **Problem** **set:** Glossary and begin exam note preparations.
+ **Lab:** Weeks 3 to 5 you will be doing a large EFA analysis.
	+ This week you will be looking at the reproducibility of a section of your solution and on reliability estimates.
+ **Reading:** Reading list on LEARN.
+ **Homework:** Factor analysis 2.
	+ Live now, closes Sunday at 17:00
