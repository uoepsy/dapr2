---
title: ' Lecture 15:Data checks and Estimation '
subtitle: 'Research Methods & Statistics 3  Alex Weiss: alex.weiss@ed.ac.uk, B18 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today’s lecture 

+ Overview steps in a factor analytic study
+ Discuss data checks and suitability.
+ Discuss selection of method of estimation of factor model.
	+ Relate this to our survey/questionnaire design choices.

---
#  Steps in FA 


**Before all of this, a decision is made on the data reduction approach.**
+ Check the appropriateness of the data and decide of the appropriate estimator .
+ Decide which methods to use to select a number of factors.
+ Decide conceptually whether to apply rotation and how to do so.
+ Decide on the criteria to assess and modify your solution .
+ Run the analysis.
+ Evaluate the solution (apply 4)
+ Select a final solution and interpret the model, labelling the factors.
+ Report your results.

---
#  Conceptual Example 

+ Let's say we have written a set of items which measurement organizational commitment :
+ Feedback is regularly given to employees about suggestions they have made.
	+ Employees' suggestions are seriously taken into consideration.
	+ I have regular meetings with my supervisor/manager to talk about my work schedule and responsibilities.
	+ Supervisors/managers regularly congratulate employees in recognition of their efforts.
	+ It would be too costly for me to leave my organisation now.
	+ A serious consequence of leaving my job would be the scarcity of viable alternatives .
	+ Too much in my life would be disrupted if I decided to leave my organisation now .
	+ It would be very hard for me to leave my organisation right now even if I wanted to.


---
#  Conceptual Example: Correlations 


```{r tbl4, echo = FALSE}
tbl4 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl4), font_size = 18)
```


---
#  Conceptual Example: Correlations 


```{r tbl5, echo = FALSE}
tbl5 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl5), font_size = 18)
```


---
#  Conceptual Example: Correlations 


```{r tbl6, echo = FALSE}
tbl6 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl6), font_size = 18)
```


---
#  Conceptual Example: Factor loadings 

+ Factor loading:
	+ The magnitude of the relationship between the item and the factor.
	+ Squared it provides the amount of variance in the item accounted for by the common factor.
	+ Range from -1 to 1.
+ Primary loading:
	+ The largest, ideally hypothesized, loading
+ Cross-loading:
	+ Loading of the item on the remaining factors.
+ **Remember, all items load on all factors.**

```{r tbl7, echo = FALSE}
tbl7 <- tibble::tribble(
~`Item`, ~`Factor 1`, ~`Factor 2`,
"1",".22",".61",
"2",".14",".69",
"3",".22",".87",
"4",".26",".72",
"5",".93","-.18",
"6",".59","-. 05",
"7",".80","-.09",
"8",".71","-.07"
)

kableExtra::kable_styling(knitr::kable(tbl7), font_size = 18)
```


---
#  Conceptual Example: Interpretation 

+ Item set 1: Communication
+ Feedback is regularly given to employees about suggestions they have made.
+ Employees' suggestions are seriously taken into consideration.
+ I have regular meetings with my supervisor/manager to talk about my work schedule and responsibilities.
+ Supervisors/managers regularly congratulate employees in recognition of their efforts.
+ Item set 2 : Costs of leaving
+ It would be too costly for me to leave my organisation now.
+ A serious consequence of leaving my job would be the scarcity of viable alternatives.
+ Too much in my life would be disrupted if I decided to leave my organisation now.
+ It would be very hard for me to leave my organisation right now even if I wanted to.


---
#  Conceptual Example: Rotation 




---
#  Conceptual Example: Rotated factor loadings 

+ Unrotated loading matrix:
	+ Structure matrix
+ Rotated loading matrix:
	+ Pattern matrix
+ Look at the general effect of rotation:
	+ Primary loadings are bigger
	+ Cross-loadings are smaller
	+ Therefore solution is more “simple”
	+ And thus easier to interpret

```{r tbl10, echo = FALSE}
tbl10 <- tibble::tribble(
~`Item`, ~`Factor 1 Rotated`, ~`Factor 1 Unrotated`, ~`Factor 2 Rotated`, ~`Factor 2 Unrotated`,
"1",".04",".22",".64",".61",
"2","-.05",".14",".71",".69",
"3","-.02",".22",".90",".87",
"4",".06",".26",".75",".72",
"5",".95",".93","-.04","-.18",
"6",".58",".59",".04","-. 05",
"7",".79",".80",".03","-.09",
"8",".71",".71",".03","-.07"
)

kableExtra::kable_styling(knitr::kable(tbl10), font_size = 18)
```


---
#  Steps in FA 


**Before all of this, a decision is made on the data reduction approach.**
+ **Check** **the appropriateness of the** **data and decide** **of the appropriate** **estimator** **.**
+ Decide which methods to use to select a number of factors.
+ Decide conceptually whether to apply rotation and how to do so.
+ Decide on the criteria to assess and modify your solution .
+ Run the analysis.
+ Evaluate the solution (apply 4)
+ Select a final solution and interpret the model, labelling the factors.
+ Report your results.

---
#  Assumption and data checks: Overview 

+ Conceptually and theoretically, are the variables suited to factor analysis.
	+ Garbage in garbage out.
+ Assumptions of regression/correlation – the model
+ Are the magnitudes of the correlations big enough?
	+ Kaiser-Meyer-Olkin test; Bartlett's test
+ Sample size


---
#  Assumption and data checks: Conceptual considerations 

+ Factor analysis is a statistical procedure.
	+ It can't tell you if what you are doing makes sense.
	+ It will give you an answer no matter what.
+ Whether it makes sense to ask whether a set of variables has a common cause is the decision of the researcher **before** an analysis starts .


---
#  Assumption and data checks: Conceptual considerations 

+ We must pay attention to the variables we enter into factor analysis.
	+ We are looking to understand **common causes** of item responses.
	+ Is it theoretically reasonable to assume a common cause exists to our variable set?
	+ If not, then EFA may not be appropriate
		+ Here we may think about PCA or other data reduction methods.
+ Garbage in, garbage out!!


---
#  Model related assumptions 

+ These are very similar to those made in regression.
	+ Factor analysis is really just regression at heart.
	+ Imagine the factor as the IV, and all the items as DVs, estimated all at once.
+ Specifically:
	+ Residuals should be uncorrelated across items.
	+ The residuals should not correlate with the factor.
	+ In the standard model, relationships between items and factors should be linear.
+ These are not often tested in practice, but it is useful to recognize they are there as it helps cement the idea of FA as a model for the data.

---
#  Model related assumptions 

+ Factor analysis is based on the correlations between items.
+ So if we use a Pearson correlation, we need data to:
	+ Approximate continuous measurement.
	+ Approximate normality.
+ Our design choices could effect the magnitude and pattern of correlations.
	+ This will effect the results of our analysis.
	+ Think about the method effects, biases etc. discussed in week 1.
	+ All could impact the size and pattern of correlations.
	+ We must be aware of this when interpreting results.


---
#  Testing magnitude of correlationsKaiser-Meyer-Olkin test 

+ Based on ratio of squared correlation to squared partial correlation.
+ Logic:
	+ If the variables share a common cause then the partial correlations should be small compared to the zero-order correlations.
	+ Partial correlation is defined as the correlation between 2 variables controlling for all other items.
+ Interpretation:
	+ Ranges between 0 and 1.
	+ Values close to 1 suggest partial correlations are small compared to zero-order.
	+ Values >0.80 considered good for factor analysis.


---
#  Testing the magnitude of correlations:Bartlett's test 

+ Significance test for the degree to which the observed correlation matrix differs from an identity matrix.
+ **Identity Matrix:**
	+ Matrix where there is no association between variables.
	+ 1's on diagonal
	+ 0's everywhere else

```{r tbl18, echo = FALSE}
tbl18 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`,
"Item 1","1.00","0.00","0.00",
"Item 2","0.00","1.00","0.00",
"Item 3","0.00","0.00","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl18), font_size = 18)
```


---
#  Testing the magnitude of correlations:Bartlett's test 

+ Steps:
	+ Calculate the determinant of the observed matrix
	+ Calculate the determinant of an identity matrix.
		+ This is always 1.0.
	+ Compare the two determinants using a chi-square test.
+ A significant Bartlett's test says:
	+ Formally: The determinants are significantly different.
	+ Practically: That there are observed correlations bigger than 0.


---
#  Method of factor extraction 

+ Once we have decided we have suitable data, next we must decide ***how*** to do our factor analysis.
+ There are a wide array of approaches to conducting factor analysis.
	+ Many modern methods will refer to different *estimators* (you have already come across maximum likelihood).
+ It is beyond the scope of an introductory set of lectures to get into the details of these, but we will outline two different approaches.

---
#  Method of factor extraction:Principal axis factoring 

+ Principal axis factoring is perhaps the simplest form of FA.
+ It differs from PCA only in the use of a reduced input matrix.
+ The reduced matrix includes communality estimates on the diagonal.
+ The inclusion of communalities means as the unique variance in the data increases, the PCA and principal axis solution will differ more.


---
#  Method of factor extraction: EFA - principal axis factoring 

+ PCA
+ EFA - Principal Axis Factoring

```{r tbl22, echo = FALSE}
tbl22 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`,
"Item 1","1.00",".60",".55",".45",
"Item 2",".60","1.00",".61",".48",
"Item 3",".55",".61","1.00",".71",
"Item 4",".45",".48",".71","1.00",
" ","Item 1","Item 2","Item 3","Item 4",
"Item 1",".42",".60",".55",".45",
"Item 2",".60",".47",".61",".48",
"Item 3",".55",".61",".61",".71",
"Item 4",".45",".48",".71",".51"
)

kableExtra::kable_styling(knitr::kable(tbl22), font_size = 18)
```


---
#  Method of factor extraction:EFA - maximum likelihood 

+ Maximum likelihood estimation:
	+ We have a set of things we want to have an estimate of in our model.
		+ In factor analysis, this is primarily our factor loadings.
			+ How the latent factors relate to each the variables we have measured.
		+ Generally referred to as parameters.
	+ Maximum likelihood looks to find the best values (most probable estimates) for all of the parameters at the same time.


---
#  Method of factor extraction:EFA - maximum likelihood 

+ This makes ML a more complex way of estimating our factor solution, but it provides optimal solutions.
+ The issue is that for big analyses, sometimes it is not possible to find values for factor loadings that = MLE estimates.
	+ Referred to as non-convergence (you may see warnings)
	+ Also may produce solutions with impossible values
		+ Factor loadings > 1.00 (Heywood cases), thus negative residuals.
		+ Factor correlations > 1.00
+ In such situations, simpler methods like principal axis factoring may be preferable.

---
#  What if data is not continuous? 

+ If data are continuous, we can use Pearson correlations.
+ Why might they not be continuous?
	+ Think about study designs from week 1.
	+ In particular, think about the response scales you (or whoever designed the questionnaire) have chosen to use…

---
#  Categorical data and FA 

![](assets/img/image1.png)

---
#  What if data is not continuous? 

+ When we have enough categories (research indicates > 4 or 5), maximum likelihood is not a biased estimator as long as the data are not very skewed.
+ If we have fewer categories, we can still conduct factor analyses, but we need a different approach:
	+ Use a polychoric correlation matrix (easily do-able in R)
	+ Use an item response theory model
		+ We will not discuss this in this class.
		+ But I can provide reading to anyone who is interested.

---
#  Assumption and data checks:Sample size 

+ Finally, a quick comment on sample size.
	+ This is a much debated topic for factor analysis (see reading list)
+ Preacher & MacCallum (2002) note the primary question is what N is required to achieve stable, accurate interpretable factor loadings.
+ This is influenced by:
	+ Number of factors
	+ Number of variables
	+ Level of communality amongst variables
	+ Degree of model error (lack of model fit)
+ Often see ratio's of variables to sample size mentioned in the literature.
	+ This is far too simplistic and should not be relied on.


---
#  Tasks for this week… 

+ **Problem** **set** **:** Glossary, simple EFA code and results interpretation.
+ **Lab** **:** Weeks 3 to 5 you will be doing a large EFA analysis.
	+ This week you will start with data checks and no. of factors
+ **Reading:** Reading list on LEARN.
+ **Homework** **:** PCA.
	+ Live now, closes Sunday at 17:00
