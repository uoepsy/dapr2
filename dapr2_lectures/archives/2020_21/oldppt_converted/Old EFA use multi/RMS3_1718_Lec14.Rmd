---
title: ' Lecture 14:PCA in R & Intro to Factor Analysis '
subtitle: 'Research Methods & Statistics 3  Alex Weiss: alex.weiss@ed.ac.uk, B18 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Today’s lecture 

+ Run through of a PCA analysis in R with interpretation.
+ Introducing EFA and how it differs from PCA.

---
#  PCA in R 

+ Although the underlying calculations are quite complex, implementing PCA is quite straightforward:
	+ For data checks we simply need a sense of the correlations – cor ()
	+ For the required number of components we can make a subjective interpretation of the eigenvalues
		+ Scree plot.
	+ And run PCA direct using principal()
		+ Or we can build from the eigen -decomposition using eigen ()
		+ See additional lab exercises.
+ Next lecture we will run through an example.

---
#  Scenario 

+ In our LBC data set, we have 5 variables that concern the health behaviors'. These are:
	+ Average alcohol intake per week.
	+ Pack years – how many packets of cigarettes smoked per year.
	+ Minutes exercise per week.
	+ Minutes sedentary per week.
	+ Rating of diet quality.
+ I am conducting a much bigger analysis, and I am not sure I want to use all 5 variables individually.
+ So I need a method to produce a summary for me.

---
#  Worked example of pca 



---
#  What is EFA? 

+ FA is a statistical model for the data.
	+ Contrasts with PCA which is a mathematical transformation of original data.
+ FA seeks to explain the pattern of correlations present in a matrix.
	+ Contrasts with PCA which just looks to find composites which explain maximal variance.


---
#  Conceptual Example: Correlations 


```{r tbl6, echo = FALSE}
tbl6 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl6), font_size = 18)
```


---
#  Conceptual Example: Correlations 


```{r tbl7, echo = FALSE}
tbl7 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl7), font_size = 18)
```


---
#  Conceptual Example: Correlations 


```{r tbl8, echo = FALSE}
tbl8 <- tibble::tribble(
~` `, ~`Item 1`, ~`Item 2`, ~`Item 3`, ~`Item 4`, ~`Item 5`, ~`Item 6`, ~`Item 7`, ~`Item 8`,
"Item 1","1.00"," "," "," "," "," "," "," ",
"Item 2",".60","1.00"," "," "," "," "," "," ",
"Item 3",".55",".61","1.00"," "," "," "," "," ",
"Item 4",".45",".48",".71","1.00"," "," "," "," ",
"Item 5",".10",".01",".04",".13","1.00"," "," "," ",
"Item 6",".05",".00",".08",".20",".52","1.00"," "," ",
"Item 7",".14",".02",".11",".14",".76",".51","1.00"," ",
"Item 8",".07",".11",".13",".04",".68",".54",".48","1.00"
)

kableExtra::kable_styling(knitr::kable(tbl8), font_size = 18)
```


---
#  What is EFA? 

+ FA supposes that such patterns are meaningful .
	+ Contrasts to PCA in that no underlying theoretical assumptions about the components.
+ The **factors** can be thought of as unmeasured common causes of the patterns of correlation (latent variables).


---
#  Latent variable 

+ Key concept in much of psychometrics
	+ Of which FA is part.
+ A latent variable is commonly theorized to be an unmeasured common cause of responses to a set of variables.
	+ E.g. Cognitive ability, Extraversion etc.
	+ “Identified” by virtue of correlations between measured variables.
+ The concept of a latent variable is one feature which distinguishes EFA from PCA.

---
#  Visual Representation of FA and PCA 

+ PCA – composite variable
+ FA latent variable
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ COMP
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ LV


---
#  Visual Representation of FA and PCA 

+ PCA – composite variable
+ FA latent variable
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ COMP
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ LV
+ **An aside:** These arrows represent the PCA loadings discussed yesterday and earlier


---
#  Visual Representation of FA and PCA 

+ PCA – composite variable
+ FA latent variable
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ COMP
+ V1
+ V6
+ V2
+ V3
+ V4
+ V5
+ LV
+ **An aside:** And these are the equivalent factor loadings – which we will talk more about next week.


---
#  What is EFA? 

+ Did you wonder what those extra circles and arrows were in the EFA diagram?
+ Well they are important for a further distinction and feature of EFA.
+ FA is concerned with explaining **common** **variance** in a set of variables.
	+ Contrasts to PCA which concerns total variance.


---
#  Item variance 

+ In practice we are not able to separate specific variance and error variance.
+ As we have previously noted, PCA concerns total variability. So no distinction is needed.
+ In EFA, we attempt to model and understand the structure of common variance only.
![](assets/img/image1.png)


---
#  Item variance 

+ To borrow some terminology from R output, so it is familiar:
+ H 2 = common variance
+ U 2 = unique variance
+ Total variance = H 2 + U 2
+ V1
+ LV
+ Item = total variance
+ Factor loading tells us about common variance
+ Residual = unique variance
![](assets/img/image1.png)


---
#  Equation for Factor Model 

+ The general equation for the factor model:
+ Where:
	+ S = observed covariance matrix (the data)
	+ <U+039B> = Matrix of loadings (relate factors to items)
	+ <U+03A8> = matrix latent variables (the latent factors)
	+ T = unique variances or error (what is left)

---
#  Communalities 

+ Given we are only concerned with common variance, we need a way to estimate it.
+ The communality tells us how much variance in an item is explained by the other variables, or factors .
	+ The degree of common variance.
+ Estimating communalities is more difficult than it may appear as the population communalities are unknown.
	+ Communality has a range of 0 (no shared variance) to 1 (all variance is shared).
		+ There is no such thing as negative variance.
	+ On occasion communality estimates will be greater than 1. This is referred to as a Heywood Case.


---
#  Communalities 

+ The square multiple correlation (SMC) of a variable with all other variables is a practical lower bound.
+ The reliability of the variable has been proposed as a more reasonable upper bound to communality than unity.
	+ Unreliable (error) variance can not be related to the factor.
+ Other estimates include:
	+ Unities in a truncated solution
	+ Largest correlations
	+ Iterative procedures


---
#  So what is EFA? 

+ FA is a statistical model for the data.
+ FA seeks to explain the pattern of correlations present in a matrix.
+ FA supposes that such patterns are meaningful.
+ The **factors** can be thought of as unmeasured common causes of the patterns of correlation (latent variables).
+ FA is concerned with explaining **common variance** , not total variance (as in PCA).


---
#  And how does EFA contrast with PCA? 

![](assets/img/image3.png)


---
#  Why are they often equated? 

+ PCA and EFA are often considered to be the same thing.
	+ This is wrong.
+ So why do people equate them?
	+ Often simply because the solutions from the two procedures look similar.
	+ Not a strong reason (see Widaman , 1993; & discussion in Multivariate Behavioural Research from reading list).
+ Conceptually , the previous slides should show how the methods differ .
	+ Observed similarities are more likely a result of features of the data.

---
#  Tasks for this week… 

+ **Problem** **set:** Glossary of key terms & PCA interpretation.

****
+ **Lab:** Walk through PCA with example to complete yourself.
	+ **Extra exercise:** Linking PCA to eigen decomposition.

****
+ **Reading:** See list on LEARN.
+ **Homework:** Homework is live now. Survey design.
	+ Closes Sunday at 17:00
