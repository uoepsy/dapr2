---
title: ' Lecture 16:Number of factors and rotation '
subtitle: 'Research Methods & Statistics 3  Alex Weiss: tom.booth@ed.ac.uk, B18 7 George Square '
author: Tom Booth
date: 2020-06-23
output:
  xaringan::moon_reader:
  lib_dir: libs
  nature:
    highlightStyle: github
    highlightLines: true
    countIncrementalSlides: false
---
#  Todays lecture 


**Before all of this, a decision is made on the data reduction approach.**
+ Check the appropriateness of the data and decide of the appropriate estimator .
+ **Decide** **which methods to use to select a number of factors.**
+ **Decide conceptually whether to apply rotation and how to do so.**
+ Decide on the criteria to assess and modify your solution .
+ Run the analysis.
+ Evaluate the solution (apply 4)
+ Select a final solution and interpret the model, labelling the factors.
+ Report your results.

---
#  Number of factors problem:Overview 

+ Aim is to find the optimal number of factors to retain given our data .
+ This is often theoretically driven:
	+ Although EFA is by definition “exploratory” so is often conducted when we do not have strong theory.
+ There is no “correct” number.
	+ Many people discuss the problem in this way.
	+ It is not about correct number, but about optimal for the data and theoretically coherent .
	+ A questionnaire designed to measure 5 factors, may have a different optimal number in a given sample.


---
#  Number of factors problem:Overview 

+ Under-extraction
	+ Retain fewer than optimal number of factors.
	+ Meaning of factors sometimes harder to ascertain.
		+ “Smaller” factors squashed together
+ Over-extraction
	+ Retain more than the optimal number of factors.
	+ Factors retained with only a small number of primary loadings.
	+ Related to the retention of “difficulty factors”
+ Different methods have tendencies to under- or over-extract.

---
#  Number of factors problem:Kaiser Criterion 

+ Retain factors with eigenvalue >1
	+ Logic here is that as there are as many eigenvalues as items, the mean eigenvalue is 1.
	+ Therefore we retain all factors which explain more variance than the average (or than a single item).
+ Limitations:
	+ Very poor performance in simulation studies.
	+ Strong tendency to over-extract.
+ Recommendations:
	+ I do not use Kaiser criterion!


---
#  Number of factors problem:Scree Plots 

+ Plot the eigenvalues in descending order.
	+ Retain those above a ‘kink’ in the plot (a sudden decrease in eigenvalues).
+ Limitations:
	+ Subjective, inconsistent performance in simulation studies, not always an obvious kink or sometimes more than one kink.
+ Recommendation:
	+ Use only as an adjunct to other methods, as a graphical display of results


---
#  Number of factors problem 

.pull-left[![](assets/img/image1.png)]

.pull-right[![](assets/img/image2.png)]


---
#  Number of factors problem 

+ **Minimum Average Partial (MAP):**
	+ Compute a correlation matrix.
	+ Compute average squared correlation (M 0 )
	+ Compute the partial correlation matrix accounting for 1 component ( M 1 )
	+ Compare M 0  to M 1
	+ Continue extracting factors ( M 2, M 3, M 4 ) etc. etc. until the average of the partial correlation matrix is at a minimum.
		+ That is when we observe that the value of M k is higher than M k-1
	+ Tendency to under-extract.


---
#  Number of factors problem 

+ **Parallel Analysis (PA):**
	+ Generate random eigenvalues from uncorrelated data with the same N and number of items.
	+ Do this many times and get a distribution of the random eigenvalues.
	+ Compare this distribution to the observed eigenvalues.
	+ Retain only those factor with eigenvalues greater than the random values.
	+ Tendency to over-extract.


---
#  Number of factors problem:So how do we decide...  

+ No method will give you ***the absolute*** correct answer.
+ So, to decide:
	+ Use multiple methods you believe are the most robust.
	+ Use this information to highlight a range of solutions
	+ Explore each of these solutions given your criteria for an optimal solution
		+ Variance explained; suitable number of loadings; small cross-loadings; appropriate factor correlations.
+ **But most important is the theoretical consistency of the factors.**


---
#  Factor Rotation 

+ Primary goal of factor rotation is to provide maximally interpretable factor solutions based on some criterion.
+ However, there is an associated and very important issue with rotation, namely ***rotational indeterminacy*** .


---
#  Factor Rotation: Rotational indeterminacy 

+ The problem is simply stated:

*Given an initial solution, there exists an infinite number of pairs of factor loading and factor score matrices which will fit the data equally well, and are thus indistinguishable by any numerical criteria.*
+ In other words, there is not a unique solution to the factor problem.


---
#  Factor Rotation: Simple Structure 

+ Thurstone’s Simple Structure Conditions:
	+ Each variable (row) should have at least one zero loading.
	+ Each factor (column) should have the same number of zero’s as there are factors.
	+ Every pair of factors (columns) should have several variables which load on one factor, but not the other.
	+ Whenever more than four factors are extracted, each pair of factors (columns) should have a large proportion of variables which do not load on either factor.
	+ Every pair of factors should have few variables which load on both factors.

*Adapted from Sass & Schmitt (2011)*


---
#  Factor Rotation: Simple Structure 

![](assets/img/image3.png)


---
#  Factor Rotation: Methods: Broad categories 

+ Orthogonal rotations:
	+ Force the extracted factors to be uncorrelated
	+ Examples : Varimax
+ Oblique rotations:
	+ Allow the factors extracted to correlate.
	+ Examples : Direct Oblimin ; Promax
+ Different rotation methods seek to aid interpretation by focusing on different of the simple structure criteria.


---
#  Factor Rotation: Which to choose? 

+ Simplistic distinction is between orthogonal (uncorrelated) and oblique (correlated) solutions.
	+ This should be a relatively easy decision on theoretical grounds.
+ Generally I suggest always using oblique rotations.
	+ Oblique solutions allow data to be orthogonal if it is truly orthogonal.
	+ Orthogonal rotations force this structure on the data.
+ Within oblique rotations, very subtle differences.
	+ Without considering how algorithms work, a simple solution is to run your model with a couple of methods, and note if major differences occur.


---
#  Factor Rotation:The effect 

+ ORIGINAL CORRELATIONS
+ FACTOR SOLUTION
+ NO ROTATION
.pull-left[![](assets/img/image5.png)]

.pull-right[![](assets/img/image4.png)]

---
#  Factor Rotation:The effect 

+ ORIGINAL CORRELATIONS
+ FACTOR SOLUTION
+ ORTHOGONAL ROTATION
.pull-left[![](assets/img/image6.png)]

.pull-right[![](assets/img/image4.png)]

---
#  Factor Rotation:The effect 

+ ORIGINAL CORRELATIONS
+ FACTOR SOLUTION
+ OBLIQUE ROTATION
.pull-left[![](assets/img/image7.png)]

.pull-right[![](assets/img/image4.png)]

---
#  Should we rotate? 

+ Pro’s:
	+ Rotations do provide location for factor axes which result in patterns of factor loadings which are interpretable.
	+ But it is important to remember your chosen rotation/location is numerically equivalent to any other - and thus, its veracity is largely subjective.
+ Con’s :
	+ Factor correlations make some aspects of interpretation more complex.
	+ In the context of PCA, the rotated solution no longer represents the principal components

---
#  Should we rotate? 

+ To elaborate slightly on the first of the con’s on the previous slide, when we have an obliquely rotated solution, we need to draw a distinction between the pattern and structure matrix.
	+ **Pattern** **Matrix:** matrix of regression weights (loadings) from factors to variables.
	+ **Structure** **Matrix:** matrix of correlations between factors and variables .
+ When we orthogonally rotate, the pattern and structure matrix are the same.
+ When we obliquely rotate, the structure matrix is the pattern matrix multiplied by the factor correlations.


---
#  Tasks for this week… 

+ **Problem** **set** **:** Glossary, simple EFA code and results interpretation.
+ **Lab** **:** Weeks 3 to 5 you will be doing a large EFA analysis.
	+ This week you will start with data checks and no. of factors
+ **Reading:** Reading list on LEARN.
+ **Homework** **:** PCA.
	+ Live now, closes Sunday at 17:00
