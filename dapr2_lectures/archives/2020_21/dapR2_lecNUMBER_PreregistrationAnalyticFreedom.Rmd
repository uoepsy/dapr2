---
title: "<b> Preregistration and Analytic Freedom </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "AY 2020-2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  extra_css = list(".scroll-ouput" = list("height" = "90%", "overflow-y"="scroll")),
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```




# Learning Objectives

- What is preregistration and why would we preregister studies. 

- Limitations to preregistration and the relationship between preregistration and exploratory analyses

- Concrete example of the benefits and difficulties of preregistration



---
# The Issue: Standard Scientific Practice Involves Analytic Freedom



- Stopping criterion: How many participants should I collect? Is 20 enough? 100?

--

- Exclusion criteria: Are there participants whom I should exclude? How would I figure that out? Is there a "bad" participant

--

- Analysis choices: What covariates should I include in my statistical model? How should I treat my dependent variable -- should I transform it in some way? How would that impact the conclusions I draw

--

- Family of comparisons: Do I have a key dependent variable or many I'm interested in? What are the implications of looking at one vs. many? 

---
# Nosek et al. 2018

"A vast number of choices in analyzing data could be made. If those choices are made during analysis, observing the data may make some paths more likely and others less likely. By the end, it may be impossible to estimate the paths that could have been selected had the data looked different" 

---
# Proposed Solution: Preregistrations

- Conduct exploratory or background research

--

- Form a prediction having explored some initial data / read the literature specifying the analysis (es) you aim to run completely.

- For example, DV ~ IV1 + IV2 vs. DV ~ IV1*IV2

--

- Test (either based on fit or test statistic) your model on this new data after having registered it

--

- Warning: This is more challenging than you might think for all but the simplest designs! And the less you know going into the study the harder these registrations will be. 

---
# Several ways to register your predictions

- https://aspredicted.org/

- https://osf.io/

---
# What are included in registrations

- There are several templates for different branches of research, but the area you work in will likely affect what makes the most sense to use 

- For social psychology, one template from vant Veer, A.E., and Giner-Sorolla, R. (2016)

--

- This template asks for information like the following:

  - Describe the hypotheses in terms of directional relationships between manipulated/measured variables

--

  - For interactions, describe the shape these interactions are likely to take

--

  - If you are manipulating a variable, make predictions for successful manipulation check variables or explain why no manipulation check is included 

--

  - Describe the analyses which will test the main predictions and for each one include the relevant variables of interest, how they are calculated, the statistical technique / model and the rationale for including or omitting covariates
  
  
---

# Other templates

More templates: https://osf.io/zab38/wiki/home/

- OSF Preregistration page: https://osf.io/k5wns/

- OSF How-To and Resources: https://www.cos.io/initiatives/prereg?ga=2.85949657.1114272946.1607948696-474133586.1547657474

---

# Exploratory Analyses 

- Exploration is good 

  - Do not let concerns about preregistration interfere with asking questions about your data you didn't think of when you first registered your analyses!

  - In fact, exploration is very useful for discovering patterns in data that were not predicted and motivating future confirmatory research 

--

- But, preregistrations make it clear (mostly to your future self but also to others) what analyses you considered primary when you began the project, and what was comparatively secondary. 

---

# Many Challenges

- Writing analysis plans is difficult and takes quite a lot of time 
  
  - Foreseeing contingencies
  
--
  
  - The documents one creates can be pretty long
  
--
  
  - Upside: Once you get the data, the analysis is pretty fast. You've already thought a lot about the analyses you will run and even written the R script 
  

---

# Many Challenges

- With very new projects, it can be difficult to know exactly what to preregister

  - Initial preregistrations can be quite general and thus might not be completely convincing to the skeptical reader. But that doesn't mean they aren't worth doing. 
  
--
  
  - Subsequent preregistrations of direct replications or follow-up experiments will often be more specific and constrained 
  
--
  
  - Deviations and amendments are just fine, but the point is to be transparent about those deviations

---

# What preregistration does not fix

 - Deliberate dishonesty 
 
 - Preregistering predictions after you've already looked at the data
 
 - Ignoring the preregistration (e.g., dropping a dependent variable that was a central part of your analytic plan)
 
 - Results that don't generalize because of biased sampling, poorly designed experiments, or all the other stuff that can go wrong in science

---

# Working through an example from an actual preregistration I wrote but haven't look at in three years

- First, what is change my view: https://www.reddit.com/r/changemyview/comments/mas8o3/cmv_once_fully_vaccinated_for_covid19_you_can/

- https://osf.io/jdxa8/


---
# Read data into R



The file path used here will depend on where your file is saved, but something like the following will work:

```{r}
library(tidyverse)
Preregistration_Dataset_Example <- read_csv("preregistration_reddit.csv")
head(Preregistration_Dataset_Example)
```

---


```{r}
library(summarytools)
view(dfSummary(Preregistration_Dataset_Example))
```

---

We can see looking at some of these variables, Comments, Dacs, Deltas are all distributed non-normally

  - In our preregistration, we anticipated that this count variable would not be normally distributed and we would likely need to fit either a poisson model or a negative binomial model

```{r}
library(MASS)
```
---

- The most basic thing we expected was for non-social moral posts to have more deltas than social moral posts. These categorizations were prespecified by handcoding the dataset. 

- So we need to fit a model to test this because notice that in our preregistration, it was quite open what model we would fit to exam this question. Because this is a count-based variable, we will begin with a poisson distribution. We need to fit a reduced model first because we expect the number of deltas in a thread to always be conditional on the number of comments in the thread. 

- Note this assumption was never preregistered and so a skeptical reviewer might not completely believe including the predictor was necessary/well thought through. 

---

```{r}
summary(preregistered.model.1.reduced <- glm(Deltas ~ scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```

---

```{r}
summary(preregistered.model.1 <- glm(Deltas ~ Sociomoral + scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```

---

The initial model we've run provides some evidence against the prediction. Adding the Sociomoral predictor didn't improve model fit as indicated by a similar AIC. About the same number of deltas in a thread for sociomoral vs non sociomoral posts.

---

We controlled for comments because it might be that sociomoral posts are simply discussed more. To check that, let's predict comments on sociomoral variable. What distribution we fit is quite tricky here. Let's begin with just a gaussian 


```{r}
summary(exploratory.model.comments <- lm(Comments ~ Sociomoral, data = Preregistration_Dataset_Example))
```
---

Are we violating any of the assumptions of this model? It does look like people comment a lot more about sociomoral issues, which makes sense but would be hard to predict because it wasn't exactly our main interest in this paper. Let's plot to see how a gaussian distribution would fit 

```{r, fig.height = 6, fig.width = 6}
plot(exploratory.model.comments)
```
---

Our qqplot shows some pretty big deviations and indeed when we plot the distribution, it does not look normal (though distribution of the residuals is of primary interest).

```{r, fig.height = 6, fig.width = 6}
ggplot(Preregistration_Dataset_Example)+
  geom_histogram(aes(x = Comments, colour = Sociomoral, fill = Sociomoral), position = "dodge")+
  theme_bw(12)
```

---

- We can also see here that a statistical model isn't really necessary to understand that there are vast differences in the number of comments on sociomoral vs non-sociomoral posts. 

- However, our standard modeling tools might not be up to the task with this dataset, and it isn't some a preregistration could be easy to identify in advance. However, by preregistering what we knew or predicted, it makes it very clear how much we are learning from the data as we model it. 


---

There were several other things of interest in this study we subsequently explored. For example, the extent to which controlling for comments, statistical language predicted Delta awarding 

```{r}
summary(exploratory.model.2.reduced <- glm(Deltas ~ scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```
---

```{r}
summary(exploratory.model.2 <- glm(Deltas ~ scale(Stat_Language, center=FALSE) + scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```
---

It appears to, and we had some a priori reason to think this, but we didn't write this model down in advance so we need to be on guard about reading too much into it. And model fit is only slightly improved adding this predictor. We should also plot this relationship.

```{r, fig.height = 6, fig.width = 6}
plot(exploratory.model.2)
```
---

```{r, fig.height = 6, fig.width = 6}
ggplot(Preregistration_Dataset_Example)+
  geom_smooth(aes(x = Stat_Language, y = Deltas), method = "glm", method.args = list(family = "poisson"))+
  theme_bw(12)
```

---

```{r, fig.height = 6, fig.width = 6}
ggplot(Preregistration_Dataset_Example)+
  geom_smooth(aes(x = Stat_Language, y = Deltas), method = "glm", method.args = list(family = "poisson"))+
  geom_point(aes(x = Stat_Language, y = Deltas), shape = 1)+
  theme_bw(12)
```
---

```{r}
df <- Preregistration_Dataset_Example%>%
  filter(Deltas < 30)

summary(exploratory.model.2.1 <- glm(Deltas ~ scale(Stat_Language, center=FALSE) + scale(Comments, center=FALSE), data = df), family = "poisson")
```

---

# We can ask other questions of our dataset while we're exploring.

- Maybe that will help us run a subsequent study that we preregister

---

For instance, we could ask is there converging evidence that other things that relate to evidence rather than emotional appeals predicts delta awarding. First fit the reduced model. 

---

```{r}
summary(exploratory.model.3.reduced <- glm(Deltas ~  scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```

---

```{r}
summary(exploratory.model.3 <- glm(Deltas ~ scale(Total_Links, center=FALSE)+ scale(Comments, center=FALSE), data = Preregistration_Dataset_Example), family = "poisson")
```

---

Providing evidence in the form of links to citations seems to predict delta awarding


```{r, fig.height = 5.5, fig.width = 6}
ggplot(Preregistration_Dataset_Example)+
  geom_smooth(aes(x = scale(Total_Links), y = Deltas), method = "glm", method.args = list(family = "poisson"))+
  geom_point(aes(x = scale(Total_Links), y = Deltas), shape = 1)+
  theme_bw(12)
```

---

```{r}
summary(exploratory.model.3.1.reduced <- glm(Deltas ~  scale(Comments,  center=FALSE), data = df), family = "poisson")
```

---

Some evidence even after removing this observation that included links improves model fit. 
```{r}
summary(exploratory.model.3.1 <- glm(Deltas ~ scale(Total_Links,  center=FALSE)+ scale(Comments,  center=FALSE), data = df), family = "poisson")
```
---

# Just a few of the possible analyses

- We were interested in several other things in this dataset. 

--

  - How much comments with links, sociomoral, and so forth predicted attitude change. 

--

  - We also had several dv's dacs (Delta awarded comments), comments, Total_Links

--

- This is really just the beginning of the dataset because we could go back to reddit an extract a host of additional information from these same posts. 
  
  - For instance, we could also look at upvotes; maybe good arguments are upvoted a lot but not particularly persuasive to people who disagree. 

---

# Where are we?

- We began with a very open ended preregistration to investigate this Reddit dataset. We went through the registration, but on the analytic front there were MANY things left unspecified. This can happen when you first begin exploring a new questions, as indeed we were. 

- But by preregistering something it makes it clear how open ended these analyses were. This example shows that exploration was also necessary because it would be difficult to predict the shape of the distributions we were working with and so our preregistered analyses may have needed adjustment in any case. 

- Imagine we had preregistered a model as a linear regression but, upon seeing the distribution of the residuals, it was clear this model was not the most appropriate. In this case, we needed exploration to make the right modeling choices. 


---


# Summary

- What covered what preregistration is, where to do it, and how to do it

- Limitations to preregistration and the relationship between preregistration and exploratory analyses

- Concrete example of the benefits and difficulties of preregistration


