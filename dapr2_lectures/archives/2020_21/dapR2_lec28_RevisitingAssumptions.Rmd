---
title: "<b>Revisiting Assumptions </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "Tom Booth and Alex Doumas"
institute: "Department of Psychology<br>The University of Edinburgh"
date: "AY 2020-2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(car)
library(fGarch)
```

# Weeks Learning Objectives
1. Understand the problem of multiple comparisons. 

2. Run appropriate tests and corrections for multiple comparisons. 

3. Apply assumption tests to linear models for experimental designs. 

---
# Topics for today

+ A brief word about assumptions in linear models for experimental designs.

---
# Assumptions

+ And a reminder of linear model assumptions:
  + **L**inearity: The relationship between $y$ and $x$ is linear.
  + **I**ndependence of errors: The error terms should be independent from one another.
  + **N**ormality: The errors $\epsilon$ are normally distributed
  + **E**qual variances ("Homoscedasticity"): The scale of the variability of the errors  $\epsilon$ is constant at all values of $x$.  

--

**That is, exactly the same assumptions as would be the case for any other linear model**

---
# Assumptions

- We've gone over these assumptions when discussing regression. 
- It's worth a bit of redundency, though, to make sure these assumptions are well understood.
- We'll review assumptions, and how to check for violations...  

---
# Visualizations vs tests
+ In talking about assumption checks, we use both statistical tests and visualizations. 

+ In general, graphical methods are often more useful. 
	+ Easier to see the nature and magnitude of the assumption violation. 
	+ There is also a very useful function for producing them all.

+ Statistical tests often suggest assumptions are violated when problem is small. 

---
# Visualizations made easy
+ For a many assumption and diagnostic plots, we will make use of the `plot()` function.

+ If we give `plot()` a linear model object (e.g. `m1` or `m2`), we can automatically get 6 useful plots.
  + Today we'll use a couple, but we'll get to others in the coming weeks.

---
# Example
+ The data comes from a study into patient care in a paediatric wards. 

+ A researcher was interested in whether the subjective well-being of patients differed dependent on the post-operation treatment schedule they were given, and the hospital in which they were staying. 

+ **Condition 1**: `Treatment` (Levels: TreatA, TreatB, TreatC).
  
+ **Condition 2**: `Hosp` (Levels: Hosp1, Hosp2). 
  
+ Total sample n = 180 (30 patients in each of 6 groups).
  + Between person design. 

+ **Outcome**: Subjective well-being (SWB). 
  + An average of multiple raters (the patient, a member of their family, and a friend). 
  + SWB score ranged from 0 to 20.


---
# The data
```{r}
hosp_tbl <- read_csv("hospital.csv", col_types = "dff")
hosp_tbl %>%
  slice(1:10)
```

---
# Our results
```{r}
m4 <- lm(SWB ~ Treatment + Hospital + Treatment*Hospital, data = hosp_tbl)
anova(m4)
```

---
# Our results
```{r}
m4sum <- summary(m4)
round(m4sum$coefficients,2)
```

---
#  Linearity assumption
+ For regression, **Assumption**: The relationship between $y$ and $x$ is linear.
  + Assuming a linear relation when the true relation is non-linear can result in under-estimating that relation


+ **Investigated with**:
  + Scatterplots with loess lines. 

+ For **ANOVA** there is no formal linearity assumption because of the group structure of all IVs. 

---
# Normally distributed errors 
+ **Assumption**: The errors ( $\epsilon_i$ ) are normally distributed around each predicted value.

+ **Investigated with**:
  + QQ-plots
  +	Histograms
	+ Shapiro-Wilk test

---
# Visualizations 
+ **Histograms**: Plot the frequency distribution of the residuals.

```{r, eval=FALSE}
hist(m4$residuals)
```

--

+ **Q-Q Plots**: Quantile comparison plots.
	+ Plot the standardized residuals from the model against their theoretically expected values.
	+ If the residuals are normally distributed, the points should fall neatly on the diagonal of the plot.
	+ Non-normally distributed residuals cause deviations of points from the diagonal.
		+ The specific shape of these deviations are characteristic of the distribution of the residuals.

```{r, eval=FALSE}
plot(m4, which = 2) #<<
```

---
# Visualizations

.pull-left[

```{r, echo=FALSE}
hist(m4$residuals)
```

]


.pull-right[

```{r, echo=FALSE}
plot(m4, which = 2) #<<
```

]

---
# shapiro.test() 
+ The Shapiro-Wilk test provides a significance test on the departure from normality.

+ A significant $p$-value ( $\alpha = .05$ ) suggests that the residuals deviate from normality.

```{r}
shapiro.test(m4$residuals)
```


---
#  Equal variance (Homoscedasticity) 

+ **Assumption**: The equal variances assumption is constant across values of the predictors $x_1$, ... $x_k$, and across values of the fitted values $\hat{y}$. 
	+ Heteroscedasticity refers to when this assumption is violated (non-constant variance). 

+ **Investigated with**:
  + Plot Pearson residual values against the predicted values ( $\hat{y}$ ).
	+ Breusch-Pagan test (Non-constant variance test). 

---
#  Residual-vs-predicted values plot 

.pull-left[
+ In R, we can plot the residuals vs predicted values using `residualPlot()` function in the `car` package.

  + Categorical predictors should show a similar spread of residual values across their levels. 

  + The plots for continuous predictors should look like a random array of dots. 

	  + The solid line should follow the dashed line closely. 

```{r, eval=FALSE}
residualPlot(m4)
```
]

.pull-right[
```{r, echo=FALSE}
residualPlot(m4)
```
]

---
#  Breusch-Pagan test 

.pull-left[
+ Also called the non-constant variance test. 

+ Tests whether residual variance depends on the predicted values. 

+ Implemented using the `ncvTest()` function in R. 
  + Non-significant $p$-value suggests homoscedasticity assumption holds. 
]

.pull-right[

```{r}
ncvTest(m4)
```

]

---
#  Independence of errors 
+ **Assumption**: The errors are not correlated with one another. 

+ Difficult to test unless we know the potential source of correlation between cases. 

+ We can test a limited form of the assumption by testing for autocorrelation between errors. 
	+ Achieved using the Durbin-Watson test. 

---
#  Durbin-Watson test 
+ Durbin-Watson test implemented in R using the `durbinWatsonTest()` function:

```{r}
durbinWatsonTest(m4)
```


+ The D-W statistic can take values between 0 and 4. 
	+ 2= no autocorrelation. 
+ Therefore, we ideally want D-W values close to 2 and a non-significant $p$-value. 
  + Values <1 or >3 may indicate problems. 

---
class: center, middle
# Violated Assumptions
What do we do about non-normality of residuals, heteroscedasticity and non-linearity? 


---
# Non-linear transformations 
+ Often non-normal residuals, heteroscedasticity and non-linearity can be ameliorated by a non-linear transformation of the outcome and/or predictors.

+ This transformation involves applying a function (see first week) to the values of a variable. 
  + The transformation changes the values and overall shape of the distribution. 

+ For non-normal residuals and heteroscedasticity, skewed outcomes can be transformed to normality. 

+ Non-linearity may be helped by a transformation of both predictors and outcomes. 

---
#  Transforming variables to normality 
+ Positively skewed data can be made more normally distributed using a log-transformation.

+ Negatively skewed data can be made more normally distributed using same procedure but first reflecting the variable (make biggest values the smallest and smallest the biggest) and then applying the log-transform. 

---
# Visualizing Skew

.pull-left[


```{r, echo=FALSE}
df_skew <- tibble(
  pos = rsnorm(100000, 50, 10, 2.5),
  neg = rsnorm(100000, 50, 10, -2.5)
)
df_skew %>%
  ggplot(., aes(x=pos)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n x", y = "Frequency \n", title = "Positive Skew")
```

]

.pull-right[
```{r, echo=FALSE}
df_skew %>%
  ggplot(., aes(x=neg)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n x", y = "Frequency \n", title = "Negative Skew")
```

]

---
#  Log-transformations 
+ Log-transformations can be implemented in R using the `log()` function.

+ If your variable contains zero or negative values, you need to first add a constant to make all your values positive. 
	+ A good strategy is to add a constant so that your minimum value is one. 
	+ E.g., if your minimum value is -1.5, add 2.5 to all your values. 


---
# Log-transformation in action

```{r}
df_skew <- df_skew %>%
  mutate(
    log_pos = log(pos), #<<
    neg_ref = ((-1)*neg) + (max(neg)+1), #<<
    log_neg = log(neg_ref) #<<
  )
```


---
# Log-transformation in action

.pull-left[
```{r, echo=FALSE}
df_skew %>%
  ggplot(., aes(x=pos)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n x", y = "Frequency \n", title = "Positive Skew")
```

]

.pull-right[
```{r, echo=FALSE}
df_skew %>%
  ggplot(., aes(x=log_pos)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n Log(x)", y = "Frequency \n", title = "Transformed Positive Skew")
```

]


---
# Log-transformation in action

.pull-left[
```{r, echo=FALSE}
df_skew %>%
  ggplot(., aes(x=neg)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n x", y = "Frequency \n", title = "Negative Skew")
```

]

.pull-right[
```{r, echo=FALSE}
df_skew %>%
  ggplot(., aes(x=log_neg)) +
  geom_histogram(bins = 50, fill = "lightblue", colour = "blue") +
  labs(x = "\n Log(transformed x)", y = "Frequency \n", title = " Transformed Negative Skew")
```

]

---
# Summary of today
- Reviewed LINE assumptions for linear models. 
- Discussed these assumptions related to experimental (ANOVA analysed) data and how to check for assumption violations. 


