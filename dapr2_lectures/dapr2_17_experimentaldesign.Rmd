---
title: "<b> Case Diagnostics 2 </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
```

# Week's Learning Objectives
1. Understand the relation between the ANOVA family of tests and experimental design.
2. Recognise ANOVA models as special cases of linear models with categorical predictors.
3. Understand the distinction between simple, main and interaction effects. 

---
# Topics for today
+ Experimental design
+ Analysing an experiment with a linear model and an ANOVA
+ F tests. 

---
# Experimental Design: manipulation
+ A key feature of experimental designs is that we actively manipulate our predictor (IV).

+ The intention is that changing the predictor will result in changes in the outcome (DV).

+ That is our manipulation will lead to variation in the outcome.

+ Our experiments can fail because we design these manipulations poorly.

+ The predictors in an experiment are (primarily) experimental conditions.

---
# Conditions/Factors & levels
+ **Conditions**: 
  + Are part of our experimental designs.
  + They are what is manipulated.
  
+ **Factors**
  + The resultant variables in our data set that code the experimental conditions are typically called factors.
  + Generally the terms conditions and factors are used interchangeably.
  + But it is useful to differentiate the design (conditions) and the data that represents aspects of the design (factors)

+ Factors can have **levels**
  + These are the number of ways we vary or manipulate the condition

---
# Between vs Within Person
+ Two broad choices of study structure:
  + **Between person**: Participants only appear on one level/condition
  + **Within person**: Participants appear in multiple level/conditions

--

+ The labels we use to refer to kinds of studies reflect the number of conditions and whethert the conditions are between vs within.
  + One-way between person
  + Two-way within person
  + etc.

---
# A new study
+ Suppose we wanted to look at the number of reading errors caused by noise distraction. 

+ We might devise a task where participants had to read a passage of text and put a cross through all verbs. 

+ Our outcome, or dependent variable, is the number of verbs correctly crossed out.
+ Our predictor, or independent variable, is the noise level. 

---
# One-way Between Person

```{css, echo=F}
    .remark-slide thead, .remark-slide tr:nth-child(2n) {
        background-color: white;
    }
```

```{r, echo=FALSE}
tibble(
  None = c("Tom", "Aja", "Alex", "Brandy", "Darren", "Lucy"),
  Noise = c("Adam", "Fiona", "Simon", "Tasha", "Josh", "Charlotte")
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c("Noise Level" = 2))
```


???
0db
85db

---
# One-way Between Person (more levels)


```{r, echo=FALSE}
tibble(
  None = c("Tom", "Aja", "Alex", "Brandy"),
  Moderate = c("Darren", "Lucy", "Josh", "Charlotte"),
  Loud = c("Adam", "Fiona", "Simon", "Tasha")
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c("Noise Level" = 3))
```

???
0db
85db
100db

---
# Two-way Between Person

```{r, echo=FALSE}
tibble(
  Distraction = c("Words", "Words", "No Words", "No Words"),
  None = c("Tom", "Aja", "Alex", "Brandy"),
  Moderate = c("Darren", "Lucy", "Josh", "Charlotte"),
  Loud = c("Adam", "Fiona", "Simon", "Tasha")
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  row_spec(1:2, background = "lightblue") %>%
  add_header_above(c("", "Noise" = 3)) %>%
  collapse_rows(1, valign = "top")
```

???
0db
85db
100db

As we have a verbal task, possible noise with words would be a greater distraction


---
# One-way Within Person

```{r, echo=FALSE}
tibble(
  None = c("Tom", "Aja", "Alex", "Brandy", "Darren", "Lucy",  "Josh", "Charlotte"),
  Noise = c("Tom", "Aja", "Alex", "Brandy", "Darren", "Lucy",  "Josh", "Charlotte")
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c("Noise Level" = 2))
```


---
# Two-way Within Person

```{r, echo=FALSE}
tibble(
  Distraction = c(rep("Word", 4), rep("No Word", 4)),
  None = c(rep(c("Tom", "Aja", "Alex", "Brandy"),2)),
  Noise = c(rep(c("Tom", "Aja", "Alex", "Brandy"),2))
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  row_spec(1:4, background = "lightblue") %>%
  add_header_above(c("", "Noise Level" = 2)) %>%
  collapse_rows(1, valign = "top")
```


---
# Mixed Designs
```{r, echo=FALSE}
tibble(
  Distraction = c(rep("Word", 4), rep("No Word", 4)),
  None = c("Tom", "Aja", "Alex", "Brandy", "Darren", "Lucy",  "Josh", "Charlotte"),
  Noise = c("Tom", "Aja", "Alex", "Brandy", "Darren", "Lucy",  "Josh", "Charlotte")
) %>%
  kable(.) %>%
  kable_styling(full_width = F) %>%
  row_spec(1:4, background = "lightblue") %>%
  add_header_above(c("", "Noise Level" = 2)) %>%
  collapse_rows(1, valign = "top")
```


---
# Including Covariates
- What other qualities of the participants might have an effect on the outcome of the experiment? 
  - What about something like age? 
  - What if Alex is an old fogey that gets bothered by even the slightest sound, but Tom is a hip young fellow who is used to ignoring noise from all his time at the clubs? 

---

# Including Covariates
+ When we conduct an experiment we hope that, via the processes of randomization of participants into conditions, we control for covariates or nuisance factors.

+ There are various types of design (e.g. randomized block designs) where explicit consideration of covariates is made during group assignment.

+ If we do not have key information before hand, we can measure covariates and include them.
  + Consider age in our noise example.
  
---

# Experiments and ANOVA
- Within behavioural science there is a tradition of analysing experimental designs with a statistical model called the ANOVA (or analysis of variance). 
- The basic logic of the ANOVA is as follows:
  - Look at how much variation there is within groups
$$
within \: group \: variation = random \: variation
$$
  - Look at how much variation there is between groups.
$$
between \: group \:variation = random \: variation + variation \: due \: to \: groups
$$

---

# Experiments and ANOVA
- The basic logic of the ANOVA:
  - Take the ratio of between to within group variation, or
  
$$
\frac{between  \: group \: variation}{within \: group \: variation} = \frac{random \: variation + variation \: due \: to \: groups}{random \: variation}
$$
  - But if there is variation caused by being in different groups (i.e. the experimental manipulation actually has an effect), then:

$$
\frac{between  \: group \: variation}{within \: group \: variation} \neq 1
$$

- We'll go over this logic in more detail in the next lecture. 
- For now, it's enough to know that the ANOVA is a common tool for analysing experiments in psychology and much of the terminology used in experimental design in psychology follows from the logic of the ANOVA. 

---

# Interim Summary 

+ Key points:
  + Experimental designs yield nominal categorical variables
  + These variables code groups.
  + There is a language of ANOVA models for analysing this data
  + But, as we know, we can include categorical variables in linear models.

+ We will now look at...
  + the equivalence of ANOVA and linear models, and 
  + how we can analyse experimental designs in linear models.

---
# Recall linear model

$$
y_i = b_0 + b_1x_1 + b_2x_2 ...b_kx_k + \epsilon_i
$$

+ Where the $b$ represent the effects of the predictors on the outcome

+ As a set of $b$ represent our model/design.

+ So we could re-write informally as:

$$
y_i = \text{stuff we think predicts the outcome} + \epsilon_i
$$


---
# Models and Experiments
+ In the context of an experimental designs...

**Things that predict outcome = experimental conditions**

+ So the task in our analyses to is try and understand if different conditions actually do account for variance in the outcome.

---
# A brief example

+ Suppose that we care about the effects of 2 drug treatments on the reading ability of hyperactive children. 

+ In our design, we randomly select 3 groups of children (n=5 per group, N = 15)
  + we give group 1 a placebo (a1), 
  + group 2 one drug (a2), and 
  + group 3 a different drug (a3). 

+ After 1 hour we let the children study a passage of text for 10 minutes, then administer a standard test of comprehension

---
# The data
```{r, echo=FALSE}
data <- tibble(
  ID = paste("ID", 1:15, sep=""),
  A_condition = c(rep("a1_control", 5), rep("a2_drug1", 5), rep("a3_drug2", 5)),
  score = c(16,18,10,12,19,4,7,8,10,1,2,10,9,13,11)
)
data %>%
  arrange(A_condition)
```

---
# Describe the data

.pull-left[
```{r, eval=FALSE}
data %>%
  group_by(A_condition) %>%
  summarise(
    mean = round(mean(score)),
    sd = round(sd(score),1),
    N = n()
  )
```
]

.pull-right[
```{r, echo=FALSE}
data %>%
  group_by(A_condition) %>%
  summarise(
    mean = round(mean(score)),
    sd = round(sd(score),1),
    N = n()
  )
```
]


---
# Some observations on our data

1. The group means are not the same. That is, there is some between group variation in average scores.

2. Not all individuals within the group scored the exact same value as thev mean (note we have a non-zero standard deviation - plus we can just look at the numbers!!!). So there is some within group variation.


---
# What do we want to know?
+ Is more variation between groups than there is within groups. Why? 

+ Consider what the groups represent. 
  + Each group is one of our experimental conditions. 
  + If we have designed a good experiment, then we would hope that our design would create some differences across groups. (are the drugs effective) 
  + So, in this design **we want to see lots of between group variance**. 
  + That is what tells us the study "worked".

---
# What do we want to know?
+ Moreover, we want the magnitude of this to be bigger than the fluctuations in scores we see within a group. Why? 

+ Well if we have appropriately randomly assigned people to groups, then any within group variation should be due to random error. 

+ This leads us to the key tests of experimental designs:

$$
\frac{\text{between group variation}}{\text{within group variation}}
$$

+ Or 

$$
\frac{\text{thing we think explains variation (groups or design) + error}}{\text{error}}
$$


---
# What data arises from experiments?
+ But pause for a moment.

+ Experiments produce nominal category variables.

+ We want to know if they can be used to explain variation in the outcome.

+ We know we can look at such relationships using `lm`

+ We also know we can have nominal category predictors in `lm`

+ And we know the $F$-tests in `lm` are testing the ratio of explained versus error variance.

+ So, our $F$-tests in `lm` are giving us the key tests of experimental effects.


---
class: center, middle
# Time for a break

---
class: center, middle
# Welcome Back!


---
# Hypotheses we test in experimental studies

+ One-way designs:
  + **Main effect (tests overall effect of a condition; $F$-tests )**
  + Contrasts (tests differences between specific group means; $\beta$ )

--

+ Factorial designs:
  + Simple contrasts/effects
  + Interactions (categorical*categorical)
  + Main effects


---
# Example
+ The data comes from a study into patient care in a paediatric wards. 

+ A researcher was interested in whether the subjective well-being of patients differed dependent on the post-operation treatment schedule they were given, and the hospital in which they were staying. 

+ **Condition 1**: `Treatment` (Levels: TreatA, TreatB, TreatC).
  
+ **Condition 2**: `Hosp` (Levels: Hosp1, Hosp2). 
  
+ Total sample n = 180 (30 patients in each of 6 groups).
  + Between person design. 

+ **Outcome**: Subjective well-being (SWB)
  + An average of multiple raters (the patient, a member of their family, and a friend). 
  + SWB score ranged from 0 to 20.


---
# The data
```{r}
hosp_tbl <- read_csv("hospital.csv", col_types = "dff")
hosp_tbl %>%
  slice(1:10)
```

---
# For today

+ As we are discussing one-way designs, today we will just look at the effect of treatment on SWB.

+ What we will show is that:

```{r, eval=FALSE}
lm(SWB ~ Treatment, data = hosp_tbl)
```

+ and

```{r, eval=FALSE}
aov(SWB ~ Treatment, data = hosp_tbl)
```

+ are the same.

---
# Descriptive statistics by group

```{r}
hosp_tbl %>%
  select(1:2) %>%
  group_by(Treatment) %>%
  summarise(
    mean = round(mean(SWB)),
    sd = round(sd(SWB),1),
    N = n()
  )
```

---
# In R

```{r}

```



---
# Summary of today

+ Conceptually reviewed the relation between experiments, ANOVA and the structure of linear models.

+ Key take home:
  + A linear model with nominal category predictors can do the same analytic job as ANOVA

+ In the next videos, and in your lab, we will show with examples, and more formally, this equivalence.


---
class: center, middle
# Thanks for listening!
