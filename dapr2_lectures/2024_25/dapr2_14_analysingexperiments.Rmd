---
title: "<b> Analysing Experiments </b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(emmeans)

hosp_tbl <- read_csv("./data/hospital.csv", col_types = "dff")

```

# Course Overview

.pull-left[

<!---
I've just copied the output of the Sem 1 table here and removed the bolding on the last week, so things look consistent with the trailing opacity produced by the course_table.R script otherwise.
-->


<table style="border: 1px solid black;>
  <tr style="padding: 0 1em 0 1em;">
    <td rowspan="5" style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1;text-align:center;vertical-align: middle">
        <b>Introduction to Linear Models</b></td>
    <td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Intro to Linear Regression</td>
  </tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Interpreting Linear Models</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Testing Individual Predictors</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Model Testing & Comparison</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Linear Model Analysis</td></tr>

  <tr style="padding: 0 1em 0 1em;">
    <td rowspan="5" style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1;text-align:center;vertical-align: middle">
        <b>Analysing Experimental Studies</b></td>
    <td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Categorical Predictors & Dummy Coding</td>
  </tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        	Effects Coding & Coding Specific Contrasts</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Assumptions & Diagnostics</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Bootstrapping</td></tr>
  <tr><td style="border: 1px solid black;padding: 0 1em 0 1em;opacity:1">
        Categorical Predictor Analysis</td></tr>
</table>



]

.pull-right[


```{r echo = FALSE, results='asis'}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block3_name,block4_name,block3_lecs,block4_lecs,week=4)
```


]


---


# Week's Learning Objectives

1. Interpretation of interactions with effects-coded variables
2. Distinguish between interactions, main effect contrasts, and simple effect contrasts
3. Use contrasts to code specific interaction hypotheses
4. Apply pairwise tests and corrections


---


# Hypotheses Testing in Factorial Designs

+ **Interactions (categorical-categorical)**
  + A change in the effect of one condition as a function of another
  + Does the effect of `Treatment` differ by `Hospital`? 

  <!-- + With effects coding, we can also think of this as a difference in simple effects. -->
  

+ **Main effects / main effect contrasts** 
  + An overall, or average, effect of a condition over combined levels of another
  + Is there an effect of `Treatment` averaged over `Hospital`? 

+ **Simple Effects**
  + An effect of one condition at a specific level of another
  + Is there an effect of `Hospital` for those receiving `Treatment A`? (...and so on for all combinations)
  + In factorial designs with more than two levels of one or more of the conditions, one can also distinguish between *simple effects* and *simple contrasts*
  + A simple contrast is a more focused test that directly compares only two cells (or a particular combination of cells)


---
# Our model and coefficients

+ The effects-coded model:

$$y_{i} = b_0 + \underbrace{(b_1E_1 + b_2E_2)}_{\text{Treatment}} + \underbrace{b_3E_3}_{\text{Hospital}} + \underbrace{b_4E_{13} + b_5E_{23}}_{\text{Interactions}} + \epsilon_{i}$$
+ Where the variables represent the following comparisons: 

```{r, echo=FALSE}
tibble(
  Treatment = c("A", "A", "B", "B", "C", "C"),
  Hospital = rep(c("Hosp1", "Hosp2"),3),
  E1 = c(1,1,0,0,-1,-1),
  E2 = c(0,0,1,1,-1,-1),
  E3 = c(1,-1,1,-1,1,-1),
  E13 = c(1,-1,0,0,-1,1),
  E23 = c(0,0,1,-1,-1,1)
)
```



---
# Specifying our model in R

+ And our model in R:

```{r}
m1 <- lm(SWB ~ Treatment + Hospital + Treatment*Hospital, data = hosp_tbl,
         contrasts = list(Treatment = contr.sum, 
                          Hospital = contr.sum)) # you can code contrasts within lm
```

+ Remember that the `contrasts` argument of the `lm` function will be set to dummy coding by default, unless otherwise specified
+ Here we have specified the value of the `contrasts` argument within the `lm` function

---
# Table of means

+ Useful to keep in mind the group means:

```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

+ Marginal means indicate the means of group means by rows and columns (for the two variables)
+ 9.88 is the grand mean (mean of all group means)

---
# What will the model tell us?

+ **Marginal effects:** Does a marginal mean differ from the grand mean?

+ **Interactions:** Does the distance of the row or column marginals from the grand mean differ depending on the level of the other variable?

```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```


---
# Our results
```{r, echo=FALSE}
m1sum <- summary(m1)
summary(m1)
```

---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>

+ $b_0$ (`Intercept`) = Grand mean
--

<br>

+ 9.88


---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>

+ $b_1$ (`Treatment1`) = Difference between row marginal for treatment A and the grand mean
--

<br>

+ 9.33 - 9.88


---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>


+ $b_2$ (`Treatment2`) = Difference between row marginal for treatment B and the grand mean
--

<br>

+ 11.27 - 9.88


---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>

+ $b_3$ (`Hospital1`) = Difference between column marginal for Hospital 1 and the grand mean
--

<br>

+ 10.11 - 9.88


---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>

+ $b_4$ (`Treatment1:Hospital1`) = Difference between Treatment A in Hospital 1 and the grand mean, compared to the overall effect of Treatment A (vs grand mean) and the overall effect of Hospital 1 (vs grand mean)
--

<br>

+ $b_4 = (\mu_{TreatmentAHospital1} - b_0) - b_1 - b_3$
--

<br>

+ (10.80 - 9.88) - (9.33 - 9.88) - (10.11 - 9.88)


---
# Interpretation with effects coding

.pull-left[
```{r, echo=FALSE}
round(m1sum$coefficients,2)[,1:2]
```
]

.pull-right[
```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

<br>

+ $b_5$ (`Treatment2:Hospital1`) = Difference between Treatment B in Hospital 1 and the grand mean, compared to the overall effect of Treatment B (vs grand mean) and the overall effect of Hospital 1 (vs grand mean)
--
<br>

+ $b_5 = (\mu_{TreatmentBHospital1} - b_0) - b_2 - b_3$
--
<br>

+ (9.43 - 9.88) - (11.27 - 9.88) - (10.11 - 9.88)


---
# Predictions from the regression formula

$$\hat{y} = b_0 + b_1E_1 + b2E_2 + b_3E_3 + b_4E_{13} + b_5E_{23}$$
+ Plugging in our $\beta$ estimates:

$$\hat{y} = 9.88 - 0.55 \cdot E_1 + 1.39 \cdot E_2 + 0.23 \cdot E_3 + 1.24 \cdot E_{13} - 2.07 \cdot E_{23}$$
<br>


.pull-left[
```{r, echo=FALSE}
tibble(
  Treatment = c("A", "A", "B", "B", "C", "C"),
  Hospital = rep(c("Hosp1", "Hosp2"),3),
  E1 = c(1,1,0,0,-1,-1),
  E2 = c(0,0,1,1,-1,-1),
  E3 = c(1,-1,1,-1,1,-1),
  E13 = c(1,-1,0,0,-1,1),
  E23 = c(0,0,1,-1,-1,1)
)
```
]

.pull-right[

```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```
]

---
class: center, middle
# Questions?

---
# Interacting manual contrasts

+ We can test an interaction based on manual contrasts

+ Suppose we wanted to test whether there is a difference between Treatment A vs Treatment B and Treatment C across hospitals

+ Steps:
  + Define the contrasts for each factor
  + Calculate the product (remember all interactions are products)
  + Create the contrast code
  + Run the contrast
  
---
# Interacting manual contrasts in R

+ Step 1: Individual contrasts (Comparing Treatment A to the rest)

```{r}
treat_con <- c("TreatA" = 1, "TreatB" = -1/2, "TreatC" = -1/2)
hosp_con <- c("Hosp1" = 1, "Hosp2" = -1)
```

+ Step 2: Calculating the product (with the `outer` function)

```{r}
contr_prod <- outer(treat_con, hosp_con)
contr_prod
```


---
# Interacting manual contrasts in R

+ Step 3: Create the contrast

```{r}
m1_emm <- emmeans(m1, ~Treatment*Hospital)
m1_emm
```


Input the weights we calculates into an object:

```{r}
int_comp <- list("TreatA vs B and C across hospital" = 
                   c(1, -1/2, -1/2, -1, 1/2, 1/2))
```


---
# Interacting manual contrasts in R

+ Step 4: Run the contrast



```{r}
int_comp_test <- contrast(m1_emm, int_comp)
int_comp_test
```

<br>

+ For interpretation, remember the product of contrast weights:

```{r}
contr_prod
```


---
class: center, middle
# Questions?


---
# Simple Effects
+ We noted previously that simple effects consider the effect of one condition at a specific level of the other

  + Is there an effect of `Hospital` for those receiving `Treatment A`? (and so on for all combinations)
  
  + Or, put another way, is there a difference in `SWB` between Hospitals 1 and 2 for people receiving Treatment A?

---
# Simple Effects with `emmeans`

.pull-left[
```{r}
m1_emm <- emmeans(m1, ~Treatment*Hospital)
m1_simple1 <- pairs(m1_emm, 
                    simple = "Hospital")
m1_simple1
```
]

.pull-right[

```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]
---
# Simple Effects with `emmeans`

.pull-left[
```{r}
m1_simple2 <- pairs(m1_emm, 
                    simple = "Treatment")
m1_simple2
```
]

.pull-right[

```{r, echo=FALSE}
knitr::kable(tibble::tribble(
   ~V1, ~V2, ~V3, ~V4,
  "TreatA", "10.80",  "7.85", "9.33",
  "TreatB", "9.43",  "13.11", "11.27",
  "TreatC", "10.10",  "7.98", "9.04", 
  "Marginal","10.11", "9.65", "9.88"
  ), col.names = c("", "Hosp1",  "Hosp2", "Marginal"))
```

]

---
# Simple Effects with plots

.pull-left[
```{r, out.width="90%"}
emmip(m1, Treatment ~ Hospital)
```

]

.pull-right[
```{r}
m1_simple1
```

]

---
# Simple Effects with plots

.pull-left[
```{r, out.width="90%"}
emmip(m1, Hospital ~ Treatment)
```

]

.pull-right[


```{r}
m1_simple2
```

]

---
class: center, middle
# Questions?



---
# Pairwise comparisons
+ As the name suggests, pairwise comparisons compare all levels of a given predictor variable with all levels of the other

+ A fully exploratory pairwise analysis

```{r}
pairs_res <- pairs(m1_emm)
```

---
# Pairwise comparisons

```{r, echo=FALSE}
pairs_res
```



---
# Why do pairwise comparisons?

+ Sometimes we do not have a concrete hypothesis to test

+ Sometimes we do, but the exploratory analysis still gives useful information

+ Pairwise comparisons throws up a statistical issue, namely the problem of multiple comparisons
  + When we do lots and lots of tests, the chances of Type I error (false-positives) increase

+ How we can adjust our inferences to deal with this?

---
# Types of errors

+ Type I error = False positive
  + Reject the null when the null is true
  + $\alpha = P(\text{Type I Error})$

+ Type II error = False negative
  + Fail to reject the null when the null is false
  + $\beta = P(\text{Type II Error})$
  
---
# A single test
+ If we perform a single test, our Type I error rate is $\alpha$
  + So if we set $\alpha = 0.05$, the probability of a false positive is 5%

+ However, what if we do multiple tests ( $m$ ), each with the same $\alpha$ level?

+ What is the probability of a false positive among $m$ tests?

---
# Multiple tests

$$P(\text{Type I error}) = \alpha$$
$$P(\text{not making a Type I error}) = 1 - \alpha$$

$$P(\text{Not making a Type I error in m tests}) = (1 - \alpha)^m$$

$$P(\text{Making a Type I error in m tests}) = 1 - (1-\alpha)^m$$

---
# P(Making a Type I error in m tests)

+ Suppose $m=2$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^2)
```

+ Suppose $m=5$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^5)
```

+ Suppose $m=10$ and $\alpha = 0.05$

```{r}
1 - ((1-0.05)^10)
```

---
# Why does this matter?

+ The $P(\text{Making a Type I error in m tests}) = 1 - (1-\alpha)^m$ is referred to as the family-wise error rate

+ A "family" is a set of related tests

+ When we analyse an experimental design, and we look at lots of specific comparisons, we can think of all these tests as a "family"

+ The larger the family, the more likely we are to find a false positive (see previous slide)

---
# Corrections
+ There are various methods designed to control for the number of tests
  + Here control means to keep the Type I Error rate at an intended $\alpha$

+ Many options. Some of most common:
  + Bonferroni
  + Sidak
  + Tukey
  + Scheffe

+ Others you may see:
  + Holm's step-down
  + Hochberg's step-up


---
# Bonferroni & Sidak
+ Both are considered "conservative" adjustments

+ Each treats individual tests within the family as if they are independent
  + Consider an $\alpha = 0.05$ and $m=\text{number of tests}=15$

+ **Bonferroni**: $\alpha_{Bonferroni} = \frac{\alpha}{m}$

```{r}
0.05/15
```


+ **Sidak**: $\alpha_{Sidak} = 1 - (1- \alpha)^{\frac{1}{m}}$

```{r}
1-((1-0.05)^(1/15))
```

---
# No adjustments

```{r, warning = FALSE, eval=TRUE}
pairs(m1_emm, adjust="none")
```

---
# Bonferroni in action: `emmeans`

```{r, echo=TRUE}
pairs(m1_emm, adjust="bonferroni")
```

---
# Sidak with `emmeans`

```{r}
pairs(m1_emm, adjust = "sidak")
```

---
# What if there were fewer tests?

```{r}
pairs(m1_emm, simple="Treatment", adjust="bonferroni")
```


---
# Scheffe & Tukey
+ **Scheffe procedure** involves an adjustment to the critical value of $F$
  + The adjustment relates to the number of comparisons being made
  + And makes the critical value of $F$ larger for a fixed $\alpha$
  + The more tests, the larger $F$

+ The square-root of the adjusted F provides and adjusted $t$

--

+ **Tukey's HSD** (Honest significant differences)
  + Compares all pairwise group means
  + Each difference is divided by the $SE$ of the sum of means
  + This produces a $q$ statistic for each comparison.
  + And is compared against a studentised range distribution


---
# With `emmeans`

```{r}
pairs(m1_emm, adjust = "tukey")
```


---
# With `emmeans`

```{r}
pairs(m1_emm, adjust = "scheffe")
```


---
# Summary

+ This week we have looked at ways we can code categorical data to test experimental effects

+ Interpreting model output for effects-coded categorical $\times$ categorial interactions

+ Setting up specific contrasts and comparisons

+ Correcting for increases in the probability of Type I error


---

## This week 

.pull-left[

### Tasks

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

]

.pull-right[

### Support

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

]


