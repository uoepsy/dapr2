---
title: "dapR2 Lecture Exercise"
author: "dapR2 Team"
date: ""
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)

set.seed(1512)
df <- tibble(
  Y = c(round(rnorm(75, 115, 10),0), round(rnorm(75, 90, 50),0)),
  X = c(rep(0, 75), rep(1, 75))
)

write_csv(df, "./week3.csv")
```

## Read in the data
This is going to be specific to your machine and the location of the file, so we will jump over this here. On our case, the data set is called `df`.

## Take a look at the data
Although this wasn't on the to-do list, it is always sensible to view your data. We will use `slice` to look a the first 10 rows:

```{r}
slice(df, 1:10)
```

We can also get the group descriptive statistics:

```{r}
df %>%
  group_by(X) %>%
  summarise(
    Mean = mean(Y),
    SD = sd(Y)
  )
```


## Run linear model

```{r}
m1 <- lm(Y ~ X, data = df)
summary(m1)
```

So from this output, we can see the mean of the group coded 0 of `X` is `r round(m1$coefficients[[1]],3)` and the mean of the second group (coded 1) is `r round(m1$coefficients[[2]],3)` , indicating the mean of this group is smaller. To be exact, it is `r round(m1$coefficients[[1]] + m1$coefficients[[2]],3)`

## Run t-test

```{r}
m2 <- t.test(Y~X, data = df)
m2
```

So, lets start by confirming the group means. These appear at the bottom of the `t.test` output, and line up with our values reported above. So this is good. 

Now let's look at the t-values. From the `lm` the t-value for the effect of `X` is `r round(summary(m1)$coefficients[2,3],3)` and from the `t.test` the associated value is `r round(m2$statistic,3)`. We do not need to worry about the minus sign. In this instance, this is simply the way the two tests are substracting the group means. Remember the t-distribution is symmetric, so this is showing the same magnitude of difference, but in the case of the linear model it is X=1 - X=0, and for the t-test X=0 - X=1.

Where we do see a small difference is in the p-values. From the linear model, the p-value is `r round(summary(m1)$coefficients[2,4],6)` and from the t.test the p-value is `r round(m2$p.value, 6)`. The difference here is because of the degrees of freedom used in the t.test function, and the specific assumption being made about the variances.

You may recall that an independent sample t-test assumes that the variance of the outcome in both groups is equal. R by default **does not** assume this, and runs what is called a Welch Test, which adjusts the degrees of freedom to take account of any differences in the variance of the outcome across groups. We can see that the impact here is very small. If we want to make the results line up perfectly, we need to tell R to assume variances are equal. We have done this below using the `var.equal = T` command.

```{r}
m3 <- t.test(Y ~ X, data = df, var.equal = T)
m3
```

In this output, you can see the degrees of freedom are identical to the linear model (N-k-1, or 150-1-1 = 148), and as a result, the p-value is identical to our linear model.
