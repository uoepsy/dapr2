---
title: "<b> Testing and Evaluating LM</b>"
subtitle: "Data Analysis for Psychology in R 2<br><br> "
author: "dapR2 Team"
institute: "Department of Psychology<br>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(dev = 'svg')
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  #base_color = "#0F4C81", # DAPR1
   base_color = "#BF1932", # DAPR2
  # base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro")
)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
library(simglm)

baseColour <- '#BF1932'

test <- tibble(
  student = paste(rep("ID",10),1:10, sep=""),
  hours = seq(0.5,5,.5),
  score = c(1,3,1,2,2,6,3,3,4,8)
)

res <- lm(score ~ hours, data = test)
```


# Course Overview

.pull-left[

```{r echo = FALSE, results='asis'}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block1_name,block2_name,block1_lecs,block2_lecs,week=3)
```


]

.pull-right[


```{r echo = FALSE, results='asis'}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block3_name,block4_name,block3_lecs,block4_lecs,week=0)
```

]

---


# This Week's Learning Objectives


1. Understand how to interpret significance tests for $\beta$ coefficients

2. Understand how to calculate and interpret $R^2$ and adjusted- $R^2$ as a measure of model quality

3. Be able to locate information on the significance of individual predictors and overall model fit in R `lm` model output


---
class: inverse, center, middle

# Part 1: Overview

---
# Recap
+ Last week we expanded the general linear model equation to include multiple predictors:

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_j x_{ji} + \epsilon_i$$

+ And we ran an example concerning test scores:

$$score_i = \beta_0 + \beta_1 hours_{i} + \beta_2 motivation_{i} + \epsilon_i$$

+ And we looked at how to run this model in R:

```{r, eval=FALSE}
lm(score ~ hours + motivation, data = test_study2)
```

---
# Evaluating our model

So far we have estimated values for the key parameters of our model ( $\beta$s )

+ Now we have to think about how we evaluate the model

--

+ Evaluating a model will consist of:

  1. Evaluating the individual coefficients
  
  2. Evaluating the overall model quality
  
  3. Evaluating the model assumptions


--

**Important:** Before accepting a set of results, all three of these aspects of evaluation must be considered

+ We will talk about evaluating individual coefficients and model quality today
+ Model assumptions covered later in the course (Semester 1, Week 8)

---
#  Significance of individual effects 
+ A general way to ask this question would be to state: 

> **Is our model informative about the relationship between X and Y?**

--

+ In the context of our example from last lecture, we could ask, 

> **Is study time a useful predictor of test score?**

--

+ The above is a research question

+ We need to turn this into a testable statistical hypothesis

---
#  Evaluating individual predictors 
+ Steps in hypothesis testing:

--
 
  + Research question
    
--
  
  + Statistical hypothesis
    
--
  
  + Define the null hypothesis
    
--
  
  + Calculate an estimate of effect of interest
  
--
  
  + Calculate an appropriate test statistic
    
--
  
  + Evaluate the test statistic against the null
    

---
# Research question and hypotheses

+ **Research questions** are statements of what we intend to study. 

+ A good question defines:

--

  + constructs under study
  + the relationship being tested
  + a direction of relationship
  + target populations etc.

--

> **Does increased study time improve test scores in school-age children?**

--

+ **Statistical hypotheses** are testable mathematical statements.

--

+ In typical testing in Psychology, we define a **null ( $H_0$ )** and an **alternative ( $H_1$ )** hypothesis.
  + $H_0$ is precise, and states a specific value for the effect of interest
  + $H_1$ is not specific, and simply says "something else other than the null is more likely"


---
# Statistical significance: Overview

+ Remember, we can only ever test the null hypothesis

+ We select a significance level, $\alpha$ (typically .05)

+ Then we calculate the $p$-value associated with our test statistic

+ If the associated $p$ is smaller than $\alpha$, then we **reject** the null

+ If it is larger, then we **fail to reject** the null


---

class: center, middle

# Questions?

---
class: inverse, center, middle

# Part 2: Steps in significance testing

---
# Defining null

.pull-left[
+ Conceptually:
	+ If $x$ yields no information on $y$, then $\beta_1 = 0$
	
+ **Why would this be the case?**
]

---
count: false 

# Defining null

.pull-left[
+ Conceptually:
	+ If $x$ yields no information on $y$, then $\beta_1 = 0$
	
+ **Why would this be the case?**

+ $\beta$ gives the predicted change in $y$ for a unit change in $x$.
	+ If $x$ and $y$ are unrelated, then a change in $x$ will not result in any change to the predicted value of $y$
	+ So for a unit change in $x$, there is no (=0) change in $y$
	
+ We can state this formally as a null and alternative:

$$H_0: \beta_1 = 0$$
$$H_1: \beta_1 \neq 0$$
]

.pull-right[

```{r, echo = F, message = F, fig.height=5}
set.seed(2010)
dat <- data.frame(x=1:25, y=rnorm(25))
ggplot(dat, aes(x, y)) + geom_point() +
  geom_smooth(method = 'lm', se=F, colour = baseColour) + 
  theme(axis.title=element_text(size = 14, face = 'bold'))
```

]


???
+ For the null to be testable, we need to formally define it. 
+ Point out here the difference in the specificity of the hypotheses. $H_0$ is that the $b_1$ takes a specific value. $H_1$ is that $b_1$ has some value that is not this specific value. i.e. one is directly testable, the other is not.


---
# Point estimate and test statistic

+ We have already seen how we calculate $\hat \beta_1$.

+ The associated test statistic for $\beta$ coefficients is a $t$-statistic

$$t = \frac{\hat \beta}{SE(\hat \beta)}$$

+ where

  + $\hat \beta$ = any $\beta$ coefficient we have calculated
  + $SE(\hat \beta)$ = standard error of $\beta$ 

--

+ **Recall** that the standard error describes the spread of the sampling distribution
  + The standard error (SE) provides a measure of sampling variability
  + A smaller SE suggests a more precise estimate (=good)
  
???
+ brief reminders on test statistics
  + every quantity we wish to calculate a significance test for needs an test statistic.
  + the test statistic is a value that has a known sampling distribution
+ If sampling distribution is unfamiliar, again, recap the hypothesis testing material


---
# Lets look at the output from `lm` again

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(7284) 

sim_arguments <- list(
  formula = y ~ 1 + hours + motivation,
  fixed = list(hours = list(var_type = 'ordinal', levels = 0:15),
               motivation = list(var_type = 'continuous', mean = 0, sd = 1)),
  error = list(variance = 20),
  sample_size = 150,
  reg_weights = c(0.6, 1.4, 1.5)
)

df <- simulate_fixed(data = NULL, sim_arguments) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)

test_study2 <- df %>%
  dplyr::select(y, hours, motivation) %>%
  mutate(
    ID = paste("ID", 101:250, sep = ""),
    score = round(y+abs(min(y))),
    motivation = round(motivation, 2)
  ) %>%
  dplyr::select(ID, score, hours, motivation)

performance <- lm(score ~ hours + motivation, data = test_study2)

```


```{r}
summary(performance)
```


---
# And work out the $t$-values

+ Let's check the value for `motivation` together:

$$t = \frac{\hat \beta_2}{SE(\hat \beta_2)} = \frac{0.9163}{0.3838} = 2.388(3dp)$$

+ (Feel free to check `hours` in your own time)

+ So we know where the $\beta$ values come from, and we have just seen $t$
+ What about the $SE$ and $p$?

---
#  SE( $\hat \beta_j$ )

+ The formula for the standard error of the slope is:

$$SE(\hat \beta_j) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_{ij} - \bar{x_{j}})^2(1-R_{xj}^2)}}$$

+ Where:
	+ $SS_{Residual}$ is the residual sum of squares
	+ $n$ is the sample size
	+ $k$ is the number of predictors
	+ $x_{ij}$ is the observed value of a predictor ( $j$ ) for an individual ( $i$ )
	+ $\bar{x_{j}}$ is the mean of a predictor
	+ $R_{xj}^2$ derives from the multiple correlation coefficient of the predictors

+ $R_{xj}^2$ captures to degree to which all of our predictors are related to each other
  + For simple linear models, $R_{xj}^2$ = 0 as there is only 1 predictor
  
---
# SE( $\hat \beta_j$ )

$$SE(\hat \beta_j) = \sqrt{\frac{ SS_{Residual}/(n-k-1)}{\sum(x_{ij} - \bar{x_{j}})^2(1-R_{xj}^2)}}$$  
+ We want our $SE$ to be smaller - this means our estimate is precise

+ Examining the above formula we can see that:
	+ $SE$ is smaller when residual variance ( $SS_{residual}$ ) is smaller
	+ $SE$ is smaller when sample size ( $n$ ) is larger
	+ $SE$ is larger when the number of predictors ( $k$ ) is larger
	+ $SE$ is larger when a predictor is strongly correlated with other predictors ( $R_{xj}^2$ )


???
+ We'll return to this later when we discuss multi-collinearity issues

---
# Sampling distribution for the null

.pull-left[


+ So what about $p$?

+ $p$ refers to the likelihood of having results as extreme as ours, given $H_0$ is true

+ To compute that likelihood, we need a sampling distribution for the null 

+ For $\beta$, this is a ** $t$-distribution** 

+ Remember, the shape of the $t$-distribution changes depending on the degrees of freedom

]

.pull-right[
```{r, echo = F, warning = F, message = F, fig.height = 3.5}
ggplot() + 
  xlim(-4, 4) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=5), 
                colour = baseColour,
                linewidth = 1) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=10),
                colour = '#0F4C81',
                linewidth = 1) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=50),
                colour = '#88B04B',
                linewidth = 1) +
  labs(x='t', y = '') +
  annotate(geom = 'text', label = 'df = 5', colour = baseColour, x = 2, y = .3, size = 5) +
  annotate(geom = 'text', label = 'df = 10', colour = '#0F4C81', x = 2, y = .33, size = 5) +
  annotate(geom = 'text', label = 'df = 50', colour = '#88B04B', x = 2, y = .36, size = 5) +
  theme(axis.title = element_text(size = 12, face = 'bold'))
```

]

--

+ For $\beta$, we use a $t$-distribution with ** $n-k-1$ degrees of freedom**.
	+ $n$ = sample size
	+ $k$ = number of predictors
	+ The additional - 1 represents the intercept

---
#  A decision about the null 
+ We have a $t$-value associated with our $\beta$ coefficient in the R model summary
	
	+ $t$ = 2.388

+ We evaluate it against a $t$-distribution with $n-k-1$ degrees of freedom

--

  + $df$ = 150-2-1 = 147 

+ As with all tests we need to set our $\alpha$
	
	+ Let's set $\alpha$ = 0.05 (two tailed)

--

+ Now we need a critical value to compare our observed $t$-value to

---
# Visualise the null

.pull-left[
```{r, echo = F, fig.height=5}

(p <- ggplot() + 
  xlim(-5, 5) +
  stat_function(fun=dt,
                geom = "line",
                args = list(df=147),
                linewidth = 1) +
  labs(x='\n t', y = '') + 
  ggtitle("t-distribution (df=147)") +
  theme(axis.title=element_text(size = 14, face = 'bold'),
        plot.title=element_text(size = 14, face = 'bold')))

```

]

.pull-right[

+ $t$-distribution with 147 df (our null distribution)

]
---
count: false

# Visualise the null

.pull-left[
```{r, echo = F, fig.height=5, message = F}
p
```

]

.pull-right[
+ $t$-distribution with 147 df (our null distribution)

+ Critical values $(t^*)$ establish a boundary for significance
  
  + The probability that a $t$-value will fall within these extreme regions of the distribution given $H_0$ is true is equal to $\alpha$
    + Because we are performing a two-tailed test, $\alpha$ is split between each tail:

]

---
count: false

# Visualise the null

.pull-left[
```{r, echo = F, fig.height=5, message = F}

(p <- p + stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.025, 147), -4),
                alpha=.25,
                fill = "blue",
                args = list(df=147)) +
    stat_function(fun = dt, 
                geom = "area",
                xlim = c(qt(0.975, 147), 4),
                alpha=.25,
                fill = "blue",
                args = list(df=147)) +
  geom_vline(xintercept = qt(0.025, 147), linewidth = .2, linetype='dashed') +
  geom_vline(xintercept = qt(0.975, 147), linewidth = .2, linetype='dashed'))

```

]

.pull-right[
+ $t$-distribution with 147 df (our null distribution)

+ Critical values $(t^*)$ establish a boundary for significance
  
  + The probability that a $t$-value will fall within these extreme regions of the distribution given $H_0$ is true is equal to $\alpha$
    + Because we are performing a two-tailed test, $\alpha$ is split between each tail:

```{r}
(LowerCrit = round(qt(0.025, 147), 3))
```

```{r}
(UpperCrit = round(qt(0.975, 147), 3))
```


]

---
count: false

# Visualise the null

.pull-left[
```{r, echo = F, fig.height=5, message = F}

p + geom_vline(xintercept = 2.388, colour = baseColour) +
  stat_function(fun = dt, 
                geom = "area",
                xlim = c(2.388, 4),
                alpha= .7,
                fill = baseColour,
                args = list(df=147))

```

]

.pull-right[
+ $t$-distribution with 147 df (our null distribution)

+ Critical values $(t^*)$ establish a boundary for significance
  
  + The probability that a $t$-value will fall within these extreme regions of the distribution given $H_0$ is true is equal to $\alpha$
    + Because we are performing a two-tailed test, $\alpha$ is split between each tail:

```{r}
(LowerCrit = round(qt(0.025, 147), 3))
```

```{r}
(UpperCrit = round(qt(0.975, 147), 3))
```

+ $t$ = 2.388, $p$ = .018

]


???
+ discuss this plot.
+ remind them of 2-tailed
+ areas
+ % underneath each end
+ comment on how it would be different one tailed
+ remind about what X is, thus where the line is

---
class: center, middle

# Questions?

---
class: inverse, center, middle

# Part 3: An alternative using confidence intervals

---
# Refresher: What is a confidence interval?

+ When we perform these analyses, we obtain a parameter estimate from our sample (e.g. $\beta_2 = 0.92$)

+ It's unlikely that the true value is exactly equal to our parameter estimate

--

+ We can be much more certain we've captured the true value if we report **confidence intervals**
  
  + Range of plausible values for the parameter
  
  + The wider the range, the more confident we can be that our interval captures the true value

--

      + How many of you are confident that I'm exactly 35 years old?
      
      + How many of you are confident that I'm between 33 & 38 years old?
    
      + How many of you are confident that I'm between 29 & 42 years old?
    
      + How many of you are confident that I'm between 25 & 46 years old?

---
# Refresher: What is a confidence level?

+ To create a confidence interval we must decide on a **confidence level**
  
  + A number between 0 and 1 specified by us
  
  + How confident do you want to be that the confidence interval will contain the true parameter value?

+ Typical confidence levels are 90%, 95%, or 99%

--

> **Test your understanding:** If we select a 90% confidence level, will the range of values included in our CI be smaller or larger than if we selected a 99% confidence level?



---
#  Confidence intervals for $\beta$
+ We can also compute confidence intervals for $\hat \beta$

+ The $100 (1 - \alpha)$, e.g., 95%, confidence interval for the slope is:

$$\hat \beta_1 \pm t^* \times SE(\hat \beta_1)$$
--


+ So, the 95% confidence interval for the effect of `motivation` would be:

```{r}
(LowerCI = round(0.91634 - (qt(0.975, 147) * 0.38376), 3))
(UpperCI = round(0.91634 + (qt(0.975, 147)* 0.38376), 3))
```

--

+ We can be 95% confident that the range 0.158 and 1.675 contains the true value of our $\beta_2$


---
# `confint` function

+ We can get confidence intervals for our models more easily:

```{r}
confint(performance)
```

+ The confidence intervals for both `motivation` and `hours` do not include the null value (in this case, 0) 

+ This provides support (beyond $p<.05$) that **motivation and hours are statistically significant predictors of test scores**


---
class: center, middle

# Questions?

---
class: inverse, center, middle

# Part 4: Cofficient of determination ( $R^2$ )

---
# Model output again

```{r}
performance <- lm(score ~ hours + motivation, data = test_study2)
summary(performance)
```


---
#  Quality of the overall model 

+ When we measure an outcome ( $y$ ) in some data, the scores will vary (we hope).

  + Variation in $y$ = total variation of interest

--

+ The aim of our linear model is to build a model which describes our outcome variable as a function of our predictor variable(s)

	+ We are trying to explain variation in $y$ using variation in $x$
  	+ When $y$ co-varies with $x$...
  	+ we can predict changes in $y$ based on changes in $x$...
  	+ so we say the variance in $y$ is explained or accounted for

--

+ But the model will not explain all the variance in $y$

  + What is left unexplained is called the residual variance

--

+ We can break down variation in our data (i.e. variation in $y$) based on sums of squares as:

$$SS_{Total} = SS_{Model} + SS_{Residual}$$

---
#  Coefficient of determination 

+ One way to consider how good our model is, would be to consider the proportion of total variance our model accounts for 

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$$

+ $R^2$ = coefficient of determination

--

  + Quantifies the amount of variability in the outcome accounted for by the predictors
  + The more variance accounted for, the better the model fit
  + Represents the extent to which the prediction of $y$ is improved when predictions are based on the linear relation between $x$ and $y$, compared to not considering $x$

--

+ To illustrate, we can calculate the different sums of squares


---
# Total Sum of Squares

.pull-left[
+ Each Sums of Squares measure quantifies different sources of variation

$$SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

+ Squared distance of each data point from the mean of $y$

+ Mean is our baseline

> **Test your understanding:** Why might this be the case?


]

.pull-right[

```{r, echo=FALSE, fig.height = 6}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color=baseColour, size = 2) +
  labs(x="Hours of Study", y = "Test Score") +
  ggtitle(latex2exp::TeX(r'($SS_{Total}$)')) + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12, face = 'bold'),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0), size = 12, face = 'bold'),
        plot.title = element_text(size = 16)) +
  geom_hline(aes(yintercept = mean(score)),color='#0F4C81', linewidth=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(rep(mean(score),10))), color = baseColour, lty = 2)
```

]

---
count: false

# Total Sum of Squares

.pull-left[
+ Each Sums of Squares measure quantifies different sources of variation

$$SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2$$

+ Squared distance of each data point from the mean of $y$

+ Mean is our baseline

> **Test your understanding:** Why might this be the case?

> Without any other information, our best guess at the value of $y$ for any person is the mean.

]

.pull-right[

```{r, echo=FALSE, fig.height=6}

test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color=baseColour, size = 2) +
  labs(x="Hours of Study", y = "Test Score") +
  ggtitle(latex2exp::TeX(r'($SS_{Total}$)')) + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12, face = 'bold'),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0), size = 12, face = 'bold'),
        plot.title = element_text(size = 16)) +
  geom_hline(aes(yintercept = mean(score)),color='#0F4C81', linewidth=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(rep(mean(score),10))), color = baseColour, lty = 2)

```

]


---
# Residual Sum of Squares

.pull-left[
+ Each Sums of Squares measure quantifies different sources of variation

$$SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

+ This may look familiar

+ Squared distance of each point from the predicted value
]

.pull-right[


```{r, echo=FALSE, fig.height=6}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color=baseColour, size = 2) +
  labs(x="Hours of Study", y = "Test Score") +
  ggtitle(latex2exp::TeX(r'($SS_{Residual}$)')) + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12, face = 'bold'),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0), size = 12, face = 'bold'),
        plot.title = element_text(size = 16)) +
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", linewidth=1) + 
  geom_segment(aes(x = hours, y = score, xend = hours, yend = c(res$fitted.values)), color = baseColour, lty =2)

```

]


---
# Model Sums of Squares

.pull-left[
+ Each Sums of Squares measure quantifies different sources of variation

$$SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2$$

+ The deviance of the predicted scores from the mean of $y$

+ Easy to calculate if we know total sum of squares and residual sum of squares

$$SS_{Model} = SS_{Total} - SS_{Residual}$$

]

.pull-right[

```{r, echo=FALSE, fig.height=6}
test %>%
  ggplot(., aes(hours, score)) +
  geom_point(color=baseColour, size = 2) +  
  labs(x="Hours of Study", y = "Test Score") +
  ggtitle(latex2exp::TeX(r'($SS_{Model}$)')) + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size = 12, face = 'bold'),
        axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 10, l = 0), size = 12, face = 'bold'),
        plot.title = element_text(size = 16)) +
  geom_hline(aes(yintercept = mean(score)),color='#0F4C81', linewidth=1) + 
  geom_abline(aes(intercept=res$coefficients[1], slope=res$coefficients[2]), color="lightblue", linewidth=1) + 
  geom_segment(aes(x = hours, c(res$fitted.values), xend = hours, yend = c(rep(mean(score),10))), color = baseColour, lty =2)
```

]

---
# Values in our sample
+ In the current example, these values are:

  + $SS_{total}$ = 8556.06
  + $SS_{residual}$ = 2826.83
  + $SS_{model}$ = 5729.23

+ In the Learn folder for this week, there is a document that shows the calculations from the raw data


---
#  Coefficient of determination 
+ Let's come back to $R^2$

$$R^2 = 1 - \frac{SS_{Residual}}{SS_{Total}}$$

+ Or

$$R^2 = \frac{SS_{Model}}{SS_{Total}}$$

+ So in our example:

$$R^2 = \frac{SS_{Model}}{SS_{Total}} = \frac{5729.23}{8556.06} = 0.6695$$

--

** $R^2$ = 0.6695 means that 66.95% of the variation in test scores is accounted for by hours of revision and student motivation.**

---
#  Check against model output 

```{r}
summary(performance)
```

???
We can check this against the R-output:
Be sure to flag small amounts of rounding difference from working through "by hand" and so presenting to less decimal places.

---
#  Adjusted $R^2$ 

+ When there are two or more predictors, $R^2$ tends to be an inflated estimate of the corresponding population value

  + Due to random sampling fluctuation, even when $R^2 = 0$ in the population, it's value in the sample may $\neq 0$ 

  + In **smaller samples** , the fluctuations from zero will be larger on average

  + With **more predictors** , there are more opportunities to add to the positive fluctuation


+ We therefore compute an adjusted $R^2$


$$\hat R^2 = 1 - (1 - R^2)\frac{N-1}{N-k-1}$$

+ Adjusted $R^2$ adjusts for both sample size ( $N$ ) and number of predictors ( $k$ )

---
#  In our example 

.pull-left[
```{r, eval = F}
summary(performance)
```

```{r, echo=FALSE, out.height='50%'}
sum_performance <- summary(performance)
knitr::include_graphics('figs/perfResults.png')
```
]

.pull-right[
+ **Based on adjusted R-squared, hours studying and student motivation explain `r round(sum_performance$adj.r.squared,3)*100`% of the variance in test scores**

+ As the sample size is large and the number of predictors small, unadjusted (`r round(sum_performance$r.squared,3)`) and adjusted R-squared (`r round(sum_performance$adj.r.squared,3)`) are similar
]

---
class: center, middle

# Questions?

---
# Summary

+ Key take homes:
  1. We have an inferential test, based on a $t$-distribution, for the significance of $\beta$
  2. We can compute confidence intervals that give us more certainty that we have captured the true value of $\beta$
  3. We are more likely to find a statistically significant effect when residuals are small and we have a large sample
  4. We can assess the degree to which our model explains variance in the outcome based on $R^2$
  5. When we have multiple predictors, we should use the adjusted $R^2$ to get a more conservative estimate
  
+ Next week we will look at overall model significance and comparisons between models

---

## This week 

.pull-left[

### Tasks

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

Quizzes from now onwards contribute to your final mark (14/18 best scores counted)

]

.pull-right[

### Support

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

<br>

```{r, echo = F, out.width='10%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

]


---
class: inverse, center, middle

# Thanks for listening