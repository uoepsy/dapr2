---
title: "Categorical predictors and dummy coding"
author: "Elizabeth Pankratz (elizabeth.pankratz@ed.ac.uk)"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=F}
library(tidyverse)
library(patchwork)
library(emmeans)
library(simglm)
library(latex2exp)  # for betas in ggplots
source('_theme/theme_quarto.R')

theme_set(theme_quarto(title_font_size=42))
theme_update(
  text = element_text(family = 'Source Sans 3'),
  axis.title.y = element_text(angle=0, 
                              vjust=0.5, 
                              hjust = 0,
                              margin = margin(t = 0, r = 10, b = 0, l = 0)
                              ),

  )

dapr2red <- "#BF1932" 
pal <- c("#3173c9", "#ff94b0", "#51b375")
```


```{r message=F, warning=F, include=F}
# simulate the data

set.seed(3119) 

sim_arguments <- list(
  formula = y ~ 1 + hours + motivation + study + method,
  fixed = list(hours = list(var_type = 'ordinal', levels = 0:15),
               motivation = list(var_type = 'continuous', mean = 0, sd = 1),
               study = list(var_type = 'factor', 
                            levels = c('alone', 'others'),
                            prob = c(0.53, 0.47)),
               method = list(var_type = 'factor', 
                            levels = c('read', 'summarise', 'self-test'),
                            prob = c(0.3, 0.4, 0.3))),
  error = list(variance = 20),
  sample_size = 250,
  reg_weights = c(0.6, 1.4, 1.5, 6, 6, 2)
)

df3 <- simulate_fixed(data = NULL, sim_arguments) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)

score_data <- df3 %>%
  dplyr::select(y, hours, motivation, study, method) %>%
  mutate(
    ID = paste("ID", 101:350, sep = ""),
    score = round(y+abs(min(y))),
    motivation = round(motivation, 2),
    study = factor(study),
    method = factor(method)
  ) %>%
  dplyr::select(ID, score, hours, motivation, study, method)

score_data <- score_data |>
  mutate(study_num = ifelse(study == 'alone', 0, 1))

# get group means
mean_alone <- filter(score_data, study == 'alone')$score |> mean()
sd_alone <- filter(score_data, study == 'alone')$score |> sd()
mean_others <- filter(score_data, study == 'others')$score |> mean()
sd_others <- filter(score_data, study == 'others')$score |> sd()
```




# Course Overview {.smaller}

<br>

:::: {.columns}

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block1_name = "Introduction to Linear Models"
block1_lecs = c("Intro to Linear Regression",
                "Interpreting Linear Models",
                "Testing Individual Predictors",
                "Model Testing & Comparison",
                "Linear Model Analysis")
block2_name = "Analysing Experimental Studies"
block2_lecs = c("Categorical Predictors & Dummy Coding",
                "	Effects Coding & Coding Specific Contrasts",
                "Assumptions & Diagnostics",
                "Bootstrapping",
                "	Categorical Predictor Analysis")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week = 6)
```

:::

::: {.column width="50%"}

```{r echo = FALSE, results='asis', warning = FALSE}
block3_name = "Interactions"
block3_lecs = c("Interactions I",
                "Interactions II",
                "Interactions III",
                "Analysing Experiments",
                "Interaction Analysis")
block4_name = "Advanced Topics"
block4_lecs = c("Power Analysis",
                "Binary Logistic Regression I",
                "Binary Logistic Regression II",
                "Logistic Regression Analysis",
                "	Exam Prep and Course Q&A")

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block3_name,block4_name,block3_lecs,block4_lecs,week = 0)
```

:::

::::

## This week's learning objectives

<br>

::: {.fragment}
::: {.dapr2callout}
How can we include categorical variables as predictors in a linear model?
:::
:::

::: {.fragment}
::: {.dapr2callout}
When we use a categorical predictor, how do we interpret the linear model’s coefficients?
:::
:::

::: {.fragment}
::: {.dapr2callout}
What hypotheses are tested by the default way that R represents categorical predictors?
:::
:::

::: {.fragment}
::: {.dapr2callout}
If we wanted to test different hypotheses after fitting the model, how would we do that?
:::
:::




<!-- ======================================== -->

# Categorical predictors with two levels <br>("binary" predictors)

## Examples of binary predictors  {.smaller}

![](figs/TODO-binary.jpeg){fig-align="center"}

<!-- TODO: use study data as example -->

But: linear models can only deal with input in number form.

So we need some way of representing these variables numerically.

Today: **dummy coding**, aka **treatment coding**.


## How does dummy/treatment coding represent categories as numbers?

Represent one level as 0, and the other level as 1.


:::: {.columns}
::: {.column width="50%"}
**Schematic:**

![](figs/TODO-trtmt.jpeg){fig-align="center"}
Here, `alone` is coded as 0, `others` is coded as 1.

<!-- TODO: use study data as example -->

:::
::: {.column width="50%"}
**In the data:**

:::{ style="font-size: 80%;" }

```{r df head}
score_data |>
  select(ID, study, study_num) |>
  head(10)
```

:::

:::
::::


## Does it matter which level is coded as 0 and which level is coded as 1?

:::{.hcenter}
Yes.

:::

:::{style="font-size: 70%;"}

To illustrate: Here are some test scores from students who studied either `alone` (blue) or with `others` (pink).
:::

:::: {.columns}
::: {.column width="50%"}

```{r plot violin, echo=F, fig.width = 8, fig.asp=1, fig.align='center'}
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, size = 5, width = 0.3) +
  theme(
    legend.position = 'none',
    panel.grid.minor = element_blank()
    ) +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +
  # stat_summary(colour = 'black', fun = mean, geom = 'point', size = 2) +
  # geom_segment(aes(x = 'alone', xend = 'others', y = mean_alone, yend = mean_others), colour = 'black') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  NULL
```

:::

::: {.column width="50%"}

```{r plot xy 1, echo=F, fig.width = 8, fig.asp=1, fig.align='center'}
xlim_lower <- -2.2
xlim_upper <-  2.2
ylim_lower <- -15
ylim_upper <-  55

set.seed(1)  # seed for constant jitter
p_xy_study <- score_data |>
  ggplot(aes(x = study_num, y = score)) +
  geom_jitter(aes(colour = study), alpha = 0.25, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0,
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, 
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  scale_colour_manual(values = pal) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = 'bottom'
  ) +
  labs(
    x = 'study (in numeric space)'
  ) +
 guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  NULL

p_xy_study
```


:::
::::



## Does it matter which level is coded as 0 and which level is coded as 1? {.smaller}

<!-- TODO: dont' just use fragments, fully duplicate slides, so that there's a version of the PEOE slide that doesn't overlay the solution on the main slide -->

:::{.hcenter}
Yes.

:::

:::: {.columns}
::: {.column width="50%"}

::: {.r-stack}
```{r plot xy 2, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study
```

::: {.fragment}

```{r plot xy with line, echo=F, fig.width=8, fig.asp=1}
gm <-(mean_alone + mean_others)/2

set.seed(1)
p_xy_study <- p_xy_study + 
  geom_abline(slope = 4.73, intercept = mean_alone, colour = 'black', linewidth = 2) +
  NULL

p_xy_study
```
:::

:::


A linear model will fit a line to this data.

Before I show you this line, I want you to make predictions about it.

<!-- (You'll learn better long-term if you genuinely guess and avoid peeking ahead in the slides.) -->



:::
::: {.column width="50%" style="font-size: 70%;"}
**Activity! Individual {{< iconify material-symbols person-rounded size=1.5em >}} and in pairs/threes {{< iconify material-symbols group-outline-rounded size=1.5em >}} :**

- **{{< iconify material-symbols person-rounded size=1.5em >}} Predict**: Write down your guesses: 

  1. The line's intercept will be the same as the mean of either **alone** or **others**. Which one? Why?
  2. Will the slope of the line be **positive** or **negative**? Why?


- **{{< iconify material-symbols group-outline-rounded size=1.5em >}} Explain**: 
Why do you think your guesses are likely to be correct?


- **{{< iconify material-symbols person-rounded size=1.5em >}} Observe**: Did your guesses match the results?


- **{{< iconify material-symbols group-outline-rounded size=1.5em >}} Explain**: Why are the results the way they are?

:::
::::

<!-- at Observe, show line, annotate it with Intercept and Slope. ask someone to articulate it in plenum (wait til several hands raised). -->

## How do we describe this line?


:::: {.columns}
::: {.column width="50%"}

:::{.r-stack}

```{r plot xy pre betas, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study
```

:::{.fragment}
```{r plot xy with betas, echo=F, fig.width=8, fig.asp=1}
set.seed(1)  # seed for constant jitter
p_xy_study +
  # right triangle
  geom_segment(x = 0, xend = 1, y = mean_alone, yend = mean_alone,
               colour = dapr2red, linewidth = 2) +
  geom_segment(x = 1, xend = 1, y = mean_alone, yend = mean_others,
               colour = dapr2red, linewidth = 2) +
  # arrows
  geom_segment(x = -.8, xend = -.15, y = mean_alone+13, yend = mean_alone+3, arrow = arrow(), colour = dapr2red) +
  geom_segment(x = 1.7, xend = 1.1, y = gm, yend = gm, arrow = arrow(), colour = dapr2red) +
  # betas
  geom_label(x = -1, 
             y = mean_alone+13, 
             label = TeX('$\\hat{\\beta}_0$'), 
             size = 12, 
             col = dapr2red) +
  geom_label(x = 1.8, 
             y = gm, 
             label = TeX('$\\hat{\\beta}_1$'), 
             size = 12, 
             col = dapr2red) +
  NULL
```

:::
:::

:::
::: {.column width="50%"  style="font-size: 70%;"}
Using our mathematical toolkit:

$$
score_i = \beta_0 + (\beta_1 \times study) + \epsilon_i
$$

<br>

**$\beta_0$:** intercept, mean of `alone` (the level coded as 0):

```{r}
mean_alone
```

<br>

**$\beta_1$:** slope, the difference between mean of `others` (coded as 1) and mean of `alone` (coded as 0):

```{r}
mean_others - mean_alone
```

<br>

**$\epsilon_i$:** error for each individual data point $i$

:::
::::

<br>

::: {.hcenter}
**Questions!**
:::


---

<br>

We can find these parameters values all on our own, without a linear model.

But we need a linear model to answer research questions like:

**Do students who study with others score significantly better than students who study alone?**

<br>

Next, we'll fit a model that predicts score as a function of study patterns: `score ~ study`.

<!-- my instinct is to want students to do some predictions here, see what param estimates would be consistent with each outcome. but I think they don't know yet what hyps are being tested by each param. so it might be premature. we can do it later.-->



## Modelling `score ~ study` {.smaller}

```{r}
m1 <- lm(score ~ study, data = score_data)
```

```{r}
summary(m1)
```

<br>

**Those coefficient estimates look familiar...**

:::: {.columns}
::: {.column width="50%"}
```{r}
mean_alone
```
:::
::: {.column width="50%"}
```{r}
mean_others - mean_alone
```

:::
::::




## Model evaluation works the same as before

<br>

**$R^2$:**

- Still tells us how much variance in the data is accounted for by the model.

```{r echo=F}
cat(capture.output(summary(m1))[17])
```

<br>

**$F$-ratio (aka $F$-statistic):**

- Still tells us the ratio of explained variance to unexplained variance.
- Still tests the hypothesis that all regression slopes in the model = 0.

```{r echo=F}
cat(capture.output(summary(m1))[18])
```

<br>

(Revisit Lecture 4 for a refresher!)


## What hypotheses are being tested for each coefficient? {.smaller}

```{r echo=F}
summary(m1)$coefficients
```

<br> 

**`(Intercept)` aka $\beta_0$:**

- Null hypothesis (aka H0): the intercept is equal to zero (aka $\beta_0 = 0$).
- $p$-value (aka `Pr(>|t|)`): the probability of observing an intercept of `r mean_alone`, assuming that the true value of the intercept is zero.


**`studyothers` aka $\beta_1$:**

- Null hypothesis (aka H0): the difference between levels is equal to zero (aka $\beta_1 = 0$).
- $p$-value: the probability of observing a difference of `r round(mean_others-mean_alone, 2)`, assuming that the true value of the difference is zero.

<br>

Our research question was about how studying alone or with others is associated with test score.

**Are both of these hypothesis tests relevant to our research question?**

<!-- if time, could do a three-minute TPS here -->



## Reporting the model's estimates

> Study habits significantly predicted student test scores, F(`r summary(m1)$fstatistic[['numdf']]`, `r summary(m1)$fstatistic[['dendf']]`) = `r summary(m1)$fstatistic[['value']]`, p < 0.001.

```{verbatim}
Study habits significantly predicted student test scores, F(`r summary(m1)$fstatistic[['numdf']]`, `r summary(m1)$fstatistic[['dendf']]`) = `r summary(m1)$fstatistic[['value']]`, p < 0.001.
```

> Study habits explained `r round( summary(m1)$r.squared * 100, 2)`% of the variance in test scores.

```{verbatim}
Study habits explained `r round( summary(m1)$r.squared * 100, 2)`% of the variance in test scores.
```

> Specifically, students who studied with others (M = `r round(mean_others, 2)`, SD = `r round(sd_others, 2)`) scored significantly higher than students who studied alone (M = `r round(mean_alone, 2)`, SD = `r round(sd_alone, 2)`), 
$\beta_1$ = `r round( summary(m1)$coefficients['studyothers', 'Estimate'], 2)`, 
SE = `r round( summary(m1)$coefficients['studyothers', 'Std. Error'], 2)`, 
t = `r summary(m1)$coefficients['studyothers', 't value']`, 
p < 0.001.

```{verbatim}
Specifically, students who studied with others (M = `r round(mean_others, 2)`, SD = `r round(sd_others, 2)`) scored significantly higher than students who studied alone (M = `r round(mean_alone, 2)`, SD = `r round(sd_alone, 2)`), $\beta_1$ = `r round( summary(m1)$coefficients['studyothers', 'Estimate'], 2)`, SE = `r round( summary(m1)$coefficients['studyothers', 'Std. Error'], 2)`, t = `r summary(m1)$coefficients['studyothers', 't value']`, p < 0.001.
```

<br>

::: {.hcenter}
**Questions!**
:::


## Visualising the model's estimates

![](figs/TODO-nonref-ci.jpeg){fig-align="center"}

We have almost all the pieces!

But finding the 95% CI for the non-reference level is mathematically a little complicated.
(It's not just the 95% CI of $\beta_1$, the slope parameter.)

A nice shortcut: **an R package called `emmeans`.**

<!-- - The linear model gives us intercept and slope parameters. -->
<!-- - We have mean and 95% CI for the reference level (= the intercept). -->
<!-- - What if we want mean and 95% CI for the non-reference level? -->
<!-- - To get the mean, we can just add intercept and slope parameters together. -->
<!-- - But finding the CI is mathematically more complicated, so, here's a shortcut: -->


## `emmeans` = "Estimated Marginal Means"

- **"Estimated"** $\rightarrow$ based on a model's estimates
- **"Marginal"**  $\rightarrow$ for each level of the predictor
- **"Means"**  $\rightarrow$ means of the estimated outcomes for each level of the predictor—but also confidence intervals!


## `emmeans` = "Estimated Marginal Means"

We already have our model:

```{r}
m1 <- lm(score ~ study, data = score_data)
```

<br> 

Use `emmeans()` to get each group's estimated mean and the variability associated with it.

```{r}
m1_emm <- emmeans(m1, ~study)
m1_emm
```

<br>

In `lower.CL` and `upper.CL` are our 95% confidence intervals.


## Plotting marginal means and CIs

`emmeans` has a built-in plot style.

```{r fig.align='center'}
plot(m1_emm) + coord_flip()
```


It's OK, but a bit boring. We can definitely do better.


## Plotting marginal means and CIs

:::: {.columns}
::: {.column width="50%" style="font-size: 70%;"}
```{r eval=F}
# Save EMMs in tibble format, 
# with compatible column names
m1_emm_df <- m1_emm |>
  as_tibble() |>
  rename(score = emmean)

# Plot the data like before, 
# and now overlay EMMs
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.3, size = 5) +
  theme(legend.position = 'none') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  geom_errorbar(
    data = m1_emm_df, 
    aes(ymin = lower.CL, ymax = upper.CL), 
    colour = 'black', 
    width = 0.2,
    linewidth = 2
  ) +
  geom_point(
    data = m1_emm_df, 
    colour = 'black', 
    size = 5
  )
```
:::
::: {.column width="50%"}
```{r echo=F, fig.width=8, fig.asp=1}
# Save EMMs in tibble format, 
# renaming "emmean" column so 
# it matches the original data
m1_emm_df <- m1_emm |>
  as_tibble() |>
  rename(score = emmean)

# Plot the data like before, 
# and now overlay EMMs
score_data |>
  ggplot(aes(x = study, y = score, fill = study, colour = study)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.3, size = 5) +
  theme(legend.position = 'none') +
  scale_colour_manual(values = pal) +
  scale_fill_manual(values = pal) +
  # The error bar with 95% CIs:
  geom_errorbar(
    data = m1_emm_df, 
    aes(ymin = lower.CL, ymax = upper.CL), 
    colour = 'black', 
    width = 0.2,
    linewidth = 2
  ) +
  # The point with group means:
  geom_point(
    data = m1_emm_df, 
    colour = 'black', 
    size = 5
  )
```
:::
::::

`emmeans` is an amazingly useful tool.

It can do LOTS more than we've seen here!

We'll see more examples in the coming days and weeks.


## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-06-0.svg){.fragment height="550" }

![](figs/block2-flowchart-06-1.svg){.fragment height="550" }

![](figs/block2-flowchart-06-2.svg){.fragment height="550" }
:::




## Extension: Changing the reference level {.smaller}

:::: {.columns}
::: {.column width="50%"}

If reference level = `alone`:

```{r plot xy ref alone, echo = F, fig.width=8, fig.asp = 1}
set.seed(1)
p_xy_study
```

:::
::: {.column width="50%"}

If reference level = `others`:

```{r plot xy ref others, echo=F, fig.width = 8, fig.asp = 1}
score_data |>
  mutate(study_num = ifelse(study == 'others', 0, 1)) |>
  ggplot(aes(x = study_num, y = score)) +
  geom_jitter(aes(colour = study), alpha = 0.25, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0,
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, 
               arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  scale_colour_manual(values = pal) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = 'bottom'
  ) +
  labs(
    x = 'study (in numeric space)'
  ) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
  geom_abline(slope = -4.73, intercept = mean_others, colour = 'black', linewidth = 2) +
  NULL
```

:::
::::

When `others` is the reference level (that is, the level represented as 0):

- What does the intercept represent?
- What does the slope represent?
- Why is the slope negative now, when before it was positive?
- Challenge: What hypotheses would a linear model test about this data?


<!-- ======================================== -->

# Categorical predictors with >2 levels

## Examples of categorical predictors with >2 levels

![](figs/TODO-categ.jpeg){fig-align="center"}


## Today's data: Study method

```{r include=F}
mean_read <- filter(score_data, method == 'read')$score |> mean()
sd_read <- filter(score_data, method == 'read')$score |> sd()
mean_self <- filter(score_data, method == 'self-test')$score |> mean()
sd_self <- filter(score_data, method == 'self-test')$score |> sd()
mean_summ <- filter(score_data, method == 'summarise')$score |> mean()
sd_summ <- filter(score_data, method == 'summarise')$score |> sd()
```

:::{.r-stack}
```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol <- score_data |>
  ggplot(aes(x = method, y = score, fill = method, colour = method)) +
  geom_violin(alpha = 0.5) +
  geom_jitter(alpha = 0.5, width = 0.2, size = 5) +
  theme(legend.position = 'none') +
  scale_fill_manual(values = pal) +
  scale_colour_manual(values = pal) +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 5) +
  NULL

p_viol
```
:::{.fragment}
```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol +
  # line from read to self-test:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'self-test', y = mean_read, yend = mean_self), linewidth = 2) +
  # line from read to summarise:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'summarise', y = mean_read, yend = mean_summ), linewidth = 2) +
  NULL
```
:::
:::

:::{style="font-size: 80%;"}
How do we fit a line to data from three groups?

- It's impossible to draw one single straight line through all three group means.
- The smallest number of straight lines that connect all three group means is **two**.
- For this reason, we're going to use **two** predictors, called **"dummy variables"**.
:::



## Dummy variables let us extend dummy/treatment coding to >2 levels

:::: {.columns}
::: {.column width="50%"}

```{r echo=F, fig.width = 10, fig.align = 'center'}
set.seed(1)
p_viol +
  # line from read to self-test:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'self-test', y = mean_read, yend = mean_self), linewidth = 2) +
  # line from read to summarise:
  geom_segment(colour = 'black', aes(x = 'read', xend = 'summarise', y = mean_read, yend = mean_summ), linewidth = 2) +
  NULL
```


:::
::: {.column width="50%"}

Both dummy variables have the same reference level: `read`.

The first dummy variable will compare `self-test` back to `read`.

The second dummy variable will compare `summarise` back to `read`.

:::
::::

Let's look at the dummy variables in numeric space.


## Dummy variables let us extend dummy/treatment coding to >2 levels {.smaller}

:::: {.columns}
::: {.column width="50%"}

First dummy variable: `self-test` vs. `read`.

```{r xy dummy 1, echo=F, fig.width=7.5, fig.asp=1}
xlim_lower <- -2.2
xlim_upper <-  2.2
ylim_lower <- -25
ylim_upper <-  55

set.seed(1)
p1 <- score_data |> 
  filter(method %in% c('read', 'self-test')) |>
  mutate(method_num = ifelse(method == 'read', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_self, colour = 'black') +
  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_read, colour = 'red', linewidth = 2) +
  # geom_segment(x = 1, xend = 1, y = mean_read, yend = mean_self, colour = 'red', linewidth = 2) +
  labs(
    x = 'method (in numeric space)'
  ) +
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[1], pal[2])) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  geom_abline(slope = (mean_self - mean_read), intercept = mean_read, linewidth = 2) +
  NULL
p1
```
:::
::: {.column width="50%"}

Second dummy variable: `summarise` vs. `read`.

```{r xy dummy 2, echo=F, fig.width=7.5, fig.asp=1}
set.seed(1)
p2 <- score_data |> 
  filter(method %in% c('read', 'summarise')) |>
  mutate(method_num = ifelse(method == 'read', 0, 1)) |>
  ggplot(aes(x = method_num, y = score, colour = method)) +
  geom_jitter(alpha = 0.3, width = 0.1, size = 5) +
  scale_x_continuous(limits = c(xlim_lower, xlim_upper), expand = c(0, 0)) +
  scale_y_continuous(limits = c(ylim_lower, ylim_upper), expand = c(0, 0)) +
  geom_segment(x = xlim_lower, xend = xlim_upper, y = 0, yend = 0, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  geom_segment(x = 0, xend = 0, y = ylim_lower, yend = ylim_upper, arrow = arrow(ends = 'both', length = unit(12, 'pt')), colour = 'black') +
  stat_summary(fun = mean, geom = 'point', colour = 'black', size = 8, show.legend = FALSE) +

  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_summ, colour = 'black') +
  # geom_segment(x = 0, xend = 1, y = mean_read, yend = mean_read, colour = 'red', linewidth = 2) +
  # geom_segment(x = 1, xend = 1, y = mean_read, yend = mean_summ, colour = 'red', linewidth = 2) +
  labs(
    x = 'method (in numeric space)'
  ) +  
  theme(legend.position = 'bottom') +
  scale_colour_manual(values = c(pal[1], pal[3])) +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  geom_abline(slope = (mean_summ - mean_read), intercept = mean_read, linewidth = 2) +
  NULL
p2
```

:::
::::

1. Is the slope of the first dummy variable **positive or negative**? {{< iconify codicon thumbsup-filled size=1em >}} {{< iconify codicon thumbsdown-filled size=1em >}}
2. Is the slope of the second dummy variable **positive or negative**? {{< iconify codicon thumbsup-filled size=1em >}} {{< iconify codicon thumbsdown-filled size=1em >}}
3. Is the **slope** of the first dummy variable **bigger** than the slope of the second dummy variable? {{< iconify codicon thumbsup-filled size=1em >}} {{< iconify codicon thumbsdown-filled size=1em >}} 
4. Is the **intercept** of the first dummy variable **bigger** than the intercept of the second dummy variable? {{< iconify codicon thumbsup-filled size=1em >}} {{< iconify codicon thumbsdown-filled size=1em >}}


## How can we tell that a model will use *these* particular dummy variables? {.smaller}

<br>

With the function `contrasts()`.

```{r}
contrasts(score_data$method)
```

<br>

- Each **column** contains one dummy variable.
  - `self-test` compares `self-test` (the 1 in that column) to `read`.
  - `summarise` compares `summarise` (the 1 in that column) to `read`.

- We know that the reference level is `read` because **in BOTH dummy variables, `read` is coded as 0.**


## Modelling `score ~ method` {.smaller}

In maths:

$$
score_i = \beta_0 + (\beta_1 \times method_{self-test}) + (\beta_2 \times method_{summarise}) + \epsilon_i
$$

In R:

```{r}
m2 <- lm(score ~ method, data = score_data)
```

Write down your prediction: **How many coefficients will this model estimate?**

<br>

```{r}
summary(m2)
```


## What does each coefficient mean? {.smaller}

```{r echo = F}
cat(paste0(capture.output(summary(m2)), '\n')[9:13])
```

<br>

**`(Intercept)` aka $\beta_0$:**
The mean score for the reference level (`read`).

```{r}
mean_read
```

<br>

**`methodself-test` aka $\beta_1$:**
The difference between the mean score of `self-test` and the mean score of `read`.

```{r}
mean_self - mean_read
```

<br>

**`methodsummarise` aka $\beta_2$:**
The difference between the mean score of `summarise` and the mean score of `read`.

```{r}
mean_summ - mean_read
```

<!-- TODO: activity, predict what these coefs actually mean. -->


## What hypotheses are being tested for each coefficient? {.smaller}

```{r echo = F}
cat(paste0(capture.output(summary(m2)), '\n')[10:13])
```

<br>

**`(Intercept)` aka $\beta_0$:**

- Null hypothesis: The mean score for the reference level (`read`) is equal to zero.
- $p$-value: the probability of observing an intercept of `r mean_read`, assuming that the true intercept is zero.


**`methodself-test` aka $\beta_1$:**

- Null hypothesis: The difference between the mean score of `self-test` and the mean score of `read` is equal to zero.
- $p$-value: the probability of observing a difference of `r mean_self-mean_read`, assuming that the true difference is zero.


**`methodsummarise` aka $\beta_2$:**

- Null hypothesis: The difference between the mean score of `summarise` and the mean score of `read` is equal to zero.
- $p$-value: the probability of observing a difference of `r mean_summ-mean_read`, assuming that the true difference is zero.


<!-- **Can we reject each of these null hypotheses?** {{< iconify codicon thumbsup-filled size=1em >}} {{< iconify codicon thumbsdown-filled size=1em >}} -->


## What if we want to test different hypotheses? {.smaller}

For example, what if we want to know about how the two non-reference levels `self-test` and `summarise` are different from each other?

<br>

The solution: use our old friend `emmeans`, "Estimated Marginal Means".

- **"Estimated"** $\rightarrow$ based on a model's estimates
- **"Marginal"**  $\rightarrow$ for each level of the predictor
- **"Means"**  $\rightarrow$ means (and CIs) of the estimated outcomes for each level of the predictor



## Intro to testing hypotheses with `emmeans` {.smaller}

1. Get estimated means and 95% CIs for each level of the predictor. (We could plot these like we did last time, if we wanted!)

```{r}
(m2_emm <- emmeans(m2, ~method))
```

<br>

2. Based on those estimates, we can run hypothesis tests on any two levels of the predictor we want.
Or phrased differently: "we can test any contrasts we want".


## Intro to testing our own contrasts {.smaller}

First, we need to know the order of levels in the categorical predictor.

```{r}
levels(score_data$method)
```

<br>

For every contrast you want to test, assign `1` to the level you're interested in and `-1` to the level you want to compare it to, using the level order above.

```{r}
m2_comparisons <- list(
  'self-test vs. read'      = c(-1, 1,  0), # read -1, self-test 1, summarise  0
  'summarise vs. read'      = c(-1, 0,  1), # read -1, self-test 0, summarise  1
  'self-test vs. summarise' = c( 0, 1, -1)  # read  0, self-test 1, summarise -1
)
```

The level coded as –1 will be subtracted from the level coded as 1, and we'll test the significance of that difference.



## Intro to testing our own contrasts {.smaller}

Use `contrast()` to find the $p$-value for each contrast.

```{r}
(m2_emm_contr <- contrast(m2_emm, m2_comparisons))
```

<br>

These `estimate`s look familiar:

:::: {.columns}
::: {.column width="33%"}
```{r}
mean_self - mean_read
```

:::
::: {.column width="33%"}
```{r}
mean_summ - mean_read
```

:::

::: {.column width="33%"}
```{r}
mean_self - mean_summ
```
:::
::::

```{r echo=F, fig.align = 'center'}
p_viol
```


## Intro to testing our own contrasts {.smaller}

Use `contrast()` to find the $p$-value for each contrast.

```{r}
(m2_emm_contr <- contrast(m2_emm, m2_comparisons))
```

<br>

Finally, use `confint()` to get the 95% CIs of each contrast:

```{r}
confint(m2_emm_contr)
```


<br>

**Testing our own contrasts using `emmeans` is not necessary for every analysis.**

But it is useful if the hypotheses we want to test are not the same hypotheses that our *a priori* contrast coding (e.g., our dummy coding) will test.

<br>

::: {.hcenter}
**Questions!**
:::


## Changing the reference level {.smaller}

<br>

First: another prediction activity, individually {{< iconify material-symbols person-rounded size=1.5em >}} and in pairs/threes {{< iconify material-symbols group-outline-rounded size=1.5em >}}.

<br>

:::{.dapr2callout}

Imagine the reference level of `method` was `summarise`, instead of `read`.

- Would you expect **the model coefficients** to be the same or different?
- Would you expect **the hypotheses that the model tests** to be the same or different?
- Would you expect **the estimated marginal means** to be the same or different?

:::

<br>

**{{< iconify material-symbols person-rounded size=1.5em >}} Predict:**
Write down your guesses about each question.

**{{< iconify material-symbols group-outline-rounded size=1.5em >}} Explain:**
Why do you think your guesses are likely to be correct?



## Changing the reference level {.smaller}

TODO: plot dummy variables on xy plane.

show coding with contrasts().


## Changing the reference level {.smaller}

TODO: fit model m3, show that coefs are different (show m2 and m3 side by side).


## Changing the reference level {.smaller}

TODO: compute emmeans of m3, show that estims are same (show emmeans plots m2 and m3 side by side)



## Changing the reference level {.smaller}

<br>

:::{.dapr2callout}

When the reference level of `method` is `summarise`, instead of `read`:

- **The model coefficients** are different.
- **The hypotheses that the model tests** are different.
- **The estimated marginal means** are the same.

:::

<br>

**{{< iconify material-symbols person-rounded size=1.5em >}} Observe:**
Did your guesses match these results?

**{{< iconify material-symbols group-outline-rounded size=1.5em >}} Explain:**
Why are the results the way they are?


<!-- ======================================== -->

## Building an analysis workflow

<br> 

::: {.r-stack}
![](figs/block2-flowchart-06-0.svg){.fragment height="550" }

![](figs/block2-flowchart-06-1.svg){.fragment height="550" }

![](figs/block2-flowchart-06-2.svg){.fragment height="550" }
:::



<!-- ======================================== -->


## Revisiting this week's learning objectives

<!-- ::: {.fragment} -->
::: {.dapr2callout}
**How can we include categorical variables as predictors in a linear model?**

::: {style="font-size: 80%;"}
- Represent the variable numerically, for example using dummy coding (also called treatment coding).
- In dummy coding, the reference level (also called baseline level) is represented ("coded") as 0, and the non-reference level is coded as 1.
- For categorical predictors with >2 levels, dummy coding uses "dummy variables" to individually compare each non-reference level to the same reference level.
:::

:::
<!-- ::: -->

<!-- ::: {.fragment} -->
::: {.dapr2callout}
**When we use a categorical predictor, how do we interpret the linear model's coefficients?**

::: {style="font-size: 80%;"}
- Intercept (also written as $\beta_0$): The mean outcome for the reference level.
- Slope (also written as $\beta_1$, $\beta_2$, etc., or for short, $\beta_j$): The difference between (1) the mean outcome for the non-reference level and (2) the mean outcome for the reference level (when all other predictors are at zero).
:::
:::
<!-- ::: -->

## Revisiting this week's learning objectives

<!-- ::: {.fragment} -->
::: {.dapr2callout}
**What hypotheses are tested by the default way that R represents categorical predictors?**

::: {style="font-size: 80%;"}
- By default, R uses dummy coding/treatment coding. And by default, the reference level is the level that comes first in the alphabet.
- The intercept's hypothesis test: The mean outcome for the reference level is different from zero.
- The slope's hypothesis test: The difference between the (1) mean outcome for the non-reference level and (2) the mean outcome for the reference level is different from zero.
:::
:::
<!-- ::: -->

<!-- ::: {.fragment} -->
::: {.dapr2callout}
**If we wanted to test different hypotheses after fitting the model, how might we do that?**

::: {style="font-size: 80%;"}
- We can use a linear model to estimate what we'd expect our outcome variable to be for every value of our predictor(s).
- These expected outcome values are called "expected marginal means".
- By comparing expected marginal means after fitting the model, we can test basically any hypotheses we want.

:::
:::
<!-- ::: -->

<!-- ======================================== -->

## This week 

<br>

:::: {.columns}
::: {.column width="50%"}

### Tasks

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/labs.svg')
```

**Attend your lab and work together on the exercises** 

:::
::: {.column width="50%"}

### Support

<br>

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/forum.svg')
```

**Help each other on the Piazza forum**

:::
::::

<br>

:::: {.columns}
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/exam.svg')
```

**Complete the weekly quiz**

:::
::: {.column width="50%"}

```{r, echo = F, out.width='15%'}
knitr::include_graphics('figs/oh.png')
```

**Attend office hours (see Learn page for details)**

:::
::::


# Appendix

## Predictive equations

TODO: maths


## Why can't we just compare each group individually to the overall mean? Why do we need to bother with a reference level?

TODO: overparameterised.





<!-- :::: {.columns} -->
<!-- ::: {.column width="50%"} -->
<!-- a -->
<!-- ::: -->
<!-- ::: {.column width="50%"} -->
<!-- b -->
<!-- ::: -->
<!-- :::: -->



<!-- style="font-size: 70%;" -->