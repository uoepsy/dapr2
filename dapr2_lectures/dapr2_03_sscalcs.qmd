---
title: "Calculating Sums of Squares & R-Squared for the Linear Model"
author: "Emma Waterston"
format: html
---


```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(simglm)
library(kableExtra)

set.seed(7284) 

sim_arguments <- list(
  formula = y ~ 1 + hours + motivation,
  fixed = list(hours = list(var_type = 'ordinal', levels = 0:15),
               motivation = list(var_type = 'continuous', mean = 0, sd = 1)),
  error = list(variance = 20),
  sample_size = 150,
  reg_weights = c(0.6, 1.4, 1.5)
)

df <- simulate_fixed(data = NULL, sim_arguments) %>%
  simulate_error(sim_arguments) %>%
  generate_response(sim_arguments)

test_study2 <- df %>%
  dplyr::select(y, hours, motivation) %>%
  mutate(
    ID = paste("ID", 101:250, sep = ""),
    score = round(y+abs(min(y))),
    motivation = round(motivation, 2)
  ) %>%
  dplyr::select(ID, score, hours, motivation)

```

In this document we are going to work through the hand calculations for:
  
- $SS_\text{total}$  
- $SS_\text{residual}$  
- $SS_\text{model}$  
- $R^2$  
- $\hat R^2$  

## Packages

For this example we will only need the `tidyverse` and `kableExtra` packages.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)
```

## Data

```{r}
test_study2 <- read_csv("https://uoepsy.github.io/data/test_study2.csv")
```


We will make our calculations for the model we have been using in class this week:

```{r}
performance <- lm(score ~ hours + motivation, data = test_study2)
summary(performance)
```
 
# Sums of Squares Total

$$
SS_{Total} = \sum_{i=1}^{n}(y_i - \bar{y})^2
$$

Applying the formula involves:

- taking the observed value of `score` for each individual ($y_i$)
- taking the mean of `score` ($\bar{y}$), which is the same for everyone, and subtracting it from each individual value of `score`($y_i$) (i.e., $y_i$ - $\bar{y}$)
- squaring each of the obtained $y_i - \bar{y}$ values
- and summing them together

We will build an object called `ss_tab` (to stand for "sums of squares table"). Here we will add our calculations to the original data.

```{r}
ss_tab <- test_study2 |>
    mutate(
        y_dev = score - mean(score), #subtract the mean from each observed value of score
        y_dev2 = round(y_dev^2, 2) # square these values, and round to 2dp
    )
```

In the code above, `y_dev` = $(y_i - \bar{y})$ , and `y_dev2` squares these values. Each individual has these values calculated. This we can see below where we print the top 10 rows of `ss_tab`.

```{r, echo=FALSE}
ss_tab |>
  slice(1:10) |>
  kable(digits = 2) |>
  kable_styling(full_width = F)
```


We can then calculate the sum of `y_dev2` to give us our sums of squares total: 

```{r}
ss_tot <- sum(ss_tab$y_dev2)
ss_tot
```


# Sums of Squares Residual

$$
SS_{Residual} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

Applying the formula involves:

- taking the observed values of `score` for each individual ( $y_i$ )
- subtracting from each $y_i$ the model-predicted value of `score` for that individual ( $\hat{y}_i$ )
- squaring each of the obtained $y_i - \hat{y}_i$ values
- and summing them

```{r}
ss_tab <- ss_tab |>
  mutate(
    y_pred = round(performance$fitted.values,2), # extract the predicted values from the model
    pred_dev = round((score - y_pred),2), # subtract these from the observed values of score
    pred_dev2 = round(pred_dev^2,2) # square these values, and round to 2dp
  )
```

```{r, echo=FALSE}
ss_tab |>
  slice(1:10) |>
  dplyr::select(-c(4,5)) |>
  kable(digits = 2) |>
  kable_styling(full_width = F)
```

As we did previously, we can then sum this final column to give the sums of squares residual:

```{r}
ss_resid <- sum(ss_tab$pred_dev2)
ss_resid
```

# Sums of Squares Model

$$
SS_{Model} = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2
$$

Applying the formula involves:

- taking the predicted value of `score` for each individual from our model ( $\hat{y}_i$ )
- subtracting the mean of `score` ( $\bar{y}$ ) from each of the $\hat{y}_i$ values 
- squaring each of the obtained $\hat{y}_i - \bar{y}$ values
- and summing them

However, there is a much more efficient way to do this which is simply:

$$
SS_{Model} = SS_{Total} - SS_{Residual}
$$



```{r}
ss_mod = ss_tot - ss_resid
ss_mod
```

# Calulate $R^2$

$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$


```{r}
ss_mod/ss_tot
```

or

```{r}
1 - (ss_resid/ss_tot)
```

We can compare this to the value at the bottom of our model summary:

```{r}
summary(performance)
```

If we wanted to save this specific value from the summary output, we can do the following:

```{r}
sum_performance <- summary(performance)
sum_performance$r.squared
```


# Calulate $\hat R^2$

$$
\hat R^2 = 1 - (1 - R^2)\frac{N-1}{N-k-1}
$$

To calculate adjusted $R^2$ (denoted as $\hat R^2$), we need to know:

+ $R^2$
+ $N$ (sample size)
+ $k$ (number of predictors)

Once we have obtained these values, we can substitute into the formula:

$$
\begin{align}
\hat R^2 = 1 - (1 - R^2)\frac{N-1}{N-k-1} \\
\\
\hat R^2 = 1 - (1 - 0.6696)\frac{150-1}{150-2-1} \\
\\
\hat R^2 = 1 - (0.3304)\frac{149}{147} \\
\\
\hat R^2 = 1 - (0.3304 \cdot 1.013605) \\
\\ 
\hat R^2 = 0.6651049
\end{align}
$$

We can compare this to the value at the bottom of our model summary:

```{r}
summary(performance)
```

If we wanted to save this specific value from the summary output, we can do the following:

```{r}
sum_performance <- summary(performance)
sum_performance$adj.r.squared
```


