---
title: "Interactions I: Num x Cat & Num x Num"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---

```{r setup, include=FALSE}
source('assets/setup.R')
library(tidyverse)
set.seed(953)
```

:::lo
### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand the concept of an interaction.
2. Be able to interpret the meaning of a numeric $\times$ categorical interaction. 
3. Be able to interpret the meaning of a numeric $\times$ numeric interaction.
4. Understand the principle of marginality and why this impacts modelling choices with interactions.
5. Visualize and probe interactions.

### <i class="fa fa-check-square-o fa-2"></i> What You Need

You will need to have completed [Lab X]. 

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych** 
* **sjPlot**
* **patchwork** 
* **kableExtra**
* **plotly**
* **pander** 

### <i class="fa fa-file"></i> Lab Data
You can download the data required for Part A of this lab [here](https://uoepsy.github.io/data/wellbeing_rural.csv) or read it in via this link https://uoepsy.github.io/data/wellbeing_rural.csv. 

You can download the data required for Part B of this lab [here](https://uoepsy.github.io/data/scs_study.csv) or read it in via this link https://uoepsy.github.io/data/scs_study.csv. 

:::

# Setup

`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read both datasets (i.e, for Part A and Part B) into R, assigning Part A 'wellbeing_rural' data it to an object named `wrdata`, and Part B 'scs_study' to an object named `scs_study`. 
 
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
#Loading the required package(s)
library(tidyverse)
library(psych)
library(sjPlot)
library(patchwork)
library(kableExtra) 
library(plotly)
library(pander)

#Reading in data for Part A and storing in object named 'wrdata'
wrdata <- read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv")

#Reading in data for Part B and storing in object named 'scs_study'
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
```
`r solend()`

# Part A

> **Research Question** 
>
> Does the association between number of social interactions and mental wellbeing differ between rural and non-rural residents?  

Researchers were specifically interested in how the number of social interactions might influence mental health and wellbeing differently for those living in rural communities compared to those in cities and suburbs. They want to assess whether the effect of social interactions on wellbeing _is moderated by_ (depends upon) whether or not a person lives in a rural area. 

To investigate how the association between the number of social interactions and mental wellbeing might be different for those living in rural communities, the researchers conducted a study, where they collected data from 200 randomly selected residents of the Edinburgh & Lothian postcodes. 

`r optbegin("Wellbeing/Rurality data codebook.", olabel=FALSE, toggle=params$TOGGLE)`  

__Description__

From the Edinburgh & Lothians, 100 city/suburb residences and 100 rural residences were chosen at random and contacted to participate in the study. The Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), was used to measure mental health and well-being. 

Participants filled out a questionnaire including items concerning: estimated average number of hours spent outdoors each week, estimated average number of social interactions each week (whether on-line or in-person), whether a daily routine is followed (yes/no). For those respondents who had an activity tracker app or smart watch, they were asked to provide their average weekly number of steps.  
  
The data in `wellbeing.csv` contain seven attributes collected from a random sample of $n=200$ hypothetical residents over Edinburgh & Lothians, and include:  

- `wellbeing`: Warwick-Edinburgh Mental Wellbeing Scale (WEMWBS), a self-report measure of mental health and well-being. The scale is scored by summing responses to each item, with items answered on a 1 to 5 Likert scale. The minimum scale score is 14 and the maximum is 70.  
- `outdoor_time`: Self report estimated number of hours per week spent outdoors  
- `social_int`: Self report estimated number of social interactions per week (both online and in-person)
- `routine`: Binary 1=Yes/0=No response to the question "Do you follow a daily routine throughout the week?"
- `location`: Location of primary residence (City, Suburb, Rural)
- `steps_k`: Average weekly number of steps in thousands (as given by activity tracker if available)
- `age`: Age in years of respondent

__Preview__

The first six rows of the data are:

```{r echo=FALSE}
read_csv('https://uoepsy.github.io/data/wellbeing_rural.csv') %>% 
    head() %>% 
    gt::gt()
```
  
`r optend()`

## Exercises

`r qbegin("A1")`
1. Specify a multiple regression model to address the research question. 
2. Visually explore the associations among the variables included in your analysis.

:::quote
"Except in special circumstances, a model including a product term for interaction between two explanatory variables should also include terms with each of the explanatory variables individually, even though their coefficients may not be significantly different from zero. Following this rule avoids the logical inconsistency of saying that the effect of $X_1$ depends on the level of $X_2$ but that there is no effect of $X_1$."  
--- @Ramsey2012
:::

+ **Hint 1:** Check the "location" variable. It currently has three levels (Rural/Suburb/City), but we only want two (Rural/Not Rural). You'll need to fix this. One way to do this would be to use `ifelse()` to define a variable which takes one value ("Rural") if the observation meets from some condition, or another value ("Not Rural") if it does not. Type `?ifelse` in the *console* if you want to see the help function. You can use it to add a new variable either inside `mutate()`, or using `data$new_variable_name <- ifelse(test, x, y)` syntax.
+ **Hint 2:**  The `pairs.panels()` function from the `psych` package will plot all variables in a dataset against one another. This will save you the time you would have spent creating individual plots.  

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
To address the research question, we are going to fit the following model, where $y$ = wellbeing; $x_1$ = weekly outdoor time; and $x_2$ = whether or not the respondent lives in a rural location or not. 

$$
y = \beta_0 + \beta_1  x_1 + \beta_2  x_2 + \beta_3 (x_1 \cdot x_2) + \epsilon \\ 
\quad \\ \text{where} \quad \epsilon \sim N(0, \sigma) \quad \text{independently}
$$
First let's create a new variable for Rural/Not Rural
```{r}
wrdata <- wrdata %>% 
  mutate(
    isRural = ifelse(location == "rural", "rural", "not rural")
  )
```

Now let's use the `pairs.panels()` function from the `psych` package. We could use it on the whole dataset, but for now we'll just do it on the variables we're interested in:
```{r}
wrdata %>% 
  select(wellbeing, social_int, isRural) %>%
  pairs.panels()
```
`r solend()`

`r qbegin("A2")`

Produce a visualisation of the association between weekly number of social interactions and well-being, with separate _facets_ for rural vs non-rural respondents. 

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
ggplot(data = wrdata, aes(x = social_int, y = wellbeing))
  geom_point() 
  facet_wrap(~isRural) 
```
`r solend()`

`r qbegin("A3")`
Fit your model using `lm()`, and assign it as an object with the name "rural_mod".   

**Hint:** When fitting a regression model in R with two explanatory variables A and B, and their interaction, these two are equivalent:  

+ y ~ A + B + A:B
+ y ~ A*B  

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
#fit model including interaction between social_int and isRural
rural_mod <- lm(wellbeing ~ 1 + social_int * isRural, data = wrdata)

#check model output
summary(rural_mod)
```
`r solend()`

:::statbox
__Interpreting coefficients for A and B in the presence of an interaction A:B__   

When you include an interaction between $x_1$ and $x_2$ in a regression model, you are estimating the extent to which the effect of $x_1$ on $y$ is different across the values of $x_2$.  

What this means is that the effect of $x_1$ on $y$ *depends on/is conditional upon* the value of $x_2$.  
(and vice versa, the effect of $x_2$ on $y$ is different across the values of $x_1$).   
This means that we can no longer talk about the "effect of $x_1$ _holding $x_2$ constant_". Instead we can talk about a _marginal effect_ of $x_1$ on $y$ at a specific value of $x_2$. 

:::frame
When we fit the model $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \cdot x_2) + \epsilon$ using `lm()`:  

- the parameter estimate $\hat \beta_1$ is the _marginal effect_ of $x_1$ on $y$ where $x_2 = 0$  
- the parameter estimate $\hat \beta_2$ is the _marginal effect_ of $x_2$ on $y$ where $x_1 = 0$  
:::

<div style="margin-left: 15px">
<small>
__N.B.__ Regardless of whether or not there is an interaction term in our model, all parameter estimates in multiple regression are "conditional" in the sense that they are dependent upon the inclusion of other variables in the model. For instance, in $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$ the coefficient $\hat \beta_1$ is conditional upon holding $x_2$ constant. 
</small>
</div>

:::

:::statbox
__Interpreting the interaction term A:B__  

The coefficient for an interaction term can be thought of as providing an _adjustment to the slope._   
  
In the model below, we have a numeric*categorical interaction:
$$
\begin{align}
\text{wellbeing} \ = \ &\beta_0 + \beta_1 \text{social_interactions} + \beta_2 \text{isRural} + \\
&\beta_3 (\text{social_interactions} \cdot \text{isRural}) + \epsilon
\end{align}
$$

The estimate $\hat \beta_3$ is the adjustment to the slope $\hat \beta_1$ to be made for the individuals in the $\text{isRural}=1$ group. 

:::


`r qbegin("A4")`  
Look at the parameter estimates from your model, and write a description of what each one corresponds to on the plot shown in Figure \@ref(fig:plot-annotate-int) (it may help to sketch out the plot yourself and annotate it).  

:::quote
"The best method of communicating findings about the presence of significant interaction may be to present a table of graph of the estimated means at various combinations of the interacting variables."    
--- @Ramsey2012  
:::


```{r plot-annotate-int, echo=FALSE, fig.cap="Multiple regression model: Wellbeing ~ Social Interactions * is Rural<br><small>Note that the dashed lines represent predicted values below the minimum observed number of social interactions, to ensure that zero on the x-axis is visible</small>"}
nd = expand_grid(social_int=0:13,isRural=c("rural","not rural"))
nd = nd %>% mutate(wellbeing = predict(rural_mod, newdata = .))
sjPlot::plot_model(rural_mod, type="int")+
  scale_fill_manual(NULL, values=c(NA,NA))+xlim(0,28)+
  geom_line(inherit.aes=FALSE,data=nd,aes(x=social_int,col=isRural,y=wellbeing), lty="longdash")+
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 1))
```

`r optbegin("Hints.", olabel=FALSE, toggle=params$TOGGLE)`
Here are some options to choose from:

+ The point at which the blue line cuts the y-axis (where social_int = 0)
+ The point at which the red line cuts the y-axis (where social_int = 0)
+ The average vertical distance between the red and blue lines. 
+ The vertical distance from the blue to the red line _at the y-axis_ (where social_int = 0)
+ The vertical distance from the red to the blue line _at the y-axis_ (where social_int = 0)
+ The vertical distance from the blue to the red line _at the center of the plot_
+ The vertical distance from the red to the blue line _at the center of the plot_
+ The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line
+ The slope of the red line
+ How the slope of the line changes when you move from the blue to the red line
+ How the slope of the line changes when you move from the red to the blue line
`r optend()`

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
We can obtain our parameter estimates using various functions such as `summary(rural_mod)`,`coef(rural_mod)`, `coefficients(rural_mod)` etc. 

```{r}
coefficients(rural_mod)
```


+ $\hat \beta_0$ = `(Intercept)` = `r round(coef(rural_mod)[1],2)`: The point at which the blue line cuts the y-axis (where social_int = 0).  
+ $\hat \beta_1$ = `social_int` = `r round(coef(rural_mod)[2],2)`: The slope (vertical increase on the y-axis associated with a 1 unit increase on the x-axis) of the blue line.
+ $\hat \beta_2$ = `isRuralrural` = `r round(coef(rural_mod)[3],2)`: The vertical distance from the blue to the red line _at the y-axis_ (where social_int = 0).  
+ $\hat \beta_3$ = `social_int:isRuralrural` = `r round(coef(rural_mod)[4],2)`: How the slope of the line changes when you move from the blue to the red line. 

`r solend()`

`r qbegin("A5")`
Load the __sjPlot__ package and try using the function `plot_model()`.  
The default behaviour of `plot_model()` is to plot the parameter estimates and their confidence intervals. This is where `type = "est"`. 
Try to create a plot like Figure \@ref(fig:plot-annotate-int), which shows the two lines (**Hint:** what are this weeks' exercises all about? `type = ???`.)
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
```{r}
plot_model(rural_mod, type="int")
```
`r solend()`

# Part B

> **Research Question**   
> Does the effect of social comparison on symptoms of depression, anxiety and stress vary depending on level of neuroticism?

Previous research has identified an association between an individual's perception of their social rank and symptoms of depression, anxiety and stress. We are interested in the individual differences in this association. 

To investigate whether the effect of social comparison on symptoms of depression, anxiety and stress varies depending on level of neuroticism, we will need to fit a multiple regression model with an interaction term. Before we think about fitting our model, it is important that we understand the data available - how many participants? What type (or class) of data will we be working with? What was the measurement scale? How were scales scored? Look at the social comparison study data codebook below. 


`r optbegin("Social Comparison Study data codebook", olabel=FALSE,toggle=params$TOGGLE)`  


__Description__

Data from 656 participants containing information on scores on each trait of a Big 5 personality measure, their perception of their own social rank, and their scores on a measure of depression.  

The data in `scs_study.csv` contain seven attributes collected from a random sample of $n=656$ participants: 

- `zo`: Openness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `zc`: Conscientiousness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `ze`: Extraversion (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `za`: Agreeableness (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `zn`: Neuroticism (Z-scored), measured on the Big-5 Aspects Scale (BFAS)
- `scs`: Social Comparison Scale - An 11-item scale that measures an individualâ€™s perception of their social rank, attractiveness and belonging relative to others. The scale is scored as a sum of the 11 items (each measured on a 5-point scale), with higher scores indicating more favourable perceptions of social rank.
- `dass`: Depression Anxiety and Stress Scale - The DASS-21 includes 21 items, each measured on a 4-point scale. The score is derived from the sum of all 21 items, with higher scores indicating higher a severity of symptoms.  

__Preview__

The first six rows of the data are:

```{r echo=FALSE}
read_csv('https://uoepsy.github.io/data/scs_study.csv') %>% 
    head() %>% 
    gt::gt()
```
  
`r optend()`

## Exercises 
`r qbegin("B1")`
 Produce plots of the relevant distributions and associations involved in the research question. 
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
ggplot(data = scs_study, aes(x=dass)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of DASS-21 Scores", 
       x = "Depression Anxiety and Stress Scale", y = "Probability density")
```

:::int 
The marginal distribution of scores on the Depression, Anxiety and Stress Scale (DASS-21) is unimodal with a mean of approximately `r round(mean(scs_study$dass))` and a standard deviation of `r round(sd(scs_study$dass))`. 
:::


```{r}
ggplot(data = scs_study, aes(x=scs)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of Social Comparison Scale (SCS) scores", 
       x = "Social Comparison Scale Score", y = "Probability density")
```

:::int 
The marginal distribution of score on the Social Comparison Scale (SCS) is unimodal with a mean of approximately `r round(mean(scs_study$scs))` and a standard deviation of `r round(sd(scs_study$scs))`. There look to be a number of outliers at the upper end of the scale. 
:::


```{r}
ggplot(data = scs_study, aes(x=zn)) + 
  geom_density() + 
  geom_boxplot(width = 1/50) +
  labs(title="Marginal distribution of Neuroticism (Z-Scored)", 
       x = "Neuroticism (Z-Scored)", y = "Probability density")
```

:::int 
The marginal distribution of Neuroticism (Z-scored) is positively skewed, with the 25\% of scores falling below `r round(quantile(scs_study$zn, .25),2)`, 75\% of scores falling below `r round(quantile(scs_study$zn, .75),2)`.
:::


```{r, fig.width = 8, fig.height = 4, out.width = '90%'}


p1 <- ggplot(data = scs_study, aes(x=scs, y=dass)) + 
  geom_point()+
  labs(x = "SCS", y = "DASS-21")

p2 <- ggplot(data = scs_study, aes(x=zn, y=dass)) + 
  geom_point()+
  labs(x = "Neuroticism", y = "DASS-21")

p1 | p2

# the kable() function from the kableExtra package can make table outputs print nicely into html.
scs_study %>%
  select(dass, scs, zn) %>%
  cor() %>% 
  kable(digits = 2) %>%
  kable_styling(full_width = FALSE)
```

:::int
There is a weak, negative, linear relationship between scores on the Social Comparison Scale and scores on the Depression Anxiety and Stress Scale for the participants in the sample. Severity of symptoms measured on the DASS-21 tend to decrease, on average, the more favourably participants view their social rank.  
There is a weak, positive, linear relationship between the levels of Neuroticism and scores on the DASS-21. Participants who are more neurotic tend to, on average, display a higher severity of symptoms of depression, anxiety and stress.  
:::

`r solend()`

`r qbegin("B2")`
Run the code below. It takes the dataset, and uses the `cut()` function to add a new variable called "zn_group", which is the "zn" variable split into 4 groups.  

_Remember:_ we have to re-assign this output as the name of the dataset (the `scs_study <- ` bit at the beginning) to make these changes occur in our __environment__ (the top-right window of Rstudio). If we didn't have the first line, then it would simply print the output.  


```{r}
scs_study <-
  scs_study %>%
  mutate(
    zn_group = cut(zn, 4)
  )
```

We can see how it has split the "zn" variable by plotting the two against one another:  
(Note that the levels of the new variable are named according to the cut-points).

```{r}
ggplot(data = scs_study, aes(x = zn_group, y = zn)) + 
  geom_point()
```

Plot the association between scores on the SCS and scores on the DASS-21, _for each group of the variable we just created._  
How does the pattern differ? Does it suggest an interaction?  

**Tip:** Rather than creating four separate plots, you might want to map some feature of the plot to the variable we created in the data, or make use of `facet_wrap()`/`facet_grid()`.  

`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r, fig.width = 8, fig.height = 4, out.width = '90%'}
ggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + 
  geom_point() + 
  facet_grid(~zn_group) +
  theme(legend.position = "none") # remove the legend
```

The associaiton between SCS scores and DASS-21 scores appears to be different between these groups. For those with a relatively high neuroticism score, the association seems stronger, while for those with a low neuroticism score there is almost no discernable association. 
This suggests an interaction - the association of DASS-21 ~ SCS differs across the values of neuroticism! 
`r solend()`

:::frame

Cutting one of the explanatory variables up into groups essentially turns a numeric variable into a categorical one. We did this just to make it easier to visualise how a an association differs across the values of another variable, because we can imagine a separate line for the association between SCS and DASS-21 scores for each of the groups of neuroticism. However, in grouping a numeric variable like this we lose information. Neuroticism is measured on a continuous scale, and we want to capture how the association between SCS and DASS-21 differs across that continuum (rather than cutting it into chunks).   
We could imagine cutting it into more and more chunks (see Figure \@ref(fig:reglinescut)), until what we end up with is a an infinite number of lines - i.e., a three-dimensional plane/surface (recall that in for a multiple regression model with 2 explanatory variables, we can think of the model as having three-dimensions). The inclusion of the interaction term simply results in this surface no longer being necessarily flat. You can see this in Figure \@ref(fig:3dint). 
 
```{r reglinescut, echo=FALSE, fig.cap="Separate regression lines DASS ~ SCS for neuroticism when cut into 4 (left) or 6 (center) or 12 (right) groups", out.width="80%"}
p1 <- ggplot(data = scs_study, aes(x = scs, y = dass, col = zn_group)) + 
  geom_point(alpha=.1) + 
  stat_smooth(method="lm",se=FALSE)+theme(legend.position = "none")

scs_study %>%
  mutate(
    zn_group = cut(zn, 6)
  ) %>% ggplot(data = ., aes(x = scs, y = dass, col = zn_group)) + 
  geom_point(alpha=.1) + 
  stat_smooth(method="lm",se=FALSE)+theme(legend.position = "none") -> p2

scs_study %>%
  mutate(
    zn_group = cut(zn, 12)
  ) %>% ggplot(data = ., aes(x = scs, y = dass, col = zn_group)) + 
  geom_point(alpha=.1) + 
  stat_smooth(method="lm",se=FALSE)+theme(legend.position = "none") -> p3

p1 | p2 | p3
```


```{r include=FALSE, echo=FALSE}
fit<-lm(dass ~ scs*zn, data = scs_study)
steps=50
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
p <- persp(scs,zn,dass, theta = -25,phi=5, col = NA)
```


```{r 3dint, echo=FALSE, fig.cap="3D plot of regression surface with interaction. You can explore the plot in the figure below from different angles by moving it around with your mouse.", fig.align = 'center', out.width = '90%'}

plot_ly(x=scs,y=zn,z=dass, type="surface") %>% layout(
    scene = list(
      xaxis = list(title = "SCS"),
      yaxis = list(title = "Neuroticism"),
      zaxis = list(title = "DASS-21")
    ))
```

:::


`r qbegin("B3")`

Specify the model required in order to answer the research question (e.g., $\text{??} = \beta_0 + \beta_1 \cdot \text{??} + .... + \epsilon$), and then fit your model using `lm()`.
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Model to answer research question:
$$
\text{DASS-21 Score} = \beta_0 + \beta_1 \cdot \text{SCS Score} + \beta_2 \cdot \text{Neuroticism} + \beta_3 \cdot (\text{SCS score} \cdot \text{Neuroticism}) + \epsilon
$$


```{r}
#fit interaction model
dass_mdl <- lm(dass ~ 1 + scs*zn, data = scs_study)

summary(dass_mdl)
```

`r solend()`

:::frame
__Refresher: Z-scores__

When we __standardise__ a variable, we re-express each value as the distance from the mean _in units of standard deviations._ These transformed values are called __z-scores.__  

To transform a given value $x_i$ into a __z-score__ $z_i$, we simply calculate the distance from $x_i$ to the mean, $\bar{x}$, and divide this by the standard deviation, $s$:    
$$
z_i = \frac{x_i - \bar{x}}{s}
$$

A Z-score of a value is the number of standard deviations below/above the mean that the value falls.  

:::

`r qbegin("B4")`
```{r echo=FALSE, render='asis'}
tbl <- summary(dass_mdl)$coefficients %>% 
  signif(digits = 3)
pander(tbl, emphasize.strong.rows = 1:3)
```

Recall that the coefficients `zn` and `scs` from our model now reflect the estimated difference in the outcome associated with an increase of 1 in the explanatory variables, _when the other variable is zero._  

**Think** - what is 0 in each variable? what is an increase of 1? Are these meaningful? Would you suggest recentering either variable?
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`
The neuroticism variable `zn` is Z-scored, which means that 0 is the mean (it is mean-centered), and 1 is a standard deviation.   
  
The Social Comparison Scale variable `scs` is the raw-score. Looking back at the description of the variables, we can work out that the minimum possible score is 11 (if people respond 1 for each of the 11 questions) and the maximum is 55 (if they respond 5 for all questions). Is it meaningful/useful to talk about estimated effects for people who score 0? Not really.  

But we can make it so that zero represents something else, such as the minimum score, or the mean score. For instance, `scs_study$scs - 11` will subtract 11 from the scores, making zero the minimum possible score on the scale.  

`r solend()`

```{r setup for question6, echo = FALSE}
scs_study <-
  scs_study %>%
  mutate(
    scs_mc = scs - mean(scs)
  )
dass_mdl2 <- lm(dass ~ 1 + scs_mc * zn, data = scs_study)

```
`r qbegin("B5")`

Recenter SCS scores to ensure that 0 is a meaningful value. After re-fitting the model using mean-centered SCS scores instead of the original variable, fill in the blanks in the statements below. 

+ For those of average neuroticism and who score average on the SCS, the estimated DASS-21 Score is **???**  
+ For those who who score **???** on the SCS, an increase of **???** in neuroticism is associated with a change of `r round(coef(dass_mdl2)[3],2)` in DASS-21 Scores
+ For those of average neuroticism, an increase of **???** on the SCS is associated with a change of `r round(coef(dass_mdl2)[2],2)` in DASS-21 Scores  
+ For every increase of **???** in neuroticism, the change in DASS-21 associated with an increase of **???** on the SCS is asjusted by **???**
+ For every increase of **???** in SCS, the change in DASS-21 associated with an increase of **???** in neuroticism is asjusted by **???**
  
`r qend()`

`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

We're going to mean-center the scores on the SCS. Think about someone who now scores zero on the `zn` variable (remember this was already z-scored) *and* zero on the mean-centered SCS:

```{r}
scs_study <-
  scs_study %>%
  mutate(
    scs_mc = scs - mean(scs)
  )
```

After re-fitting the model, you should have the following parameter estimates: 
```{r}
dass_mdl2 <- lm(dass ~ 1 + scs_mc * zn, data = scs_study)

# pull out the coefficients from the summary():
coef_sum <- summary(dass_mdl2)$coefficients
coef_sum
```

:::int 
  
+ For those of average neuroticism and who score average on the SCS, the estimated DASS-21 Score is **`r round(coef(dass_mdl2)[1],2)`**  
+ For those who who score **average (mean)** on the SCS, an increase of **1 standard deviation** in neuroticism is associated with a change of `r round(coef(dass_mdl2)[3],2)` in DASS-21 Scores
+ For those of average neuroticism, an increase of **1** on the SCS is associated with a change of `r round(coef(dass_mdl2)[2],2)` in DASS-21 Scores  
+ For every increase of **1 standard deviation** in neuroticism, the change in DASS-21 associated with an increase of **1** on the SCS is asjusted by **`r round(coef(dass_mdl2)[4],2)`**
+ For every increase of **1** in SCS, the change in DASS-21 associated with an increase of **1 standard deviation** in neuroticism is asjusted by **`r round(coef(dass_mdl2)[4],2)`**
  
:::

`r solend()`


# References {.unlisted .unnumbered}

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>