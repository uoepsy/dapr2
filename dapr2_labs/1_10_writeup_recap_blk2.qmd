---
title: "Write Up & Block 2 Recap"
link-citations: TRUE
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source('assets/setup.R')

set.seed(953)

library(DT)
library(pander)
library(sjPlot)
library(tidyverse)
```

:::lo

### <i class="fa fa-graduation-cap"></i> Learning Objectives
At the end of this lab, you will:

1. Understand how to write-up and provide interpretation of a linear model with multiple predictors (including categorical)
2. Understand how to specify dummy and sum-to-zero coding and interpret the model output
3. Understand how to specify contrasts to test specific effects
4. Be able to specify and assess the assumptions underlying a linear model with multiple predictors
5. Be able to assess the effect of influential cases on linear model coefficients and overall model evaluations

### <i class="fa fa-check-square-o fa-2"></i> What You Need

1. Be up to date with lectures
2. Have completed Labs 7 - 10

### <i class="fab fa-r-project"></i> Required R Packages
Remember to load all packages within a code chunk at the start of your RMarkdown file using `library()`. If you do not have a package and need to install, do so within the console using `install.packages(" ")`. For further guidance on installing/updating packages, see Section C [here](https://uoepsy.github.io/files/install-update-r#update-pkgs). 

For this lab, you will need to load the following package(s):

* **tidyverse** 
* **psych**
* **patchwork**
* **sjPlot**
* **kableExtra**
* **emmeans**
* **car**

### <i class="fa fa-file"></i> Lab Data
You can download the data required for this lab [here](https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv) and [here](https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv) or read the datasets in via these links https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv and  https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv

:::

# Section A: Write-Up

In this lab you will be presented with the output from a statistical analysis, and your job will be to write-up and present the results. We're going to use two simulated datasets based on a paper (the same two that you have worked on in lectures this week) concerning academic outcomes, student/class characteristics, and attendance.  

The aim in writing should be that a reader is able to more or less replicate your analyses **without** referring to your R code. This requires detailing all of the steps you took in conducting the analysis.  

The point of using RMarkdown is that you can pull your results **directly** from the code. If your analysis changes, so does your report!  

Make sure that your final report doesn't show any R functions or code. Remember you are interpreting and reporting your results in text, tables, or plots, targeting a generic reader who may use different software or may not know R at all. If you need a reminder on how to hide code, format tables, etc., make sure to review the [rmd bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::{.callout-note}

## Important - Write-Up Examples & Plagiarism

The example write-up sections included below are not **perfect** - they instead should give you a good example of what information you should include within each section, and how to structure this. For example, some information is missing (e.g., description of data checks, interpretation of descriptive statistics), some information could be presented more clearly (e.g., variable names in tables, table/figure titles/captions, and rationales for choices), and writing could be more concise in places (e.g., discussion section is quite long).  

Further, **you must not copy any of the write-up included below for future reports** - if you do, you will be committing plagiarism, and this type of academic misconduct is taken very seriously by the University. You can find out more [here](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct).

:::

## Study Overview 

> **Research Aim**
> 
> Explore the associations among academic outcomes, student/course characteristics (e.g., class time, online access), and attendance. 

> **Research Questions**
> 
> + RQ1: Does conscientiousness, frequency of access to online materials, and year of study in University predict course attendance?
> + RQ2: Is there a difference in attendance between those with early/late classes in comparison to those with midday classes?
> + RQ3: Is class attendance associated with final grades?

`r optbegin('Academics data codebook.', FALSE, show = TRUE, toggle = params$TOGGLE)`

__Description__

The data used for this write-up exercise are simulated, drawing on a meta-analysis that explored the association between student characteristics and grades. The simulated data are loosely based on the findings of this work, and acted to expand upon the methods and results reported in the paper:

Credé, M., Roch, S. G., & Kieszczynka, U. M. (2010). Class attendance in college: A meta-analytic review of the relationship of class attendance with grades and student characteristics. *Review of Educational Research, 80*(2), 272-295. [https://doi.org/10.3102/0034654310362998](https://doi.org/10.3102/0034654310362998)

The current study was split into two parts. In the first, researchers were interested in further exploring possible predictors of attendance in university courses. They collected information from 397 students across all years of study (i.e., UG (Y1 - Y4), MSc, and PhD), and recorded their class attendance across the academic year, their level of Conscientiousness (categorized as Low, Moderate, or High), the frequency of which they accessed online course materials (categorized as Rarely, Sometimes, or Often), and the timing of class (categorized as 9AM, 10AM, 11AM, 12PM 1PM, 2PM, 3PM, 4PM). In the second, researchers collected data from 200 students, and recorded their class attendance across the academic year and their final course grade. 

__Data Dictionary: Part 1__

The data in `DapR2_S1B2_PracticalPart1` contain six attributes collected from a simulated sample of $n=397$ hypothetical individuals, and includes: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
data1 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv")
tibble(
Variable = names(data1),
Description = c("Participant ID number", "Total attendance (in days)", "Conscientiousness (Levels: Low, Moderate, High)", "Time of Class (Levels: 9AM, 10AM, 11AM, 12PM, 1PM, 2PM, 3PM, 4PM)", "Frequency of access to online course materials (Levels: Rarely, Sometimes, Often)", "Year of Study in University (Y1, Y2, Y3, Y4, MsC, PhD)")
) %>% gt::gt()
```
 
 
__Preview: Part 1__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv') %>% head %>% gt::gt()
```


__Data Dictionary: Part 2__

The data in `DapR2_S1B2_PracticalPart2` contain two attributes collected from a simulated sample of $n=200$ hypothetical individuals, and includes: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
data2 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv")
tibble(
Variable = names(data2),
Description = c("Final grade (0-100)", "Total attendance (in days)")
) %>% gt::gt()
```
 

__Preview: Part 2__

The first six rows of the data are:

```{r echo=FALSE, message=FALSE}
read_csv('https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv') %>% head %>% gt::gt()
```

`r optend()`

<div class="divider div-transparent div-dot"></div>

### Setup
`r qbegin("Setup", qlabel = FALSE)`  

1. Create a new RMarkdown file
2. Load the required package(s)
3. Read the DapR2_S1B2_PracticalPart1 and DapR2_S1B2_PracticalPart2 datasets into R, assigning them to objects named `data1` and `data2`

`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r, warning=FALSE, message=FALSE}
#Loading the required package(s)
library(tidyverse)
library(patchwork)
library(sjPlot)
library(kableExtra)
library(psych)
library(emmeans)
library(car)

#Reading in two datasets and store in objects named 'data1' and  'data2'
data1 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv")
data2 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv")

#check first six rows of each dataset
head(data1)
head(data2)
```

`r solend()`

<br>

### Provided Analysis Code

Below you will find the code required to conduct the analysis to address the research questions. This should look similar (in most areas) to what you worked through in lecture.

`r optbegin("Provided Analysis Code", olabel=FALSE,toggle=params$TOGGLE)`  

## Data Management

```{r message=FALSE, warning=FALSE}
# load libraries
library(tidyverse) # for all things!
library(psych) # good for descriptive stats
library(patchwork) # grouping plots together
library(kableExtra) # useful for creating nice tables
library(sjPlot) #regression tables & plots
library(emmeans) #for contrasts
library(car) #for assumptions (crPlots, residualPlots, VIF) and bootstrapping

# read in datasets
data1 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart1.csv")
data2 <- read_csv("https://uoepsy.github.io/data/DapR2_S1B2_PracticalPart2.csv")

```


## Overall & RQ1

```{r}
#######
# Coding of Variables
#######

#check coding
str(data1)
str(data2)

#check for NAs - none in dataset, so no missing values
table(is.na(data1))
table(is.na(data2))


# make variables factors
data1 <- data1 %>%
    mutate(OnlineAccess = as_factor(OnlineAccess),
           Time = as_factor(Time),
           Conscientiousness = as_factor(Conscientiousness),
           Year = as_factor(Year))

#specify reference levels (alternatively use the below tidyverse way like Year - see lecture example code)
data1$OnlineAccess <- relevel(data1$OnlineAccess, "Sometimes")
data1$Conscientiousness <- relevel(data1$Conscientiousness, "Moderate")

#ordering of year variable - make chronological, Y1 as reference group
data1$Year <- data1$Year %>% 
  factor(., levels = c('Y1', 'Y2', 'Y3', 'Y4', 'MSc', 'PhD'))

```

## Part 1 Data

### RQ1 

```{r}
###########
# Descriptive Stats - Data Viz
###########

# Look at the marginal distributions of variables - use histograms for continuous outcomes, and barplots for categorical: 

p1 <- ggplot(data1, aes(Attendance)) + 
    geom_histogram() + 
    labs(x = "Attendance", y = "Frequency")

p2 <- ggplot(data1, aes(Conscientiousness)) + 
    geom_bar() + 
    labs(x = "Conscientiousness Level", y = "Frequency")

p3 <- ggplot(data1, aes(Year)) + 
    geom_bar() + 
    labs(x = "Year of Study", y = "Frequency")

p4 <- ggplot(data1, aes(OnlineAccess)) + 
    geom_bar()  + 
    labs(x = "Frequency of Access to Online Materials", y = "Frequency")

p1 / p2 / p3 / p4

# Look at the bivariate associations (note we are also removing the legend - it does not offer the reader any additional information and takes up space):

p5 <- ggplot(data1, aes(x = Conscientiousness, y = Attendance, fill = Conscientiousness)) + 
    geom_boxplot() + 
    labs(x = "Conscientiousness Level", y = "Attendance") + 
    theme(legend.position = "none")

p6 <- ggplot(data1, aes(x = OnlineAccess, y = Attendance, fill = OnlineAccess)) + 
    geom_boxplot() + 
    labs(x = "Frequency of Access to Online Materials", y = "Attendance") + 
    theme(legend.position = "none")

p7 <- ggplot(data1, aes(x = Year, y = Attendance, fill = Year)) + 
    geom_boxplot() + 
    labs(x = "Year of Study", y = "Attendance") + 
    theme(legend.position = "none")

p5 / p6 / p7
```

```{r}
#######
#Descriptive Stats - Numeric
#######

# check how many observations in each category
table(data1$Conscientiousness)
table(data1$OnlineAccess)
table(data1$Year)

data1 %>%
  group_by(Year, OnlineAccess, Conscientiousness) %>%
  summarise(n = n(), 
            Mean = mean(Attendance), 
            SD = sd(Attendance),
            Minimum = min(Attendance),
            Maximum = max(Attendance)) %>%
    kable(., caption = "Attendance and Academic Year, Frequency of Online Material Access, Conscientiousness Descriptive Statistics", digits = 2) %>%
    kable_styling()   

```

```{r}
#######
# Model Building
#######

#build model
m1 <- lm(Attendance ~ Conscientiousness + OnlineAccess + Year, data = data1)

#check model summary
summary(m1)
```

```{r}
#######
# Check Assumptions of m1
#######

# Linearity: Can be assumed as working with categorical predictors

# Independence of Errors: Using a between-subjects design, so can assume this

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m1, which = 2, main = "Normality Assumption Check for m1")

# Equal Variances
residualPlot(m1, main = "Equal Variances Assumption Check for m1")

#### Overall, assumption checks look fine

```

```{r}
#######
#Table for Results
#######

tab_model(m1,
          pred.labels = c('Intercept', 'Conscientiousness - High', 'Conscientiousness - Low', 'Online Access - Often', 'Online Access - Rarely', 
                              'UG Y2', 'UG Y3', 'UG Y4', 'MSc', 'PhD'),
          title = "RQ1: Regression Table for Attendance Model")
```


### RQ2

```{r}
#######
# Coding of Variables
#######

#ordering of time variable - make chronological
data1$Time <- data1$Time %>% 
  factor(., levels = c('9AM', '10AM', '11AM','12PM', '1PM', '2PM', '3PM', '4PM'))
```

```{r}
#######
#Descriptive Stats
#######

# Numeric
data1 %>%
  group_by(Time) %>%
  summarise(n = n(), 
            Mean = mean(Attendance), 
            SD = sd(Attendance),
            Minimum = min(Attendance),
            Maximum = max(Attendance)) %>%
    kable(., caption = "Attendance & Class Time Descriptive Statistics", digits = 2) %>%
    kable_styling()    

# check how many observations in each category
table(data1$Time)

# Visual
p8 <- ggplot(data1, aes(Time)) + 
    geom_bar()

p9 <- ggplot(data1, aes(x = Time, y = Attendance, fill = Time)) + 
    geom_boxplot()

p8 / p9
```

```{r}
#######
#Model Building
#######

#build model 
m2 <- lm(Attendance ~ Time, data = data1)

#check summary
summary(m2)
```

```{r}
#######
# Check Assumptions of m2
#######

# Linearity: Can be assumed as working with categorical predictors

# Independence of Errors: Using a between-subjects design, so can assume this

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m2, which = 2)

# Equal Variances
residualPlot(m2)

#### Overall, assumption checks look fine

```


```{r}
#######
#Contrast
#######

#Morning/Evening vs Afternoon

#check order
levels(data1$Time)

#table of weights
TimePeriod <- c("Early/Late", "Early/Late", "Midday", "Midday", "Midday", "Midday", "Early/Late", "Early/Late")
Time <- c("9AM", "10AM", "11AM", "12PM", "1PM", "2PM", "3PM", "4PM")
Weight <- c(1/4, 1/4, -1/4, -1/4, -1/4, -1/4, 1/4, 1/4)
weights <- tibble(TimePeriod, Time, Weight)


#get means
time_mean <- emmeans(m2, ~Time)

#look at means
time_mean

#plot means
plot(time_mean)

#specify weights for contrast
time_comp <- list('Early or Late vs Middle of the Day' = c(-1/4,-1/4, 1/4, 1/4, 1/4, 1/4, -1/4, -1/4))

#run contrast analysis
time_comp_test <- contrast(time_mean, method = time_comp)

#examine output
time_comp_test

#obtain confidence intervals
confint(time_comp_test)
```

## Part 2 Data

### RQ3 

```{r}
#######
#Descriptive Stats
#######

data2 %>%
    describe() %>%
    select(2:4, 8:9) %>%
    rename("N" = n, "Mean" = mean, "SD" = sd, "Minimum" = min, "Maximum" = max) %>%    
        kable(., caption = "Final Grades & Attendance Descriptive Statistics", digits = 2) %>%
        kable_styling()  

data2 %>%
    select(Attendance, Marks) %>%
    cor() %>%
    round(digits = 2)

ggplot(data = data2, aes(x = Attendance, y = Marks)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    labs(x = "Attendance (in days)", y = "Final Grade")
```

```{r}
#######
#Model Building
#######

#specify model
m3 <- lm(Marks ~ Attendance, data = data2)

#check summary
summary(m3)
```

```{r}
#######
# Check Assumptions of m3
#######

# Linearity (can also use plot(model, which = 1) in place of below)
ggplot(data2, aes(x = Attendance, y = Marks)) + 
    geom_point() + 
    geom_smooth(method = 'lm', se = F) + 
    geom_smooth(method = 'loess', se = F, colour = 'red') + 
    labs(x = "Attendance", y = "Final Grade", title = "Scatterplot with linear (blue) and loess (red) lines")

# Independence of Errors: Using a between-subjects design, so can assume this

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m3, which = 2)

# Equal Variances
residualPlot(m3)

```


```{r}
#######
# Bootstrap Model
#######

# use 1000 resamples
boot_m3 <- Boot(m3, R = 1000)

#check summary
summary(boot_m3)

#confidence intervals
confint(boot_m3)

```

`r optend()`

### The 3-Act Structure

We need to present our report in three clear sections - think of your sections like the 3 key parts of a play or story - we need to (1) provide some background and scene setting for the reader, (2) present our results in the context of the research question, and (3) present a resolution to our story - relate our findings back to the question we were asked and provide our answer. 

#### Act I: Analysis Strategy

`r qbegin(1)`

Attempt to draft a discussion section based on the above research question and analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Analysis Strategy - What to Include***

Your analysis strategy will contain a number of different elements detailing plans and changes to your plan. Remember, your analysis strategy should **not** contain any results. You may wish to include the following sections:  

-  Very brief data and design description:
     - Give the reader some background on the context of your write-up. For example, you may wish to describe the data source, data collection strategy, study design, number of observational units.
     - Specify the variables of interest in relation to the research question, including their unit of measurement, the allowed range (for Likert scales), and how they are scored. If you have categorical data, you will need to specify the levels and coding of your variables, and what was specified as your reference level and the justification for this choice.

-  Data management:  
     - Describe any data cleaning and/or recoding.
     - Are there any observations that have been excluded based on pre-defined criteria? How/why, and how many? 
     - Describe any transformations performed to aid your interpretation (i.e., mean centering, standardisation, etc.)

-  Model specification:  
     -  Clearly state your hypotheses and specify your chosen significance level.
     -  What type of statistical analysis do you plan to use to answer the research question? (e.g., simple linear regression, multiple linear regression, binary logistic regression, etc.)
     - In some cases, you may wish to include some visualisations and descriptive tables to motivate your model specification. 
     -  Specify the model(s) to be fitted to answer your given research question and analysis structure. Clearly specify the response and explanatory variables included in your model(s). This includes specifying the type of coding scheme applied if using categorical data. 
     - \* Specify the assumption and diagnostic checks that you will conduct. Specify what plots you will use, and how you will evaluate these. 
     
:::frame
*** Note, given time constraints in labs, we have not included any reference to diagnostic checks in this write-up example - you would be expected to include this in your report. You can find more information on diagnostic checks in the [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1).
:::
  
As noted and encouraged throughout the course, one of the main benefits of using RMarkdown is the ability to include inline R code in your document. Try to incorporate this in your write up so you can automatically pull the specified values from your code. If you need a reminder on how to do this, see [Lesson 4 of the Rmd Bootcamp](https://uoepsy.github.io//rmd-bootcamp/).

:::

`r optbegin("Example Write-Up of Analysis Strategy Section", olabel=FALSE, toggle = params$TOGGLE)`

The first dataset contained information on 397 participants, including their attendance records (in days), class times (ranging from 9AM-4PM, starting on the hour), year of study (UG Y1-Y4, MSc, PhD), levels of conscientiousness (categorized as low, moderate or high), and the frequency of which they accessed online course materials (categorized as low, moderate or high). The second dataset contained information on 200 participants, including their attendance records (in days) and their final course grades (ranging from 0 - 100). All participant data was complete (no missing values), with all values within possible ranges. 

The aim of this report was to address three research questions (the first two RQs using dataset 1, and RQ3 using dataset 2):

1. Does conscientiousness, frequency of access of online access, and year of study influence class attendance?
2. Is there a difference in attendance between those with early/late classes in comparison to those with midday classes?
3. Does class attendance influence grades?

To allow visual examination of the bivariate associations between attendance and the other variables of interest, a series of boxplots were used for RQs 1 and 2, and a scatterplot for RQ3. In order to comment on the strength of the linear association between attendance and grades in RQ3, we will also compute the correlation coefficient. 

To address RQ1, the following multiple regression model was used. We applied dummy coding for Conscientiousness (where 'Moderate' was designated as the reference group), OnlineAccess (where 'Sometimes' was designated as the reference group), and Year of study (where 'Y1' was designated as the reference group).  

$$
\begin{align*}
\text{Class Attendance} ~=~ & \beta_0 + \beta_1 \cdot Consc_{High} + \beta_2 \cdot Consc_{Low}  \\     
& + \beta_3 \cdot Online Access_{Often} + \beta_4 \cdot Online Access_{Rarely}   \\       
& + \beta_5 \cdot Year_{2} + \beta_6 \cdot Year_{3} + \beta_7 \cdot Year_{4}  \\  
& + \beta_8 \cdot Year_{MSc} + \beta_9 \cdot Year_{PhD} + \epsilon    
\end{align*}
\quad
$$

$$
\begin{align}
& \text{Where:} \\  
& Consc = \text{Conscientiousness} \\  
& OnlineAccess = \text{Frequency of Access to Online Material} \\  
& Year = \text{Year of Study} \\  
\end{align}
$$

where we tested whether there was a significant association between attendance and conscientiousness, frequency of access to online materials and/or year of study:

$$
H_0: \text{All}~~ \beta_j = 0 ~\text{(for j = 1, 2, 3, 4, 5, 6, 7, 8, 9)}
$$

$$
H_1: \text{At least one}~ \beta_j \neq \text{(for j = 1, 2, 3, 4, 5, 6, 7, 8, 9)}
$$

```{r include=FALSE}
TimePeriod <- c("Early/Late", "Early/Late", "Midday", "Midday", "Midday", "Midday", "Early/Late", "Early/Late")
Time <- c("9AM", "10AM", "11AM", "12PM", "1PM", "2PM", "3PM", "4PM")
Weight <- c(1/4, 1/4, -1/4, -1/4, -1/4, -1/4, 1/4, 1/4)
weights <- tibble(TimePeriod, Time, Weight)
```

To address RQ2, we first specified the following multiple regression model, where the Time variable was dummy coded ('9AM' was set as the reference group):

$$
\begin{align*}
\text{Class Attendance} ~=~ & \beta_0 + \beta_1 \cdot Time_{10AM} + \beta_2 \cdot Time_{11AM} + \beta_3 \cdot Time_{12PM}   \\     
& +  \beta_4 \cdot Time_{1PM} + \beta_5 \cdot Time_{2PM} + \beta_6 \cdot Time_{3PM} \\  
& + \beta_7 \cdot Time_{4PM} + \epsilon    
\end{align*}
\quad
$$

Next, in order to determine whether there was a significant difference in attendance between early/late classes and midday classes, we conducted a contrast analysis using the following weights (see @tbl-rq3-weights). 

```{r echo = FALSE}
#| label: tbl-rq3-weights
#| tbl-cap: Contrast Weights
weights %>% 
    kable(., caption = "Contrast Weights") %>%
    kable_styling(full_width = FALSE)  
```

Here we wanted to formally test the following hypothesis:

$$
\begin{aligned}
\quad H_0 &: \mu_\text{Early/Late Class} = \mu_\text{Midday Class} \\  
\quad H_0 &: \frac{1}{4}(\mu_1+\mu_2+\mu_7+\mu_8) = \frac{1}{4}(\mu_3+\mu_4+\mu_5+\mu_6)  
\\  
\\  
\quad H_1 &: \mu_\text{Early/Late Class} \neq \mu_\text{Midday Class} \\  
\quad H_1 &: \frac{1}{4}(\mu_1+\mu_2+\mu_7+\mu_8) \neq \frac{1}{4}(\mu_3+\mu_4+\mu_5+\mu_6)  
\\   
\end{aligned}
$$

To address RQ3, the following simple linear regression model was used:

$$    
\text{Final Grade} = \beta_0 + \beta_1 \cdot \text{Attendance} + \epsilon     
\quad
$$

where we tested whether there was a significant association between final grade and attendance. Formally, this corresponded to testing whether the attendance coefficient was equal to zero:

$$
H_0: \beta_1 = 0
$$

$$
H_0: \beta_1 \neq 0
$$

For models related to all RQs, as we were using between-subjects datasets, we assumed independence of our error terms. For RQ1 and RQ2 models, we assumed linearity as all predictor variables were categorical. For the RQ3 model, we visually assessed linearity using a scatterplot with loess lines (where the loess line should closely follow the data). For models related to all RQs, equal variances was assessed via partial residual plots (residuals should be evenly spread across the range of fitted values, where the spread should be constant across the range of fitted values), and normality was assessed via a qqplot of the residuals (points should follow along the diagonal line). If assumptions are found to be violated, we will consider either conducting a sensitivity analysis or bootstrapping our model (depending on which assumption(s) are violated). 

Throughout the report, effects were considered statistically significant at $\alpha = .05$. 

`r optend()`

#### Act II: Results

`r qbegin(2)`

Attempt to draft a results section based on your detailed analysis strategy and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Results - What To Include***

The results section should follow from your analysis strategy. This is where you would present the evidence and results that will be used to answer the research questions and can support your conclusions. Make sure that you address all aspects of the approach you outlined in the analysis strategy (including the evaluation of assumptions and diagnostics). 

In this section, it is useful to include tables and plots to clearly present your findings to your reader. It is important, however, to carefully select what is the key information that should be presented. You don't want to overload the reader with unnecessary or duplicate information, and you also want to save space in case there is a page limit. Make use of figures with multiple panels where you can.

As a broad guideline, you want to start with the results of any exploratory data analysis, presenting tables of summary statistics and exploratory plots. You may also want to visualise associations between/among variables and report covariances or correlations. Then, you should move on to the results from your model.

:::

`r optbegin("Example Write-Up of Results Section", olabel=FALSE, toggle = params$TOGGLE)`

Descriptive statistics related to RQ1 are displayed in @tbl-rq1-desctab. 

```{r desctab, echo = FALSE}
#| label: tbl-rq1-desctab
#| tbl-cap: Attendance and Academic Year, Frequency of Online Material Access, Conscientiousness Descriptive Statistics
data1 %>%
  group_by(Year, OnlineAccess, Conscientiousness) %>%
  summarise(n = n(), 
            Mean = mean(Attendance), 
            SD = sd(Attendance),
            Minimum = min(Attendance),
            Maximum = max(Attendance)) %>%
    kable(., caption = "Attendance and Academic Year, Frequency of Online Material Access, Conscientiousness Descriptive Statistics", digits = 2) %>%
    kable_styling()   
```

```{r include=FALSE}
sumM1 <- summary(m1)
FDat <- round(sumM1$fstatistic, 2)
m1Coefs <- round(coef(m1), 2)
tStats <- round(coef(sumM1)[, "t value"],2)
seVals <- round(coef(sumM1)[, "Std. Error"],2)
```

In relation to RQ1, full regression results, including 95\% Confidence Intervals, are shown in @tbl-rq1-results. 

```{r echo = FALSE}
#| label: tbl-rq1-results
#| tbl-cap: RQ1 - Regression Table for Attendance Model
tab_model(m1,
          pred.labels = c('Intercept', 'Conscientiousness - High', 'Conscientiousness - Low', 'Online Access - Often', 'Online Access - Rarely', 
                              'UG Y2', 'UG Y3', 'UG Y4', 'MSc', 'PhD'),
          title = "RQ1: Regression Table for Attendance Model")
```

The model met assumptions of normality (see Appendix A @fig-rq1-assumpt-norm; the QQplot showed some deviation from the diagonal line at the tails, but this was not of concern) and equal variances (there was a constant spread of residuals; see Appendix A @fig-rq1-assumpt-eqvar). 

The overall model was significant, $F(`r FDat[2]`, `r FDat[3]`) = `r FDat[1]`, p < .001$. Conscientiousness, frequency of online access, and year in university explained approximately `r round(sumM1$adj.r.squared*100, 0)`% of the variance in attendance. Both those with high and low levels of conscientiousness exhibited attendance rates that were significantly different than those with moderate levels of conscientiousness (see @fig-rq1(a)), when controlling for year of study and online access. Specifically, those with high levels of conscientiousness attended class significantly more often than those with moderate levels $(\beta = `r m1Coefs['ConscientiousnessHigh']`,~ SE = `r seVals['ConscientiousnessHigh']`,~ t = `r tStats['ConscientiousnessHigh']`,~ p < .001)$. Conversely, in comparison to those with moderate levels of conscientiousness, those with low levels attended class significantly less $(\beta = `r m1Coefs['ConscientiousnessLow']`,~ SE = `r seVals['ConscientiousnessLow']`,~ t = `r tStats['ConscientiousnessLow']`,~ p < .001$). Both those who rarely $(\beta = `r m1Coefs['OnlineAccessRarely']`,~ SE = `r seVals['OnlineAccessRarely']`,~ t = `r tStats['OnlineAccessRarely']`,~ p < .001)$ or often $(\beta = `r m1Coefs['OnlineAccessOften']`,~ SE = `r seVals['OnlineAccessOften']`,~ t = `r tStats['OnlineAccessOften']`,~ p = .009)$ accessed online materials had significantly lower attendance rates than those who accessed only sometimes regardless of Conscientiousness levels and year of study (see @fig-rq1(b)). Holding constant conscientiousness and frequency of online access to materials, all years of study, with the exception of Y3, had significantly higher attendance rates than those in Y1 (see @fig-rq1(c)).  

```{r echo = FALSE}
#| label: fig-rq1
#| fig-cap: "Association between Attendance and (a) Conscientiousness (b) Online Access (c) Year of Study"
p5 / p6 / p7
```

Descriptive statistics related to RQ2 are displayed in @tbl-rq2-desctab. 

```{r echo = FALSE}
#| label: tbl-rq2-desctab
#| tbl-cap: Attendance & Class Time Descriptive Statistics
data1 %>%
  group_by(Time) %>%
  summarise(n = n(), 
            Mean = mean(Attendance), 
            SD = sd(Attendance),
            Minimum = min(Attendance),
            Maximum = max(Attendance)) %>%
    kable(., caption = "Attendance & Class Time Descriptive Statistics", digits = 2) %>%
    kable_styling()   
```

To determine whether there was a difference in attendance between those with early/late classes in comparison to those with midday classes, we performed a test against $H_0: \frac{1}{4}(\mu_1+\mu_2+\mu_7+\mu_8) -\frac{1}{4}(\mu_3+\mu_4+\mu_5+\mu_6) = 0$. The model met the assumptions of normality (there was no extreme deviation from the diagonal line; see Appendix B @fig-rq2-assumpt-norm) and equal variances (there was a constant spread of residuals; see Appendix B @fig-rq2-assumpt-eqvar).

At the 5\% significance level, there was evidence that class attendance did significantly differ between those who had early/late classes and those who had classes in the middle of the day $(t(389) = 3.96,~ p < .001, \text{two-sided})$, where students with midday classes attended, on average, 5.36 $(SE = 1.35)$ more classes (see @fig-rq2-means). We are 95% confident that those who had midday classes attended approximately 2-8 classes more than those who had classes in the morning or late afternoon ($CI_{95}[2.70, 8.01]$). 

```{r echo = FALSE}
#| label: fig-rq2-means
#| fig-cap: "Estimated Attendance Means by Year of Study"
plot(time_mean)
```

In relation to RQ3, there was a strong positive correlation between attendance and marks $(r_{(Attendance,~Grade)} = .91)$. Our fitted model failed to satisfy all regression assumptions (see Appendix C). Although our model met the linearity (though there was a slight curve in the loess line; see Appendix C @fig-rq3-assumpt-lin), and normality of residuals assumptions (there was a slight skew, but not of concern; see Appendix C @fig-rq3-assumpt-norm), the assumption of equal variances was violated as the spread of residuals was not constant and suggested that there was evidence of heteroscedasticity (see Appendix C @fig-rq3-assumpt-eqvar). Therefore, since we could not trust that the SEs in our model results were reliable, we bootstrapped. Results suggested that there was a significant positive association between marks and attendance (see @fig-rq3). Specifically, we are 95% confident that for every day one attends class, their final mark will increase, on average, between `r round(confint(boot_m3)[2], 2)` and `r round(confint(boot_m3)[4], 2)` points. As the 95% CI did not contain zero, we rejected the null hypothesis as there was evidence of a significant association between student’ attendance and their final marks. 

```{r echo = FALSE}
#| label: fig-rq3
#| fig-cap: "Association between Final Grades and Attendance"
ggplot(data = data2, aes(x = Attendance, y = Marks)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    labs(x = "Attendance (in days)", y = "Final Grade")
```

`r optend()`

#### Act III: Discussion  

`r qbegin(3)`

Attempt to draft a discussion section based on your results and the analysis provided.

`r qend()`

:::{.callout-tip appearance="simple" collapse="true"}

### Discussion - What To Include

In the discussion section, you should summarise the key findings from the results section and provide the reader with a few take-home sentences drawing the analysis together and relating it back to the original question. 

The discussion should be relatively brief, and should not include any statistical analysis - instead think of the discussion as a conclusion, providing an answer to the research question(s).

:::

`r optbegin("Example Write-Up of Discussion Section", olabel=FALSE, toggle = params$TOGGLE)`

Previous research had identified strong associations between attendance and both class grades and GPA, as well as associations between class attendance and a variety of student characteristics. In the first part of our analysis, we examined the associations of attendance with (1) conscientiousness, frequency of access to online materials, and year of study; and (2) class time. In the second part of our analysis, we examined the association between final grade and attendance.

Our results suggested that: (1) level of conscientiousness, frequency of access to online materials, and year of study were all significant predictors of attendance. Specifically, higher conscientiousness and more advanced year of study were associated with higher attendance rates. In relation to online materials, it appeared that an over-reliance or lack of use was negatively associated with attendance - those who accessed regularly had the best attendance rates. (2) there were differences in attendance depending upon class time, where students with classes in the middle of the day were more likely to attend than those who had classes either early in the morning or later in the afternoon. (3) attendance was a significant predictor of final marks, where higher attendance was associated with higher grades. Therefore, we rejected the null hypotheses for each of our research questions. 

`r optend()`

#### Assumptions & Diagnostics Appendix

`r qbegin(4)`

Given we want to keep the report as concise as possible, we may wish to utilize the appendix to present assumption and diagnostic plots. We must however ensure that:

+ We have described what assumptions we will check in the analysis strategy, including how we will evaluate them
+ We have summarized the evaluations of our assumptions and diagnostic checks in the results section of the main report
+ We have accurately refereed to the figures and tables labels presented in the appendix in the main report (if we don't refer to them, the reader won't know what they are relevant to!)

`r qend()`

`r optbegin("Example Assumptions & Diagnostics Appendix", olabel=FALSE, toggle = params$TOGGLE)`

##### Appendix A

```{r echo = FALSE}
#| label: fig-rq1-assumpt-norm
#| fig-cap: "Normality Assumption Check for m1"

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m1, which = 2, main = "Normality Assumption Check for m1")
```

```{r echo = FALSE}
#| label: fig-rq1-assumpt-eqvar
#| fig-cap: "Equal Variances Assumption Check for m1"

# Equal Variances
residualPlot(m1, main = "Equal Variances Assumption Check for m1")
```

##### Appendix B

```{r echo = FALSE}
#| label: fig-rq2-assumpt-norm
#| fig-cap: "Normality Assumption Check for m2"

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m2, which = 2, main = "Normality Assumption Check for m2")
```

```{r echo = FALSE}
#| label: fig-rq2-assumpt-eqvar
#| fig-cap: "Equal Variances Assumption Check for m2"

# Equal Variances
residualPlot(m2, main = "Equal Variances Assumption Check for m2")
```

##### Appendix C

```{r echo = FALSE}
#| label: fig-rq3-assumpt-lin
#| fig-cap: "Linearity Assumption Check for m3"

# Linearity (can also use plot(model, which = 1) in place of below)
ggplot(data2, aes(x = Attendance, y = Marks)) + 
    geom_point() + 
    geom_smooth(method = 'lm', se = F) + 
    geom_smooth(method = 'loess', se = F, colour = 'red') + 
    labs(x = "Attendance", y = "Final Grade", title = "Scatterplot with linear (blue) and loess (red) lines for m3")
```

```{r echo = FALSE}
#| label: fig-rq3-assumpt-norm
#| fig-cap: "Normality Assumption Check for m3"

# Normality (either use plot(model, which = 2) or hist(model$residuals))
plot(m3, which = 2, main = "Normality Assumption Check for m3")
```

```{r echo = FALSE}
#| label: fig-rq3-assumpt-eqvar
#| fig-cap: "Equal Variances Assumption Check for m3"
# Equal Variances
residualPlot(m2, main = "Equal Variances Assumption Check for m3")
```

`r optend()`

<br>

# Section B: Block 2 (Weeks 7 - 10) Recap

In the second part of the lab, there is no new content - the purpose of the recap section is for you to revisit and revise the concepts you have learned over the last 4 weeks. 

:::red

Before you expand each of the boxes below, think about how comfortable you feel with each concept.  

:::

`r optbegin("Binary Variables", olabel=FALSE,toggle=params$TOGGLE)`

We can include categorical predictors in a linear regression, but the interpretation of the coefficients is very specific. Whereas we talked about coefficients being interpreted as "the change in $y$ associated with a 1-unit increase in $x$", for categorical explanatory variables, coefficients can be considered to examine differences in group means. However, they are actually doing exactly the same thing - the model is simply translating the levels (like "Yes"/"No") in to 0s and 1s!  

Our coefficients are just the same as before. The intercept is where our predictor equals zero, and the slope is the change in our outcome variable associated with a 1-unit change in our predictor.  

However, "zero" for this predictor variable now corresponds to a whole level. This is known as the "reference level". Accordingly, the 1-unit change in our predictor (the move from "zero" to "one") corresponds to the difference between the two levels. 

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html) and [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Categorical Predictors with k levels", olabel=FALSE,toggle=params$TOGGLE)`

We saw that a _binary categorical_ variable gets inputted into our model as a variable of 0s and 1s (these typically get called __"dummy variables"__).  

:::statbox
__Dummy variables__ are numeric variables that represent categorical data.  
:::

When we have a _categorical_ explanatory variable with __more than 2 levels__, our model gets a bit more - it needs not just one, but _a number of_ dummy variables. For a categorical variable with $k$ levels, we can express it in $k-1$ dummy variables.  

For example, the "species" column below has three levels, and can be expressed by the two variables "species_dog" and "species_parrot":  
```{r echo=FALSE, out.width="80%"}
data.frame(
  species = c("cat","cat","dog","parrot","dog","cat","..."),
  species_dog = c(0,0,1,0,1,0,"..."),
  species_parrot = c(0,0,0,1,0,0,"...")
)
```

+ The "cat" level is expressed whenever both the "species_dog" and "species_parrot" variables are 0.
+ The "dog" level is expressed whenever the "species_dog" variable is 1 and the "species_parrot" variable is 0.
+ The "parrot" level is expressed whenever the "species_dog" variable is 0 and the "species_parrot" variable is 1.  

`R` will do all of this re-expression for us. If we include in our model a categorical explanatory variable with 4 different levels, the model will estimate 3 parameters - one for each dummy variable. We can interpret the parameter estimates (the coefficients we obtain using `coefficients()`,`coef()` or `summary()`) as the estimated increase in the outcome variable associated with an increase of one in each dummy variable (holding all other variables equal).  

```{r echo=FALSE}
set.seed(348)
catplot <- tibble(
  species = rep(c("cat","dog","parrot"), each = 15),
  outcome = c(rnorm(15,60,5), rnorm(15,50,5), rnorm(15,55,5))
)
cstat = coef(lm(outcome~species,catplot))
pander::pander(summary(lm(outcome~species,catplot))$coefficients)
```

Note that in the above example, an increase in 1 of "species_dog" is the difference between a "cat" and a "dog". An increase in one of "species_parrot" is the difference between a "cat" and a "parrot". We think of the "cat" category in this example as the _reference level_ - it is the category against which other categories are compared against. 

```{r echo=FALSE, message=FALSE, warning = FALSE}
ggplot(catplot, aes(x=species, y=outcome))+
  #geom_boxplot(fatten=NULL)+
  geom_jitter(height=0,width=.05, alpha=.4)+
  geom_point(x=1,y=cstat[1], col="blue",size=3)+
  annotate("text",x=1,y=cstat[1],label=expression(paste(beta[0], " (intercept)")), col="blue", hjust=1.1)+
  geom_segment(aes(x=1,xend=2,y=cstat[1],yend=cstat[1]+cstat[2]),col="blue")+
  geom_segment(aes(x=1,xend=2,y=cstat[1],yend=cstat[1]),col="blue", lty="dashed")+
  geom_segment(aes(x=2,xend=2,y=cstat[1],yend=cstat[1]+cstat[2]),col="blue", lty="dashed")+
  annotate("text",x=2.15,y=mean(c(cstat[1],sum(cstat[1:2]))),label=expression(paste(beta[1], " (slope)")), col="blue", hjust=.35)+
  
  geom_segment(aes(x=1,xend=3,y=cstat[1],yend=cstat[1]+cstat[3]),col="blue")+
  geom_segment(aes(x=1,xend=3,y=cstat[1],yend=cstat[1]),col="blue", lty="dashed")+
  geom_segment(aes(x=3,xend=3,y=cstat[1],yend=cstat[1]+cstat[3]),col="blue", lty="dashed")+
  annotate("text",x=3.15,y=mean(c(cstat[1],sum(cstat[c(1,3)]))),label=expression(paste(beta[2], " (slope)")), col="blue", hjust=.35)
```

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html) and [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Side Contraints", olabel=FALSE,toggle=params$TOGGLE)`

Possible side-constraints on the parameters are:

|       Name      |             Constraint            |             Meaning of $\beta_0$            |         R         |
|:---------------:|:---------------------------------:|:-------------------------------------------:|:-----------------:|
| Sum to zero (Effects Coding) | $\beta_1 + \beta_2 + \beta_3 = 0$ | $\beta_0 = \mu$   |    `contr.sum`    |
| Reference group (Dummy Coding) |           $\beta_1 = 0$           | $\beta_0 = \mu_1$ | `contr.treatment` |


:::blue

**IMPORTANT**

- By default `R` uses the reference group constraint. If your factor has $g$ levels, your regression model will have $g-1$ dummy variables (`R` creates them for you)

- We can switch back to the default reference group constraint by applying either of these:

```{r eval = FALSE}
# Option 1
contrasts(rest_spend$music) <- NULL
# Option 2
contrasts(rest_spend$music) <- "contr.treatment"
```
:::

:::{.callout-note}

See [S1 Week 7 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_06_dummy.html), [S1 Week 8 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_07_effects.html), [S1 Week 7 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_06_LMcategorical1.html#1), [S1 Week 8 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_07_lmcategorical2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Contrasts: Rules for Assigning Weights", olabel=FALSE,toggle=params$TOGGLE)`  

+ **Rule 1**: Weights are -1 $\geq$ x $\leq$ 1
+ **Rule 2**: The group(s) in one chunk are given negative weights, the group(s) in the other get positive weights
+ **Rule 3**: The sum of the weights of the comparison must be 0
+ **Rule 4**: If a group is not involved in the comparison, weight is 0
+ **Rule 5**: For a given comparison, weights assigned to group(s) are equal to 1 divided by the number of groups in that chunk.
+ **Rule 6**: Restrict yourself to running $k$ - 1 comparisons (where $k$ = number of groups)
+ **Rule 7**: Each contrast can only compare 2 chunks of variance
+ **Rule 8**: Once a group singled out, it can not enter other contrasts 

:::{.callout-note}

See [S1 Week 8 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_07_effects.html) and [S1 Week 8 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_07_lmcategorical2.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Linearity", olabel=FALSE,toggle=params$TOGGLE)`

### Simple Linear Regression
In simple linear regression (SLR) with only one explanatory variable, we could assess linearity through a simple scatterplot of the outcome variable against the explanatory. This would allow us to check if the errors have a mean of zero. If this assumption was met, the residuals would appear to be randomly scattered around zero.  
The rationale for this is that, once you remove from the data the linear trend, what's left over in the residuals should not have any trend, i.e. have a mean of zero.

### Multiple Regression
In multiple regression, however, it becomes more necessary to rely on diagnostic plots of the model residuals. This is because we need to know whether the relations are linear between the outcome and each predictor _after accounting for the other predictors in the model._  

In order to assess this, we use **partial-residual plots** (also known as 'component-residual plots'). This is a plot with each explanatory variable $x_j$ on the x-axis, and **partial residuals** on the y-axis.

Partial residuals for a predictor $x_j$ are calculated as:
$$
\hat \epsilon + \hat \beta_j x_j
$$

:::blue

In **R**, we can easily create these plots for all predictors in the model by using the `crPlots()` function from the `car` package.  

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Equal Variances (Homoscedasticity)", olabel=FALSE,toggle=params$TOGGLE)`

The equal variances assumption is that the error variance $\sigma^2$ is constant across values of the predictor(s) $x_1, \dots,  x_k$, and across values of the fitted values $\hat y$. This sometimes gets termed "Constant" vs "Non-constant" variance. This is presented visually in @fig-ncv-violate and @fig-ncv-noviolate. 

```{r ncv1, echo=FALSE}
library(patchwork)
n=1000
x <- runif(n, min = 0, max = 100)
y.increasing <- 3 + 0.2 * x + (1 + x / 25) * rnorm(n, sd = 3)
y.good <- 3 + 0.1 * x + rnorm(n, sd = 3)


lm.good <- lm(y.good ~ x)
lm.bad <-lm(y.increasing~x)

p1<-ggplot(NULL, aes(x=fitted(lm.bad), y=resid(lm.bad)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p2<-ggplot(NULL, aes(x=fitted(lm.good), y=resid(lm.good)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p3<-ggplot(NULL, aes(x=fitted(lm.bad)<mean(fitted(lm.bad)), y=resid(lm.bad)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())

p4<-ggplot(NULL, aes(x=fitted(lm.good)<mean(fitted(lm.good)), y=resid(lm.good)))+
  geom_point(shape=1)+
  labs(x="fitted",y="residuals")+
  theme(axis.text = element_blank())
```

```{r echo = FALSE, fig.width = 8, out.width = '90%'}
#| label: fig-ncv-violate
#| fig-cap: "Non-constant variance for numeric and categorical X"
(p1 | p3) + plot_annotation(title = "Non-constant variance")
```


```{r echo = FALSE, fig.width = 8, out.width = '90%'}
#| label: fig-ncv-noviolate
#| fig-cap: "Constant variance for numeric and categorical X"
(p2 | p4) + plot_annotation(title = "Constant variance")
```

:::blue

In **R**, we can create plots of the _Pearson residuals_ against the predicted values $\hat y$ and against the predictors $x_1$, ... $x_k$ by using the `residualPlots()` function from the `car` package. This function also provides the results of a lack-of-fit test for each of these relationships (note when it is the fitted values $\hat y$ it gets called "Tukey's test").  

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Independence (of errors)", olabel=FALSE,toggle=params$TOGGLE)`

The 'independence of errors' assumption is the condition that the errors do not have some underlying relationship which is causing them to influence one another. 
<br>

There are many sources of possible dependence, and often these are issues of study design. For example, we may have groups of observations in our data which we would expect to be related (e.g., multiple trials from the same participant). Our modelling strategy would need to take this into account.
<br>

One form of dependence is **autocorrelation** - this is when observations influence those adjacent to them. It is common in data for which *time* is a variable of interest (e.g, the humidity today is dependent upon the rainfall yesterday). 

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Assumptions: Normality (of errors)", olabel=FALSE,toggle=params$TOGGLE)`

The normality assumption is the condition that the errors $\epsilon$ are normally distributed in the population.  

We can visually assess this condition through histograms, density plots, and quantile-quantile plots (QQplots) of our residuals $\hat \epsilon$.   

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Useful Assumption Plots via plot(model)", olabel=FALSE,toggle=params$TOGGLE)`

```{r}
#| echo: false
mydata <- read_csv("https://uoepsy.github.io/data/wellbeing.csv")
mymodel <- lm(wellbeing ~ outdoor_time + social_int, data = mydata) 
```

We can run `plot(mymodel)` which will cycle through these plots (asking us to press enter each time to move to the next plot), or we can arrange these plots in a matrix via `par(mfrow)`, for example in a 2 x 2 matrix as shown below (make sure to always reset your graphical parameters! If needed, we could also extract specific plots using, for instance: `plot(mymodel, which = 3)` for the third plot.

```{r}
par(mfrow=c(2,2))
plot(mymodel)
par(mfrow=c(1,1))
```

- Top Left: For the __Residuals vs Fitted__ plot, we want the red line to be horizontal at close to zero across the plot. We don't want the residuals (the points) to be fanning in/out.  
- Top Right: For the __Normal Q-Q__ plot, we want the residuals (the points) to follow closely to the diagonal line, indicating that they are relatively normally distributed.^[QQplots plot the values against the associated percentiles of the normal distribution. So if we had ten values, it would order them lowest to highest, then plot them on the y against the 10th, 20th, 30th.. and so on percentiles of the standard normal distribution (mean 0, SD 1)]
- Bottom Left: For the __Scale-Location__ plot, we want the red line to be horizontal across the plot. These plots allow us to examine the extent to which the variance of the residuals changes across the fitted values. If it is angled, we are likely to see fanning in/out of the points in the residuals vs fitted plot.
- Bottom Right: The __Residuals vs Leverage__ plot indicates points that might be of individual interest as they may be unduly influencing the model. There are funnel-shaped lines on this plot (sometimes out of scope of the plotting window). Ideally, we want our residuals inside the funnel - the further the residual is to the right (the more leverage it has), the closer to the 0 we want it to be.  

_(Note, if we have only categorical predictors in our model, many of these will show vertical lines of points. This doesn't indicate that anything is wrong, and the same principles described above continue to apply)_

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Multicollinearity", olabel=FALSE,toggle=params$TOGGLE)`

For the linear model with **multiple** explanatory variables, we need to also think about **multicollinearity** - this is when two (or more) of the predictors in our regression model are moderately or highly correlated.  

We can assess multicollinearity using the **variance inflation factor (VIF)**, which for a given predictor $x_j$ is calculated as:  

$$
VIF_j = \frac{1}{1-R_j^2} \\
$$

Suggested cut-offs for VIF are varied. Some suggest 10, others 5. Define what you will consider an acceptable value _prior_ to calculating it. You could loosely interpret VIF values >5 as moderate multicollinearity and values >10 as severe multicollinearity.    

:::blue

In **R**, the `vif()` function from the `car` package will provide VIF values for each predictor in your model. 

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Individual Case Diagnostics", olabel=FALSE,toggle=params$TOGGLE)`

We have seen in the case of the simple linear regression that individual cases in our data can influence our model more than others. We know about:

+ **Regression outliers:** A large residual $\hat \epsilon_i$ - i.e., a big discrepancy between their predicted y-value and their observed y-value.  
    + **Standardised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals. In R, the `rstandard()` function will give you these
    + **Studentised residuals:** For residual $\hat \epsilon_i$, divide by the estimate of the standard deviation of the residuals excluding case $i$. In R, the `rstudent()` function will give you these.
+ **High leverage cases:** These are cases which have considerable _potential_ to influence the regression model (e.g., cases with an unusual combination of predictor values). 
    + **Hat values:** are used to assess leverage. In R, The `hatvalues()` function will retrieve these. 
+ **High influence cases:** When a case has high leverage *and* is an outlier, it will have a large influence on the regression model. 
    + **Cook's Distance:** combines *leverage* (hatvalues) with *outlying-ness* to capture influence: $D_i = \text{Outlyingness} \times \text{Leverage}$. Cook's distance refers to the average distance the $\hat{y}$ values will move if a given case is removed. In `R`, the `cooks.distance()` function will provide these values. 
Alongside Cook's Distance, we can examine the extent to which model estimates and predictions are affected when an entire case is dropped from the dataset and the model is refitted.  
+ **DFFit:** the change in the predicted value at the $i^{th}$ observation with and without the $i^{th}$ observation is included in the regression.  
+ **DFbeta:**  the change in a specific coefficient with and without the $i^{th}$ observation is included in the regression.  
+ **DFbetas:**  the change in a specific coefficient divided by the standard error, with and without the $i^{th}$ observation is included in the regression.  
+ **COVRATIO:** measures the effect of an observation on the covariance matrix of the parameter estimates. In simpler terms, it captures an observation's influence on standard errors.

:::blue

In **R**, we can get lots of these measures with the `influence.measures()` function:


+ `influence.measures(my_model)` will give you out a dataframe of the various measures.
+ `summary(influence.measures(my_model))` will provide a nice summary of what R deems to be the influential points.

:::

:::{.callout-note}

See [S1 Week 9 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_08_assump_diag.html) and [S1 Week 9 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_08_assumptions_diagnostics.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap: Terminology", olabel=FALSE,toggle=params$TOGGLE)`

- A _parameter_ is a numerical summary for the population, e.g. the population slope $\beta_1$.
- A _statistic_ is a numerical summary calculated from the sample data, e.g. the estimated slope in the sample $\widehat \beta_1$. We use the sample statistic as a best guess, or estimate, for the unknown population parameter.
- A _bootstrap sample_ is chosen with replacement from an existing sample, using the same sample size.
- A _bootstrap statistic_ is a statistic computed for each bootstrap sample.
- A _bootstrap distribution_ collects bootstrap statistics for many bootstrap samples.

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_09_BootstrapLM.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap", olabel=FALSE,toggle=params$TOGGLE)`

The _bootstrap_ is a general approach to assessing whether the sample results are statistically significant or not, and allows us to draw inferences to the population from a regression model. This method is assumption-free and does not rely on conditions such as normality of the residuals.

It is based on sampling repeatedly with replacement (to avoid always getting the original sample exactly) from the data at hand, and then computing the regression coefficients from each re-sample. We will equivalently use the word "bootstrap sample" or "resample" (for **sample** with **re**placement).

:::frame

The basic principle is:

<center>
__The population is to the original sample__

__as__

__the original sample is to the bootstrap samples.__

</center>

:::

Because we only have one sample of size $n$, and we do not have access to the data for the entire population, we consider our original sample as our best approximation to the population. 

To be more precise, we assume that the population is made up of many, many copies of our original sample. Then, we take multiple samples each of size $n$ from this assumed population. This is equivalent to sampling _with replacement_ from the original sample.

```{r echo=FALSE, out.width = '90%'}
knitr::include_graphics('images/reg-boot.png')
```

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_09_BootstrapLM.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`

`r optbegin("Bootstrap: In R", olabel=FALSE,toggle=params$TOGGLE)`

Follow these steps:

* 1: Load the `car` library
* 2: Use the `Boot()` function (do not forget the uppercase B!) which takes as arguments:
    - the fitted model
    - `f`, saying which bootstrap statistics to compute on each bootstrap sample. By default `f = coef`, returning the regression coefficients.
    - `R`, saying how many bootstrap samples to compute. By default `R = 999` but this could be any number. To experiment we recommend 1000, when you want to produce results for journals, it is typical to go with 10,000 or more.
    - `ncores`, saying if to perform the calculations in parallel (and more efficiently). However, this will depend on your PC, and you need to find how many cores you have by running `parallel::detectCores()` on your PC. By default the function uses `ncores = 1`.
* 3: Run the code. However, please remember that the `Boot()` function does **not** want a model which was fitted using data with `NAs`. To remove, for example, you could use `na.omit`.
* 4: Look at the `summary()` of the bootstrap results. When doing so the output will show, for each regression coefficient, the value in the original sample in the column `original`, and in the `bootSE` column, the estimate of the variability of the coefficient from bootstrap sample to bootstrap sample. The `bootSE` provides us the bootstrap standard error, or bootstrap SE in short. We can use this to answer the key question of how accurate our estimate is.
* 5: Compute confidence intervals. Use your preferred confidence level (usually, and by default, 95%)
* 6: Provide interpretation in the context of your research question and report results in APA format. 

:::blue

In **R**, for example:
```{r eval = FALSE}
#specify model
mymodel <- lm(y ~ x1 + x2, data = mydata)

#load car package
library(car)

#bootstrap model
bootmymodel <- Boot(mymodel, R = 1000)

#confidence intervals
Confint(bootmymodel, level = 0.95, type = "perc")
```

:::

:::{.callout-note}

See [S1 Week 10 Lab](https://uoepsy.github.io/dapr2/2324/labs/1_09_bootstrap.html) and [S1 Week 10 Lectures](https://uoepsy.github.io/dapr2/2324/lectures/dapr2_09_BootstrapLM.html#1) for further details, examples, and to revise these concepts further.

:::

`r optend()`
